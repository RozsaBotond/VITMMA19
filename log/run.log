Unable to find image 'dl-project:latest' locally
docker: Error response from daemon: pull access denied for dl-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied

Run 'docker run --help' for more information
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | CONFIGURATION
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   Epochs                         : 100
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   Device                         : cuda
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   Log Level                      : INFO
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   Augmentation                   : True
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | DATA LOADING
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | load_data | INFO     | Loading data from /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/data
2025-12-14 19:18:06 | __main__ | load_data | INFO     | Data loaded successfully. X shape: (300, 256, 4), Y shape: (300, 256)
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | DATA PREPARATION
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | prepare_dataloaders | INFO     | Created new stratified split: train=210, val=45, test=45
2025-12-14 19:18:06 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler.
2025-12-14 19:18:06 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:18:06 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:18:06 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   hidden_size                    : 96
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   num_layers                     : 2
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   dropout                        : 0.5
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   bidirectional                  : True
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   batch_size                     : 16
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   scheduler                      : cosine
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   use_class_weights              : False
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:18:07 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 19:18:08 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 19:18:08 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 19:18:08 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 19:18:08 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 19:18:09 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 19:18:09 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 19:18:09 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 19:18:09 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 19:18:10 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 19:18:10 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 19:18:10 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 19:18:10 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 19:18:11 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 19:18:11 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 19:18:11 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 19:18:11 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 19:18:12 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 19:18:12 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 19:18:12 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 19:18:12 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 19:18:12 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 19:18:13 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 19:18:13 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 19:18:13 | __main__ | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 19:18:13 | __main__ | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 19:18:14 | __main__ | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 19:18:14 | __main__ | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 19:18:14 | __main__ | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 19:18:14 | __main__ | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 19:18:15 | __main__ | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 19:18:15 | __main__ | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 19:18:15 | __main__ | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 19:18:15 | __main__ | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 19:18:15 | __main__ | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 8.4s. Best validation F1: 0.6429
2025-12-14 19:18:15 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:15 | __main__ | log_header | INFO     | EVALUATION: LSTM_V2 (SEQLABELING)
2025-12-14 19:18:15 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:15 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:15 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) TEST SET METRICS
2025-12-14 19:18:15 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:15 | __main__ | log_evaluation | INFO     |   accuracy                  : 0.6703
2025-12-14 19:18:15 | __main__ | log_evaluation | INFO     |   f1_weighted               : 0.5380
2025-12-14 19:18:15 | __main__ | log_evaluation | INFO     |   f1_macro                  : 0.1147
2025-12-14 19:18:15 | __main__ | log_evaluation | INFO     |   detection_rate            : 0.0000
2025-12-14 19:18:15 | __main__ | log_evaluation | INFO     |   false_alarm_rate          : 0.0000
2025-12-14 19:18:15 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | Confusion Matrix:
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     |                 0        1        2        3        4        5        6
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 0           7722        0        0        0        0        0        0
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 1           1195        0        0        0        0        0        0
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 2            742        0        0        0        0        0        0
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 3            342        0        0        0        0        0        0
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 4            966        0        0        0        0        0        0
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 5            510        0        0        0        0        0        0
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 6             43        0        0        0        0        0        0
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     | Classification Report:
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |                 precision    recall  f1-score   support
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |   
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |              0       0.67      1.00      0.80      7722
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |              1       0.00      0.00      0.00      1195
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |              2       0.00      0.00      0.00       742
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |              3       0.00      0.00      0.00       342
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |              4       0.00      0.00      0.00       966
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |              5       0.00      0.00      0.00       510
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |              6       0.00      0.00      0.00        43
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |   
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |       accuracy                           0.67     11520
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |      macro avg       0.10      0.14      0.11     11520
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |   weighted avg       0.45      0.67      0.54     11520
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |   
2025-12-14 19:18:15 | __main__ | main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 19:18:15 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:15 | __main__ | log_header | INFO     | DATA PREPARATION
2025-12-14 19:18:15 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:15 | __main__ | prepare_dataloaders | INFO     | Created new stratified split: train=210, val=45, test=45
2025-12-14 19:18:15 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler.
2025-12-14 19:18:15 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:18:15 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:18:15 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | CONFIGURATION
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   Epochs                         : 100
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   Device                         : cuda
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   Log Level                      : INFO
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   Augmentation                   : True
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | DATA LOADING
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | load_data | INFO     | Loading data from /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/data
2025-12-14 19:20:10 | __main__ | load_data | INFO     | Data loaded successfully. X shape: (300, 256, 4), Y shape: (300, 256)
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | DATA PREPARATION
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | prepare_dataloaders | INFO     | Created new stratified split: train=210, val=45, test=45
2025-12-14 19:20:10 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler.
2025-12-14 19:20:10 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:20:10 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:20:10 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   hidden_size                    : 96
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   num_layers                     : 2
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   dropout                        : 0.5
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   bidirectional                  : True
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   batch_size                     : 16
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   scheduler                      : cosine
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   use_class_weights              : False
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:12 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 19:20:12 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 19:20:12 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 19:20:13 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 19:20:13 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 19:20:13 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 19:20:13 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 19:20:14 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 19:20:14 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 19:20:14 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 19:20:14 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 19:20:15 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 19:20:15 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 19:20:15 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 19:20:15 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 19:20:16 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 19:20:16 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 19:20:16 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 19:20:16 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 19:20:17 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 19:20:17 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 19:20:17 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 19:20:17 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 19:20:18 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 19:20:18 | __main__ | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 19:20:18 | __main__ | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 19:20:18 | __main__ | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 19:20:19 | __main__ | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 19:20:19 | __main__ | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 19:20:19 | __main__ | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 19:20:19 | __main__ | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 19:20:20 | __main__ | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 19:20:20 | __main__ | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 19:20:20 | __main__ | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 19:20:20 | __main__ | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 8.4s. Best validation F1: 0.6429
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_header | INFO     | EVALUATION: LSTM_V2 (SEQLABELING)
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) TEST SET METRICS
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_evaluation | INFO     |   accuracy                  : 0.6703
2025-12-14 19:20:20 | __main__ | log_evaluation | INFO     |   f1_weighted               : 0.5380
2025-12-14 19:20:20 | __main__ | log_evaluation | INFO     |   f1_macro                  : 0.1147
2025-12-14 19:20:20 | __main__ | log_evaluation | INFO     |   detection_rate            : 0.0000
2025-12-14 19:20:20 | __main__ | log_evaluation | INFO     |   false_alarm_rate          : 0.0000
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | Confusion Matrix:
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     |                 0        1        2        3        4        5        6
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 0           7722        0        0        0        0        0        0
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 1           1195        0        0        0        0        0        0
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 2            742        0        0        0        0        0        0
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 3            342        0        0        0        0        0        0
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 4            966        0        0        0        0        0        0
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 5            510        0        0        0        0        0        0
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 6             43        0        0        0        0        0        0
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     | Classification Report:
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |                 precision    recall  f1-score   support
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |   
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |              0       0.67      1.00      0.80      7722
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |              1       0.00      0.00      0.00      1195
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |              2       0.00      0.00      0.00       742
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |              3       0.00      0.00      0.00       342
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |              4       0.00      0.00      0.00       966
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |              5       0.00      0.00      0.00       510
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |              6       0.00      0.00      0.00        43
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |   
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |       accuracy                           0.67     11520
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |      macro avg       0.10      0.14      0.11     11520
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |   weighted avg       0.45      0.67      0.54     11520
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |   
2025-12-14 19:20:20 | __main__ | main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_header | INFO     | DATA PREPARATION
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | prepare_dataloaders | INFO     | Created new stratified split: train=210, val=45, test=45
2025-12-14 19:20:20 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler.
2025-12-14 19:20:20 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:20:20 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:20:20 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_header | INFO     | TRAINING: HIERARCHICAL LSTM
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_header | INFO     | HIERARCHICAL LSTM CONFIG
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_header | INFO     | HIERARCHICAL LSTM BEST HYPERPARAMETERS
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   batch_size                     : 32
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   scheduler                      : cosine
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   grad_clip                      : 0.5
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s1_hidden_size                 : 192
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s1_num_layers                  : 2
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s1_dropout                     : 0.5
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s1_bidirectional               : True
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s2_hidden_size                 : 64
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s2_num_layers                  : 3
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s2_dropout                     : 0.3
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s2_bidirectional               : False
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   use_class_weights              : True
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   class_weight_scale             : 0.6
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     | Model: HierarchicalClassifier
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     |   stage1                    :  1,193,091 params ( 1,193,091 trainable)
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     |   stage2                    :     84,675 params (    84,675 trainable)
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     | Total parameters          :  1,277,766
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     | Trainable parameters      :  1,277,766
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:21 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.3949 | Train Acc: 0.2390 | Val Loss: 1.9860 | Val Acc: 0.2101 | LR: 0.000288 | Val F1: 0.1914
2025-12-14 19:20:22 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.3152 | Train Acc: 0.2117 | Val Loss: 2.0607 | Val Acc: 0.2404 | LR: 0.000288 | Val F1: 0.2729
2025-12-14 19:20:22 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.3001 | Train Acc: 0.1852 | Val Loss: 1.9873 | Val Acc: 0.2655 | LR: 0.000288 | Val F1: 0.3146
2025-12-14 19:20:23 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.2574 | Train Acc: 0.1779 | Val Loss: 1.9836 | Val Acc: 0.2182 | LR: 0.000287 | Val F1: 0.2417
2025-12-14 19:20:24 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.2528 | Train Acc: 0.2352 | Val Loss: 1.9733 | Val Acc: 0.2745 | LR: 0.000287 | Val F1: 0.3110
2025-12-14 19:20:25 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.2219 | Train Acc: 0.2438 | Val Loss: 2.0731 | Val Acc: 0.1727 | LR: 0.000286 | Val F1: 0.1778
2025-12-14 19:20:26 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.2270 | Train Acc: 0.2021 | Val Loss: 2.0306 | Val Acc: 0.3899 | LR: 0.000285 | Val F1: 0.4496
2025-12-14 19:20:27 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.2329 | Train Acc: 0.2830 | Val Loss: 1.9791 | Val Acc: 0.1543 | LR: 0.000285 | Val F1: 0.1320
2025-12-14 19:20:28 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.2236 | Train Acc: 0.2172 | Val Loss: 2.0190 | Val Acc: 0.4240 | LR: 0.000283 | Val F1: 0.4919
2025-12-14 19:20:29 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.2004 | Train Acc: 0.3239 | Val Loss: 1.9722 | Val Acc: 0.1903 | LR: 0.000282 | Val F1: 0.2052
2025-12-14 19:20:30 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.2156 | Train Acc: 0.2425 | Val Loss: 2.0374 | Val Acc: 0.2116 | LR: 0.000281 | Val F1: 0.2280
2025-12-14 19:20:30 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.1825 | Train Acc: 0.2420 | Val Loss: 2.0586 | Val Acc: 0.1573 | LR: 0.000280 | Val F1: 0.1609
2025-12-14 19:20:31 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.1836 | Train Acc: 0.2435 | Val Loss: 1.9628 | Val Acc: 0.2378 | LR: 0.000278 | Val F1: 0.2691
2025-12-14 19:20:32 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.1835 | Train Acc: 0.2686 | Val Loss: 1.9564 | Val Acc: 0.4023 | LR: 0.000276 | Val F1: 0.4581
2025-12-14 19:20:33 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.1889 | Train Acc: 0.2617 | Val Loss: 1.9553 | Val Acc: 0.1901 | LR: 0.000274 | Val F1: 0.2030
2025-12-14 19:20:34 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.1606 | Train Acc: 0.2830 | Val Loss: 2.0343 | Val Acc: 0.2225 | LR: 0.000272 | Val F1: 0.2518
2025-12-14 19:20:34 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.1493 | Train Acc: 0.3194 | Val Loss: 2.0108 | Val Acc: 0.1785 | LR: 0.000270 | Val F1: 0.1895
2025-12-14 19:20:35 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.1637 | Train Acc: 0.2176 | Val Loss: 2.0132 | Val Acc: 0.3490 | LR: 0.000268 | Val F1: 0.4048
2025-12-14 19:20:36 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.1246 | Train Acc: 0.3117 | Val Loss: 1.9923 | Val Acc: 0.2252 | LR: 0.000266 | Val F1: 0.2524
2025-12-14 19:20:37 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.1473 | Train Acc: 0.2826 | Val Loss: 1.9858 | Val Acc: 0.3020 | LR: 0.000263 | Val F1: 0.3570
2025-12-14 19:20:38 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.1418 | Train Acc: 0.2769 | Val Loss: 1.9281 | Val Acc: 0.3245 | LR: 0.000261 | Val F1: 0.3781
2025-12-14 19:20:39 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.1467 | Train Acc: 0.2208 | Val Loss: 2.0474 | Val Acc: 0.3464 | LR: 0.000258 | Val F1: 0.4137
2025-12-14 19:20:39 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.1245 | Train Acc: 0.3384 | Val Loss: 2.0052 | Val Acc: 0.2829 | LR: 0.000255 | Val F1: 0.3234
2025-12-14 19:20:40 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.1411 | Train Acc: 0.3000 | Val Loss: 2.0087 | Val Acc: 0.3490 | LR: 0.000252 | Val F1: 0.4004
2025-12-14 19:20:40 | __main__ | train_model | INFO     | Early stopping triggered at epoch 24 due to no improvement in validation F1.
2025-12-14 19:20:40 | __main__ | train_model | INFO     | Training for Hierarchical LSTM completed in 20.4s. Best validation F1: 0.4919
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_header | INFO     | EVALUATION: HIERARCHICAL LSTM
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_header | INFO     | HIERARCHICAL LSTM TEST SET METRICS
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_evaluation | INFO     |   accuracy                  : 0.6703
2025-12-14 19:20:40 | __main__ | log_evaluation | INFO     |   f1_weighted               : 0.5380
2025-12-14 19:20:40 | __main__ | log_evaluation | INFO     |   f1_macro                  : 0.1147
2025-12-14 19:20:40 | __main__ | log_evaluation | INFO     |   detection_rate            : 0.0000
2025-12-14 19:20:40 | __main__ | log_evaluation | INFO     |   false_alarm_rate          : 0.0000
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | Confusion Matrix:
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     |                 0        1        2        3        4        5        6
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 0           7722        0        0        0        0        0        0
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 1           1195        0        0        0        0        0        0
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 2            742        0        0        0        0        0        0
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 3            342        0        0        0        0        0        0
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 4            966        0        0        0        0        0        0
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 5            510        0        0        0        0        0        0
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 6             43        0        0        0        0        0        0
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     | Classification Report:
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |                 precision    recall  f1-score   support
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |   
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |              0       0.67      1.00      0.80      7722
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |              1       0.00      0.00      0.00      1195
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |              2       0.00      0.00      0.00       742
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |              3       0.00      0.00      0.00       342
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |              4       0.00      0.00      0.00       966
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |              5       0.00      0.00      0.00       510
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |              6       0.00      0.00      0.00        43
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |   
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |       accuracy                           0.67     11520
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |      macro avg       0.10      0.14      0.11     11520
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |   weighted avg       0.45      0.67      0.54     11520
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |   
2025-12-14 19:20:40 | __main__ | main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/hierarchical_v1/best_model.pth
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_header | INFO     | DATA PREPARATION
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | prepare_dataloaders | INFO     | Created new stratified split: train=210, val=45, test=45
2025-12-14 19:20:40 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler.
2025-12-14 19:20:40 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:20:40 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:20:40 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   d_model                        : 128
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   nhead                          : 4
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   dropout                        : 0.5
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   learnable_pe                   : False
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   batch_size                     : 8
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:42 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.7402 | Train Acc: 0.1217 | Val Loss: 2.1374 | Val Acc: 0.2666 | LR: 0.000930 | Val F1: 0.3008
2025-12-14 19:20:43 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.4959 | Train Acc: 0.1478 | Val Loss: 2.1870 | Val Acc: 0.0619 | LR: 0.000930 | Val F1: 0.0309
2025-12-14 19:20:44 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.4098 | Train Acc: 0.1467 | Val Loss: 2.2735 | Val Acc: 0.0118 | LR: 0.000929 | Val F1: 0.0091
2025-12-14 19:20:46 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.3952 | Train Acc: 0.1592 | Val Loss: 2.2967 | Val Acc: 0.5253 | LR: 0.000928 | Val F1: 0.5448
2025-12-14 19:20:47 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.3772 | Train Acc: 0.2387 | Val Loss: 2.0897 | Val Acc: 0.5460 | LR: 0.000926 | Val F1: 0.5629
2025-12-14 19:20:49 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.3351 | Train Acc: 0.2571 | Val Loss: 2.0613 | Val Acc: 0.0963 | LR: 0.000924 | Val F1: 0.0456
2025-12-14 19:20:50 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.3608 | Train Acc: 0.2400 | Val Loss: 2.2677 | Val Acc: 0.4595 | LR: 0.000922 | Val F1: 0.5228
2025-12-14 19:20:51 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.3280 | Train Acc: 0.2571 | Val Loss: 2.0224 | Val Acc: 0.5381 | LR: 0.000919 | Val F1: 0.5652
2025-12-14 19:20:53 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.3256 | Train Acc: 0.2328 | Val Loss: 2.3645 | Val Acc: 0.4981 | LR: 0.000915 | Val F1: 0.5473
2025-12-14 19:20:54 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.3346 | Train Acc: 0.2884 | Val Loss: 2.2241 | Val Acc: 0.0946 | LR: 0.000912 | Val F1: 0.0606
2025-12-14 19:20:56 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.3079 | Train Acc: 0.2946 | Val Loss: 2.1268 | Val Acc: 0.3738 | LR: 0.000907 | Val F1: 0.4477
2025-12-14 19:20:57 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.2992 | Train Acc: 0.3063 | Val Loss: 2.1457 | Val Acc: 0.0781 | LR: 0.000903 | Val F1: 0.0599
2025-12-14 19:20:58 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.2994 | Train Acc: 0.3100 | Val Loss: 2.1047 | Val Acc: 0.1685 | LR: 0.000897 | Val F1: 0.1742
2025-12-14 19:20:59 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.2842 | Train Acc: 0.2777 | Val Loss: 2.0855 | Val Acc: 0.3612 | LR: 0.000892 | Val F1: 0.4269
2025-12-14 19:21:00 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.2711 | Train Acc: 0.3368 | Val Loss: 2.1809 | Val Acc: 0.0872 | LR: 0.000886 | Val F1: 0.1002
2025-12-14 19:21:02 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.2780 | Train Acc: 0.2718 | Val Loss: 2.0283 | Val Acc: 0.2968 | LR: 0.000879 | Val F1: 0.3705
2025-12-14 19:21:03 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.2919 | Train Acc: 0.3768 | Val Loss: 1.9468 | Val Acc: 0.4935 | LR: 0.000873 | Val F1: 0.5291
2025-12-14 19:21:05 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.2734 | Train Acc: 0.4133 | Val Loss: 2.1056 | Val Acc: 0.3358 | LR: 0.000865 | Val F1: 0.4299
2025-12-14 19:21:05 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.2774 | Train Acc: 0.2876 | Val Loss: 2.1090 | Val Acc: 0.5342 | LR: 0.000858 | Val F1: 0.5828
2025-12-14 19:21:07 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.2601 | Train Acc: 0.4002 | Val Loss: 2.0991 | Val Acc: 0.4229 | LR: 0.000850 | Val F1: 0.4993
2025-12-14 19:21:08 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.2497 | Train Acc: 0.3157 | Val Loss: 2.0718 | Val Acc: 0.3020 | LR: 0.000841 | Val F1: 0.3878
2025-12-14 19:21:10 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.2580 | Train Acc: 0.4132 | Val Loss: 1.9739 | Val Acc: 0.4622 | LR: 0.000833 | Val F1: 0.5270
2025-12-14 19:21:11 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.2483 | Train Acc: 0.3064 | Val Loss: 2.0120 | Val Acc: 0.5478 | LR: 0.000823 | Val F1: 0.5733
2025-12-14 19:21:12 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.2631 | Train Acc: 0.3683 | Val Loss: 2.0258 | Val Acc: 0.4151 | LR: 0.000814 | Val F1: 0.4849
2025-12-14 19:21:14 | __main__ | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 2.2470 | Train Acc: 0.3918 | Val Loss: 2.0717 | Val Acc: 0.3485 | LR: 0.000804 | Val F1: 0.4231
2025-12-14 19:21:15 | __main__ | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 2.2429 | Train Acc: 0.3449 | Val Loss: 2.1144 | Val Acc: 0.5780 | LR: 0.000794 | Val F1: 0.6053
2025-12-14 19:21:16 | __main__ | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 2.2375 | Train Acc: 0.4032 | Val Loss: 2.0546 | Val Acc: 0.5498 | LR: 0.000783 | Val F1: 0.5905
2025-12-14 19:21:17 | __main__ | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 2.2391 | Train Acc: 0.4200 | Val Loss: 2.0188 | Val Acc: 0.3359 | LR: 0.000773 | Val F1: 0.4361
2025-12-14 19:21:19 | __main__ | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 2.2327 | Train Acc: 0.2907 | Val Loss: 1.9870 | Val Acc: 0.3590 | LR: 0.000762 | Val F1: 0.4444
2025-12-14 19:21:20 | __main__ | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 2.2386 | Train Acc: 0.3917 | Val Loss: 1.9732 | Val Acc: 0.5076 | LR: 0.000750 | Val F1: 0.5643
2025-12-14 19:21:22 | __main__ | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 2.2174 | Train Acc: 0.3499 | Val Loss: 1.9999 | Val Acc: 0.4157 | LR: 0.000739 | Val F1: 0.4706
2025-12-14 19:21:23 | __main__ | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 2.2348 | Train Acc: 0.3402 | Val Loss: 1.9940 | Val Acc: 0.5273 | LR: 0.000727 | Val F1: 0.5529
2025-12-14 19:21:24 | __main__ | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 2.2364 | Train Acc: 0.3426 | Val Loss: 1.9889 | Val Acc: 0.3642 | LR: 0.000714 | Val F1: 0.4165
2025-12-14 19:21:25 | __main__ | log_epoch | INFO     | Epoch [  34/100] | Train Loss: 2.2109 | Train Acc: 0.3684 | Val Loss: 1.9684 | Val Acc: 0.4154 | LR: 0.000702 | Val F1: 0.4584
2025-12-14 19:21:26 | __main__ | log_epoch | INFO     | Epoch [  35/100] | Train Loss: 2.2285 | Train Acc: 0.3570 | Val Loss: 2.0073 | Val Acc: 0.4233 | LR: 0.000689 | Val F1: 0.4902
2025-12-14 19:21:27 | __main__ | log_epoch | INFO     | Epoch [  36/100] | Train Loss: 2.2225 | Train Acc: 0.3888 | Val Loss: 2.0480 | Val Acc: 0.4255 | LR: 0.000676 | Val F1: 0.5028
2025-12-14 19:21:28 | __main__ | log_epoch | INFO     | Epoch [  37/100] | Train Loss: 2.2083 | Train Acc: 0.3957 | Val Loss: 2.0777 | Val Acc: 0.6352 | LR: 0.000663 | Val F1: 0.6329
2025-12-14 19:21:30 | __main__ | log_epoch | INFO     | Epoch [  38/100] | Train Loss: 2.2149 | Train Acc: 0.3912 | Val Loss: 2.1173 | Val Acc: 0.2447 | LR: 0.000650 | Val F1: 0.3120
2025-12-14 19:21:31 | __main__ | log_epoch | INFO     | Epoch [  39/100] | Train Loss: 2.1937 | Train Acc: 0.2806 | Val Loss: 2.1149 | Val Acc: 0.4516 | LR: 0.000636 | Val F1: 0.5272
2025-12-14 19:21:31 | __main__ | log_epoch | INFO     | Epoch [  40/100] | Train Loss: 2.2146 | Train Acc: 0.4470 | Val Loss: 2.0805 | Val Acc: 0.3289 | LR: 0.000623 | Val F1: 0.4160
2025-12-14 19:21:32 | __main__ | log_epoch | INFO     | Epoch [  41/100] | Train Loss: 2.2278 | Train Acc: 0.3691 | Val Loss: 2.0339 | Val Acc: 0.4866 | LR: 0.000609 | Val F1: 0.5495
2025-12-14 19:21:33 | __main__ | log_epoch | INFO     | Epoch [  42/100] | Train Loss: 2.1858 | Train Acc: 0.3544 | Val Loss: 2.0835 | Val Acc: 0.6259 | LR: 0.000595 | Val F1: 0.6304
2025-12-14 19:21:34 | __main__ | log_epoch | INFO     | Epoch [  43/100] | Train Loss: 2.1987 | Train Acc: 0.4072 | Val Loss: 2.0043 | Val Acc: 0.3995 | LR: 0.000581 | Val F1: 0.4717
2025-12-14 19:21:35 | __main__ | log_epoch | INFO     | Epoch [  44/100] | Train Loss: 2.2131 | Train Acc: 0.4252 | Val Loss: 2.0462 | Val Acc: 0.4686 | LR: 0.000567 | Val F1: 0.5226
2025-12-14 19:21:37 | __main__ | log_epoch | INFO     | Epoch [  45/100] | Train Loss: 2.1985 | Train Acc: 0.3813 | Val Loss: 2.0696 | Val Acc: 0.4043 | LR: 0.000553 | Val F1: 0.4848
2025-12-14 19:21:38 | __main__ | log_epoch | INFO     | Epoch [  46/100] | Train Loss: 2.1972 | Train Acc: 0.3800 | Val Loss: 2.0399 | Val Acc: 0.3847 | LR: 0.000538 | Val F1: 0.4579
2025-12-14 19:21:39 | __main__ | log_epoch | INFO     | Epoch [  47/100] | Train Loss: 2.2099 | Train Acc: 0.4079 | Val Loss: 2.0713 | Val Acc: 0.4546 | LR: 0.000524 | Val F1: 0.5303
2025-12-14 19:21:40 | __main__ | log_epoch | INFO     | Epoch [  48/100] | Train Loss: 2.1922 | Train Acc: 0.3917 | Val Loss: 2.0124 | Val Acc: 0.3316 | LR: 0.000509 | Val F1: 0.3982
2025-12-14 19:21:41 | __main__ | log_epoch | INFO     | Epoch [  49/100] | Train Loss: 2.1793 | Train Acc: 0.3632 | Val Loss: 2.0389 | Val Acc: 0.5997 | LR: 0.000495 | Val F1: 0.6185
2025-12-14 19:21:42 | __main__ | log_epoch | INFO     | Epoch [  50/100] | Train Loss: 2.1819 | Train Acc: 0.4222 | Val Loss: 2.0264 | Val Acc: 0.4901 | LR: 0.000480 | Val F1: 0.5341
2025-12-14 19:21:43 | __main__ | log_epoch | INFO     | Epoch [  51/100] | Train Loss: 2.1827 | Train Acc: 0.3482 | Val Loss: 2.0647 | Val Acc: 0.4015 | LR: 0.000466 | Val F1: 0.4780
2025-12-14 19:21:44 | __main__ | log_epoch | INFO     | Epoch [  52/100] | Train Loss: 2.1825 | Train Acc: 0.3686 | Val Loss: 2.0804 | Val Acc: 0.4089 | LR: 0.000451 | Val F1: 0.4902
2025-12-14 19:21:44 | __main__ | train_model | INFO     | Early stopping triggered at epoch 52 due to no improvement in validation F1.
2025-12-14 19:21:44 | __main__ | train_model | INFO     | Training for Transformer completed in 64.0s. Best validation F1: 0.6329
2025-12-14 19:21:44 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:44 | __main__ | log_header | INFO     | EVALUATION: TRANSFORMER
2025-12-14 19:21:44 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:44 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:44 | __main__ | log_header | INFO     | TRANSFORMER TEST SET METRICS
2025-12-14 19:21:44 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:44 | __main__ | log_evaluation | INFO     |   accuracy                  : 0.1037
2025-12-14 19:21:44 | __main__ | log_evaluation | INFO     |   f1_weighted               : 0.0195
2025-12-14 19:21:44 | __main__ | log_evaluation | INFO     |   f1_macro                  : 0.0269
2025-12-14 19:21:44 | __main__ | log_evaluation | INFO     |   detection_rate            : 1.0000
2025-12-14 19:21:44 | __main__ | log_evaluation | INFO     |   false_alarm_rate          : 1.0000
2025-12-14 19:21:44 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | Confusion Matrix:
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     |                 0        1        2        3        4        5        6
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 0              0     7722        0        0        0        0        0
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 1              0     1195        0        0        0        0        0
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 2              0      742        0        0        0        0        0
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 3              0      342        0        0        0        0        0
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 4              0      966        0        0        0        0        0
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 5              0      510        0        0        0        0        0
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 6              0       43        0        0        0        0        0
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     | Classification Report:
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |                 precision    recall  f1-score   support
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |   
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |              0       0.00      0.00      0.00      7722
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |              1       0.10      1.00      0.19      1195
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |              2       0.00      0.00      0.00       742
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |              3       0.00      0.00      0.00       342
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |              4       0.00      0.00      0.00       966
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |              5       0.00      0.00      0.00       510
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |              6       0.00      0.00      0.00        43
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |   
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |       accuracy                           0.10     11520
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |      macro avg       0.01      0.14      0.03     11520
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |   weighted avg       0.01      0.10      0.02     11520
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |   
2025-12-14 19:21:44 | __main__ | main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 19:21:44 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:44 | __main__ | log_header | INFO     | TRAINING: STATISTICAL BASELINE
2025-12-14 19:21:44 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:45 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:45 | __main__ | log_header | INFO     | BASELINE TEST SET METRICS
2025-12-14 19:21:45 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:45 | __main__ | log_evaluation | INFO     |   accuracy                  : 0.4997
2025-12-14 19:21:45 | __main__ | log_evaluation | INFO     |   f1_weighted               : 0.4644
2025-12-14 19:21:45 | __main__ | log_evaluation | INFO     |   f1_macro                  : 0.1069
2025-12-14 19:21:45 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:45 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:45 | __main__ | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 19:21:45 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:45 | __main__ | main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 19:21:45 | __main__ | main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 19:21:45 | __main__ | main | INFO     | LSTM_v2 (SeqLabeling)     0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 19:21:45 | __main__ | main | INFO     | Hierarchical LSTM         0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 19:21:45 | __main__ | main | INFO     | Statistical Baseline      0.4997       0.4644          0.1069       0.0000           0.0000            
2025-12-14 19:21:45 | __main__ | main | INFO     | Transformer               0.1037       0.0195          0.0269       1.0000           1.0000            
2025-12-14 19:21:45 | __main__ | main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_192010.json
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | CONFIGURATION
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   Epochs                         : 100
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   Device                         : cuda
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   Log Level                      : INFO
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   Augmentation                   : True
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 19:52:03 | __main__ | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 19:52:03 | __main__ | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 19:52:03 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:52:03 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:52:03 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   hidden_size                    : 96
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   num_layers                     : 2
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   dropout                        : 0.5
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   bidirectional                  : True
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   batch_size                     : 16
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   scheduler                      : cosine
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   use_class_weights              : False
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:52:05 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 19:52:05 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 19:52:05 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 19:52:05 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 19:52:06 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 19:52:06 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 19:52:06 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 19:52:07 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 19:52:07 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 19:52:07 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 19:52:07 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 19:52:08 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 19:52:08 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 19:52:08 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 19:52:08 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 19:52:08 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 19:52:09 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 19:52:09 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 19:52:09 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 19:52:09 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 19:52:09 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 19:52:10 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 19:52:10 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 19:52:10 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 19:52:10 | __main__ | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 19:52:11 | __main__ | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 19:52:11 | __main__ | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 19:52:11 | __main__ | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 19:52:11 | __main__ | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 19:52:12 | __main__ | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 19:52:12 | __main__ | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 19:52:12 | __main__ | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 19:52:12 | __main__ | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 19:52:12 | __main__ | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 19:52:12 | __main__ | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 8.3s. Best validation F1: 0.6429
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | CONFIGURATION
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   Epochs                         : 100
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   Device                         : cuda
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   Log Level                      : INFO
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   Augmentation                   : True
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 19:54:51 | __main__ | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 19:54:51 | __main__ | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 19:54:51 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:54:51 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:54:51 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   hidden_size                    : 96
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   num_layers                     : 2
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   dropout                        : 0.5
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   bidirectional                  : True
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   batch_size                     : 16
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   scheduler                      : cosine
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   use_class_weights              : False
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:54:52 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 19:54:53 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 19:54:53 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 19:54:53 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 19:54:53 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 19:54:54 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 19:54:54 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 19:54:54 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 19:54:54 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 19:54:55 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 19:54:55 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 19:54:55 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 19:54:55 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 19:54:56 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 19:54:56 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 19:54:56 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 19:54:56 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 19:54:57 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 19:54:57 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 19:54:57 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 19:54:57 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 19:54:58 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 19:54:58 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 19:54:58 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 19:54:58 | __main__ | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 19:54:58 | __main__ | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 19:54:59 | __main__ | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 19:54:59 | __main__ | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 19:54:59 | __main__ | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 19:54:59 | __main__ | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 19:55:00 | __main__ | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 19:55:00 | __main__ | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 19:55:00 | __main__ | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 19:55:00 | __main__ | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 19:55:00 | __main__ | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 8.3s. Best validation F1: 0.6429
2025-12-14 19:55:00 | __main__ | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 19:55:00 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:55:00 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:55:00 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_header | INFO     | TRAINING: HIERARCHICAL LSTM
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_header | INFO     | HIERARCHICAL LSTM CONFIG
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_header | INFO     | HIERARCHICAL LSTM BEST HYPERPARAMETERS
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   batch_size                     : 32
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   scheduler                      : cosine
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   grad_clip                      : 0.5
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s1_hidden_size                 : 192
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s1_num_layers                  : 2
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s1_dropout                     : 0.5
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s1_bidirectional               : True
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s2_hidden_size                 : 64
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s2_num_layers                  : 3
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s2_dropout                     : 0.3
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s2_bidirectional               : False
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   use_class_weights              : True
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   class_weight_scale             : 0.6
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     | Model: HierarchicalClassifier
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     |   stage1                    :  1,193,091 params ( 1,193,091 trainable)
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     |   stage2                    :     84,675 params (    84,675 trainable)
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     | Total parameters          :  1,277,766
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     | Trainable parameters      :  1,277,766
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:55:01 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.3949 | Train Acc: 0.2390 | Val Loss: 1.9860 | Val Acc: 0.2101 | LR: 0.000288 | Val F1: 0.1914
2025-12-14 19:55:02 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.3152 | Train Acc: 0.2117 | Val Loss: 2.0607 | Val Acc: 0.2404 | LR: 0.000288 | Val F1: 0.2729
2025-12-14 19:55:03 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.3001 | Train Acc: 0.1852 | Val Loss: 1.9873 | Val Acc: 0.2655 | LR: 0.000288 | Val F1: 0.3146
2025-12-14 19:55:04 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.2574 | Train Acc: 0.1779 | Val Loss: 1.9836 | Val Acc: 0.2182 | LR: 0.000287 | Val F1: 0.2417
2025-12-14 19:55:05 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.2528 | Train Acc: 0.2352 | Val Loss: 1.9733 | Val Acc: 0.2745 | LR: 0.000287 | Val F1: 0.3110
2025-12-14 19:55:06 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.2219 | Train Acc: 0.2438 | Val Loss: 2.0731 | Val Acc: 0.1727 | LR: 0.000286 | Val F1: 0.1778
2025-12-14 19:55:07 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.2270 | Train Acc: 0.2021 | Val Loss: 2.0306 | Val Acc: 0.3899 | LR: 0.000285 | Val F1: 0.4496
2025-12-14 19:55:08 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.2329 | Train Acc: 0.2830 | Val Loss: 1.9791 | Val Acc: 0.1543 | LR: 0.000285 | Val F1: 0.1320
2025-12-14 19:55:08 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.2236 | Train Acc: 0.2172 | Val Loss: 2.0190 | Val Acc: 0.4240 | LR: 0.000283 | Val F1: 0.4919
2025-12-14 19:55:09 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.2004 | Train Acc: 0.3239 | Val Loss: 1.9722 | Val Acc: 0.1903 | LR: 0.000282 | Val F1: 0.2052
2025-12-14 19:55:10 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.2156 | Train Acc: 0.2425 | Val Loss: 2.0374 | Val Acc: 0.2116 | LR: 0.000281 | Val F1: 0.2280
2025-12-14 19:55:11 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.1825 | Train Acc: 0.2420 | Val Loss: 2.0586 | Val Acc: 0.1573 | LR: 0.000280 | Val F1: 0.1609
2025-12-14 19:55:12 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.1836 | Train Acc: 0.2435 | Val Loss: 1.9628 | Val Acc: 0.2378 | LR: 0.000278 | Val F1: 0.2691
2025-12-14 19:55:13 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.1835 | Train Acc: 0.2686 | Val Loss: 1.9564 | Val Acc: 0.4023 | LR: 0.000276 | Val F1: 0.4581
2025-12-14 19:55:14 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.1889 | Train Acc: 0.2617 | Val Loss: 1.9553 | Val Acc: 0.1901 | LR: 0.000274 | Val F1: 0.2030
2025-12-14 19:55:15 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.1606 | Train Acc: 0.2830 | Val Loss: 2.0343 | Val Acc: 0.2225 | LR: 0.000272 | Val F1: 0.2518
2025-12-14 19:55:16 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.1493 | Train Acc: 0.3194 | Val Loss: 2.0108 | Val Acc: 0.1785 | LR: 0.000270 | Val F1: 0.1895
2025-12-14 19:55:17 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.1637 | Train Acc: 0.2176 | Val Loss: 2.0132 | Val Acc: 0.3490 | LR: 0.000268 | Val F1: 0.4048
2025-12-14 19:55:17 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.1246 | Train Acc: 0.3117 | Val Loss: 1.9923 | Val Acc: 0.2252 | LR: 0.000266 | Val F1: 0.2524
2025-12-14 19:55:18 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.1473 | Train Acc: 0.2826 | Val Loss: 1.9858 | Val Acc: 0.3020 | LR: 0.000263 | Val F1: 0.3570
2025-12-14 19:55:19 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.1418 | Train Acc: 0.2769 | Val Loss: 1.9281 | Val Acc: 0.3245 | LR: 0.000261 | Val F1: 0.3781
2025-12-14 19:55:20 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.1467 | Train Acc: 0.2208 | Val Loss: 2.0474 | Val Acc: 0.3464 | LR: 0.000258 | Val F1: 0.4137
2025-12-14 19:55:21 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.1245 | Train Acc: 0.3384 | Val Loss: 2.0052 | Val Acc: 0.2829 | LR: 0.000255 | Val F1: 0.3234
2025-12-14 19:55:22 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.1411 | Train Acc: 0.3000 | Val Loss: 2.0087 | Val Acc: 0.3490 | LR: 0.000252 | Val F1: 0.4004
2025-12-14 19:55:22 | __main__ | train_model | INFO     | Early stopping triggered at epoch 24 due to no improvement in validation F1.
2025-12-14 19:55:22 | __main__ | train_model | INFO     | Training for Hierarchical LSTM completed in 21.4s. Best validation F1: 0.4919
2025-12-14 19:55:22 | __main__ | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/hierarchical_v1/best_model.pth
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 19:55:22 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:55:22 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:55:22 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   d_model                        : 128
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   nhead                          : 4
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   dropout                        : 0.5
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   learnable_pe                   : False
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   batch_size                     : 8
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:55:23 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.7402 | Train Acc: 0.1217 | Val Loss: 2.1374 | Val Acc: 0.2666 | LR: 0.000930 | Val F1: 0.3008
2025-12-14 19:55:24 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.4959 | Train Acc: 0.1478 | Val Loss: 2.1870 | Val Acc: 0.0619 | LR: 0.000930 | Val F1: 0.0309
2025-12-14 19:55:26 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.4098 | Train Acc: 0.1467 | Val Loss: 2.2735 | Val Acc: 0.0118 | LR: 0.000929 | Val F1: 0.0091
2025-12-14 19:55:27 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.3952 | Train Acc: 0.1592 | Val Loss: 2.2967 | Val Acc: 0.5253 | LR: 0.000928 | Val F1: 0.5448
2025-12-14 19:55:29 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.3772 | Train Acc: 0.2387 | Val Loss: 2.0897 | Val Acc: 0.5460 | LR: 0.000926 | Val F1: 0.5629
2025-12-14 19:55:30 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.3351 | Train Acc: 0.2571 | Val Loss: 2.0613 | Val Acc: 0.0963 | LR: 0.000924 | Val F1: 0.0456
2025-12-14 19:55:31 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.3608 | Train Acc: 0.2400 | Val Loss: 2.2677 | Val Acc: 0.4595 | LR: 0.000922 | Val F1: 0.5228
2025-12-14 19:55:32 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.3280 | Train Acc: 0.2571 | Val Loss: 2.0224 | Val Acc: 0.5381 | LR: 0.000919 | Val F1: 0.5652
2025-12-14 19:55:34 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.3256 | Train Acc: 0.2328 | Val Loss: 2.3645 | Val Acc: 0.4981 | LR: 0.000915 | Val F1: 0.5473
2025-12-14 19:55:35 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.3346 | Train Acc: 0.2884 | Val Loss: 2.2241 | Val Acc: 0.0946 | LR: 0.000912 | Val F1: 0.0606
2025-12-14 19:55:37 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.3079 | Train Acc: 0.2946 | Val Loss: 2.1268 | Val Acc: 0.3738 | LR: 0.000907 | Val F1: 0.4477
2025-12-14 19:55:37 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.2992 | Train Acc: 0.3063 | Val Loss: 2.1457 | Val Acc: 0.0781 | LR: 0.000903 | Val F1: 0.0599
2025-12-14 19:55:38 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.2994 | Train Acc: 0.3100 | Val Loss: 2.1047 | Val Acc: 0.1685 | LR: 0.000897 | Val F1: 0.1742
2025-12-14 19:55:40 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.2842 | Train Acc: 0.2777 | Val Loss: 2.0855 | Val Acc: 0.3612 | LR: 0.000892 | Val F1: 0.4269
2025-12-14 19:55:41 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.2711 | Train Acc: 0.3368 | Val Loss: 2.1809 | Val Acc: 0.0872 | LR: 0.000886 | Val F1: 0.1002
2025-12-14 19:55:43 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.2780 | Train Acc: 0.2718 | Val Loss: 2.0283 | Val Acc: 0.2968 | LR: 0.000879 | Val F1: 0.3705
2025-12-14 19:55:44 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.2919 | Train Acc: 0.3768 | Val Loss: 1.9468 | Val Acc: 0.4935 | LR: 0.000873 | Val F1: 0.5291
2025-12-14 19:55:45 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.2734 | Train Acc: 0.4133 | Val Loss: 2.1056 | Val Acc: 0.3358 | LR: 0.000865 | Val F1: 0.4299
2025-12-14 19:55:47 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.2774 | Train Acc: 0.2876 | Val Loss: 2.1090 | Val Acc: 0.5342 | LR: 0.000858 | Val F1: 0.5828
2025-12-14 19:55:48 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.2601 | Train Acc: 0.4002 | Val Loss: 2.0991 | Val Acc: 0.4229 | LR: 0.000850 | Val F1: 0.4993
2025-12-14 19:55:50 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.2497 | Train Acc: 0.3157 | Val Loss: 2.0718 | Val Acc: 0.3020 | LR: 0.000841 | Val F1: 0.3878
2025-12-14 19:55:51 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.2580 | Train Acc: 0.4132 | Val Loss: 1.9739 | Val Acc: 0.4622 | LR: 0.000833 | Val F1: 0.5270
2025-12-14 19:55:52 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.2483 | Train Acc: 0.3064 | Val Loss: 2.0120 | Val Acc: 0.5478 | LR: 0.000823 | Val F1: 0.5733
2025-12-14 19:55:54 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.2631 | Train Acc: 0.3683 | Val Loss: 2.0258 | Val Acc: 0.4151 | LR: 0.000814 | Val F1: 0.4849
2025-12-14 19:55:55 | __main__ | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 2.2470 | Train Acc: 0.3918 | Val Loss: 2.0717 | Val Acc: 0.3485 | LR: 0.000804 | Val F1: 0.4231
2025-12-14 19:55:56 | __main__ | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 2.2429 | Train Acc: 0.3449 | Val Loss: 2.1144 | Val Acc: 0.5780 | LR: 0.000794 | Val F1: 0.6053
2025-12-14 19:55:58 | __main__ | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 2.2375 | Train Acc: 0.4032 | Val Loss: 2.0546 | Val Acc: 0.5498 | LR: 0.000783 | Val F1: 0.5905
2025-12-14 19:55:59 | __main__ | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 2.2391 | Train Acc: 0.4200 | Val Loss: 2.0188 | Val Acc: 0.3359 | LR: 0.000773 | Val F1: 0.4361
2025-12-14 19:56:00 | __main__ | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 2.2327 | Train Acc: 0.2907 | Val Loss: 1.9870 | Val Acc: 0.3590 | LR: 0.000762 | Val F1: 0.4444
2025-12-14 19:56:02 | __main__ | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 2.2386 | Train Acc: 0.3917 | Val Loss: 1.9732 | Val Acc: 0.5076 | LR: 0.000750 | Val F1: 0.5643
2025-12-14 19:56:03 | __main__ | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 2.2174 | Train Acc: 0.3499 | Val Loss: 1.9999 | Val Acc: 0.4157 | LR: 0.000739 | Val F1: 0.4706
2025-12-14 19:56:04 | __main__ | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 2.2348 | Train Acc: 0.3402 | Val Loss: 1.9940 | Val Acc: 0.5273 | LR: 0.000727 | Val F1: 0.5529
2025-12-14 19:56:06 | __main__ | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 2.2364 | Train Acc: 0.3426 | Val Loss: 1.9889 | Val Acc: 0.3642 | LR: 0.000714 | Val F1: 0.4165
2025-12-14 19:56:07 | __main__ | log_epoch | INFO     | Epoch [  34/100] | Train Loss: 2.2109 | Train Acc: 0.3684 | Val Loss: 1.9684 | Val Acc: 0.4154 | LR: 0.000702 | Val F1: 0.4584
2025-12-14 19:56:09 | __main__ | log_epoch | INFO     | Epoch [  35/100] | Train Loss: 2.2285 | Train Acc: 0.3570 | Val Loss: 2.0073 | Val Acc: 0.4233 | LR: 0.000689 | Val F1: 0.4902
2025-12-14 19:56:10 | __main__ | log_epoch | INFO     | Epoch [  36/100] | Train Loss: 2.2225 | Train Acc: 0.3888 | Val Loss: 2.0480 | Val Acc: 0.4255 | LR: 0.000676 | Val F1: 0.5028
2025-12-14 19:56:11 | __main__ | log_epoch | INFO     | Epoch [  37/100] | Train Loss: 2.2083 | Train Acc: 0.3957 | Val Loss: 2.0777 | Val Acc: 0.6352 | LR: 0.000663 | Val F1: 0.6329
2025-12-14 19:56:12 | __main__ | log_epoch | INFO     | Epoch [  38/100] | Train Loss: 2.2149 | Train Acc: 0.3912 | Val Loss: 2.1173 | Val Acc: 0.2447 | LR: 0.000650 | Val F1: 0.3120
2025-12-14 19:56:13 | __main__ | log_epoch | INFO     | Epoch [  39/100] | Train Loss: 2.1937 | Train Acc: 0.2805 | Val Loss: 2.1149 | Val Acc: 0.4516 | LR: 0.000636 | Val F1: 0.5272
2025-12-14 19:56:14 | __main__ | log_epoch | INFO     | Epoch [  40/100] | Train Loss: 2.2146 | Train Acc: 0.4470 | Val Loss: 2.0805 | Val Acc: 0.3288 | LR: 0.000623 | Val F1: 0.4160
2025-12-14 19:56:16 | __main__ | log_epoch | INFO     | Epoch [  41/100] | Train Loss: 2.2278 | Train Acc: 0.3690 | Val Loss: 2.0339 | Val Acc: 0.4866 | LR: 0.000609 | Val F1: 0.5495
2025-12-14 19:56:17 | __main__ | log_epoch | INFO     | Epoch [  42/100] | Train Loss: 2.1858 | Train Acc: 0.3544 | Val Loss: 2.0835 | Val Acc: 0.6259 | LR: 0.000595 | Val F1: 0.6304
2025-12-14 19:56:18 | __main__ | log_epoch | INFO     | Epoch [  43/100] | Train Loss: 2.1987 | Train Acc: 0.4072 | Val Loss: 2.0043 | Val Acc: 0.3991 | LR: 0.000581 | Val F1: 0.4715
2025-12-14 19:56:20 | __main__ | log_epoch | INFO     | Epoch [  44/100] | Train Loss: 2.2131 | Train Acc: 0.4252 | Val Loss: 2.0462 | Val Acc: 0.4686 | LR: 0.000567 | Val F1: 0.5226
2025-12-14 19:56:21 | __main__ | log_epoch | INFO     | Epoch [  45/100] | Train Loss: 2.1985 | Train Acc: 0.3814 | Val Loss: 2.0696 | Val Acc: 0.4043 | LR: 0.000553 | Val F1: 0.4849
2025-12-14 19:56:22 | __main__ | log_epoch | INFO     | Epoch [  46/100] | Train Loss: 2.1971 | Train Acc: 0.3801 | Val Loss: 2.0398 | Val Acc: 0.3849 | LR: 0.000538 | Val F1: 0.4580
2025-12-14 19:56:24 | __main__ | log_epoch | INFO     | Epoch [  47/100] | Train Loss: 2.2098 | Train Acc: 0.4077 | Val Loss: 2.0713 | Val Acc: 0.4549 | LR: 0.000524 | Val F1: 0.5303
2025-12-14 19:56:25 | __main__ | log_epoch | INFO     | Epoch [  48/100] | Train Loss: 2.1918 | Train Acc: 0.3916 | Val Loss: 2.0126 | Val Acc: 0.3363 | LR: 0.000509 | Val F1: 0.4035
2025-12-14 19:56:26 | __main__ | log_epoch | INFO     | Epoch [  49/100] | Train Loss: 2.1791 | Train Acc: 0.3631 | Val Loss: 2.0384 | Val Acc: 0.6001 | LR: 0.000495 | Val F1: 0.6187
2025-12-14 19:56:27 | __main__ | log_epoch | INFO     | Epoch [  50/100] | Train Loss: 2.1819 | Train Acc: 0.4220 | Val Loss: 2.0265 | Val Acc: 0.4913 | LR: 0.000480 | Val F1: 0.5351
2025-12-14 19:56:28 | __main__ | log_epoch | INFO     | Epoch [  51/100] | Train Loss: 2.1828 | Train Acc: 0.3485 | Val Loss: 2.0643 | Val Acc: 0.4053 | LR: 0.000466 | Val F1: 0.4814
2025-12-14 19:56:29 | __main__ | log_epoch | INFO     | Epoch [  52/100] | Train Loss: 2.1827 | Train Acc: 0.3690 | Val Loss: 2.0801 | Val Acc: 0.4111 | LR: 0.000451 | Val F1: 0.4920
2025-12-14 19:56:29 | __main__ | train_model | INFO     | Early stopping triggered at epoch 52 due to no improvement in validation F1.
2025-12-14 19:56:29 | __main__ | train_model | INFO     | Training for Transformer completed in 67.7s. Best validation F1: 0.6329
2025-12-14 19:56:30 | __main__ | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 19:56:30 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:56:30 | __main__ | log_header | INFO     | TRAINING: STATISTICAL BASELINE
2025-12-14 19:56:30 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:56:30 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:56:30 | __main__ | log_header | INFO     | BASELINE TEST SET METRICS
2025-12-14 19:56:30 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:56:30 | __main__ | log_evaluation | INFO     |   accuracy                  : 0.4997
2025-12-14 19:56:30 | __main__ | log_evaluation | INFO     |   f1_weighted               : 0.4644
2025-12-14 19:56:30 | __main__ | log_evaluation | INFO     |   f1_macro                  : 0.1069
2025-12-14 19:56:30 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:56:30 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:56:30 | __main__ | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 19:56:30 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:56:30 | __main__ | train_main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 19:56:30 | __main__ | train_main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 19:56:30 | __main__ | train_main | INFO     | LSTM_v2 (SeqLabeling)     0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 19:56:30 | __main__ | train_main | INFO     | Hierarchical LSTM         0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 19:56:30 | __main__ | train_main | INFO     | Statistical Baseline      0.4997       0.4644          0.1069       0.0000           0.0000            
2025-12-14 19:56:30 | __main__ | train_main | INFO     | Transformer               0.1037       0.0195          0.0269       1.0000           1.0000            
2025-12-14 19:56:30 | __main__ | train_main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_195451.json
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | CONFIGURATION
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 20:54:54 | training | log_config | INFO     |   Epochs                         : 100
2025-12-14 20:54:54 | training | log_config | INFO     |   Device                         : cuda
2025-12-14 20:54:54 | training | log_config | INFO     |   Log Level                      : INFO
2025-12-14 20:54:54 | training | log_config | INFO     |   Augmentation                   : True
2025-12-14 20:54:54 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 20:54:54 | training | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 20:54:54 | training | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 20:54:54 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 20:54:54 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 20:54:54 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 20:54:54 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 20:54:54 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 20:54:54 | training | log_config | INFO     |   patience                       : 15
2025-12-14 20:54:54 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 20:54:54 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_config | INFO     |   hidden_size                    : 96
2025-12-14 20:54:54 | training | log_config | INFO     |   num_layers                     : 2
2025-12-14 20:54:54 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 20:54:54 | training | log_config | INFO     |   bidirectional                  : True
2025-12-14 20:54:54 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 20:54:54 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 20:54:54 | training | log_config | INFO     |   batch_size                     : 16
2025-12-14 20:54:54 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 20:54:54 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 20:54:54 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 20:54:54 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 20:54:54 | training | log_model_summary | INFO     | 
2025-12-14 20:54:54 | training | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 20:54:54 | training | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 20:54:54 | training | log_model_summary | INFO     | 
2025-12-14 20:54:54 | training | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 20:54:54 | training | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 20:54:54 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 20:54:54 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:54:55 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 20:54:55 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 20:54:56 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 20:54:56 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 20:54:56 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 20:54:56 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 20:54:57 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 20:54:57 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 20:54:57 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 20:54:58 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 20:54:58 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 20:54:58 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 20:54:58 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 20:54:59 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 20:54:59 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 20:54:59 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 20:54:59 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 20:54:59 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 20:55:00 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 20:55:00 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 20:55:00 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 20:55:00 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 20:55:01 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 20:55:01 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 20:55:01 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 20:55:01 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 20:55:02 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 20:55:02 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 20:55:02 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 20:55:02 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 20:55:03 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 20:55:03 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 20:55:03 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 20:55:03 | training | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 20:55:03 | training | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 8.5s. Best validation F1: 0.6429
2025-12-14 20:55:03 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 20:55:03 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 20:55:03 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 20:55:03 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_header | INFO     | TRAINING: HIERARCHICAL LSTM
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_header | INFO     | HIERARCHICAL LSTM CONFIG
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 20:55:03 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 20:55:03 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 20:55:03 | training | log_config | INFO     |   patience                       : 15
2025-12-14 20:55:03 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 20:55:03 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_header | INFO     | HIERARCHICAL LSTM BEST HYPERPARAMETERS
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 20:55:03 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 20:55:03 | training | log_config | INFO     |   batch_size                     : 32
2025-12-14 20:55:03 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 20:55:03 | training | log_config | INFO     |   grad_clip                      : 0.5
2025-12-14 20:55:03 | training | log_config | INFO     |   s1_hidden_size                 : 192
2025-12-14 20:55:03 | training | log_config | INFO     |   s1_num_layers                  : 2
2025-12-14 20:55:03 | training | log_config | INFO     |   s1_dropout                     : 0.5
2025-12-14 20:55:03 | training | log_config | INFO     |   s1_bidirectional               : True
2025-12-14 20:55:03 | training | log_config | INFO     |   s2_hidden_size                 : 64
2025-12-14 20:55:03 | training | log_config | INFO     |   s2_num_layers                  : 3
2025-12-14 20:55:03 | training | log_config | INFO     |   s2_dropout                     : 0.3
2025-12-14 20:55:03 | training | log_config | INFO     |   s2_bidirectional               : False
2025-12-14 20:55:03 | training | log_config | INFO     |   use_class_weights              : True
2025-12-14 20:55:03 | training | log_config | INFO     |   class_weight_scale             : 0.6
2025-12-14 20:55:03 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_model_summary | INFO     | Model: HierarchicalClassifier
2025-12-14 20:55:03 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 20:55:03 | training | log_model_summary | INFO     | 
2025-12-14 20:55:03 | training | log_model_summary | INFO     |   stage1                    :  1,193,091 params ( 1,193,091 trainable)
2025-12-14 20:55:03 | training | log_model_summary | INFO     |   stage2                    :     84,675 params (    84,675 trainable)
2025-12-14 20:55:03 | training | log_model_summary | INFO     | 
2025-12-14 20:55:03 | training | log_model_summary | INFO     | Total parameters          :  1,277,766
2025-12-14 20:55:03 | training | log_model_summary | INFO     | Trainable parameters      :  1,277,766
2025-12-14 20:55:03 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 20:55:03 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:55:04 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.3949 | Train Acc: 0.2390 | Val Loss: 1.9860 | Val Acc: 0.2101 | LR: 0.000288 | Val F1: 0.1914
2025-12-14 20:55:05 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.3152 | Train Acc: 0.2117 | Val Loss: 2.0607 | Val Acc: 0.2404 | LR: 0.000288 | Val F1: 0.2729
2025-12-14 20:55:06 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.3001 | Train Acc: 0.1852 | Val Loss: 1.9873 | Val Acc: 0.2655 | LR: 0.000288 | Val F1: 0.3146
2025-12-14 20:55:06 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.2574 | Train Acc: 0.1779 | Val Loss: 1.9836 | Val Acc: 0.2182 | LR: 0.000287 | Val F1: 0.2417
2025-12-14 20:55:07 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.2528 | Train Acc: 0.2352 | Val Loss: 1.9733 | Val Acc: 0.2745 | LR: 0.000287 | Val F1: 0.3110
2025-12-14 20:55:08 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.2219 | Train Acc: 0.2438 | Val Loss: 2.0731 | Val Acc: 0.1727 | LR: 0.000286 | Val F1: 0.1778
2025-12-14 20:55:09 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.2270 | Train Acc: 0.2021 | Val Loss: 2.0306 | Val Acc: 0.3899 | LR: 0.000285 | Val F1: 0.4496
2025-12-14 20:55:09 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.2329 | Train Acc: 0.2830 | Val Loss: 1.9791 | Val Acc: 0.1543 | LR: 0.000285 | Val F1: 0.1320
2025-12-14 20:55:10 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.2236 | Train Acc: 0.2172 | Val Loss: 2.0190 | Val Acc: 0.4240 | LR: 0.000283 | Val F1: 0.4919
2025-12-14 20:55:11 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.2004 | Train Acc: 0.3239 | Val Loss: 1.9722 | Val Acc: 0.1903 | LR: 0.000282 | Val F1: 0.2052
2025-12-14 20:55:12 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.2156 | Train Acc: 0.2425 | Val Loss: 2.0374 | Val Acc: 0.2116 | LR: 0.000281 | Val F1: 0.2280
2025-12-14 20:55:13 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.1825 | Train Acc: 0.2420 | Val Loss: 2.0586 | Val Acc: 0.1573 | LR: 0.000280 | Val F1: 0.1609
2025-12-14 20:55:14 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.1836 | Train Acc: 0.2435 | Val Loss: 1.9628 | Val Acc: 0.2378 | LR: 0.000278 | Val F1: 0.2691
2025-12-14 20:55:14 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.1835 | Train Acc: 0.2686 | Val Loss: 1.9564 | Val Acc: 0.4023 | LR: 0.000276 | Val F1: 0.4581
2025-12-14 20:55:15 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.1889 | Train Acc: 0.2617 | Val Loss: 1.9553 | Val Acc: 0.1901 | LR: 0.000274 | Val F1: 0.2030
2025-12-14 20:55:16 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.1606 | Train Acc: 0.2830 | Val Loss: 2.0343 | Val Acc: 0.2225 | LR: 0.000272 | Val F1: 0.2518
2025-12-14 20:55:17 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.1493 | Train Acc: 0.3194 | Val Loss: 2.0108 | Val Acc: 0.1785 | LR: 0.000270 | Val F1: 0.1895
2025-12-14 20:55:18 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.1637 | Train Acc: 0.2176 | Val Loss: 2.0132 | Val Acc: 0.3490 | LR: 0.000268 | Val F1: 0.4048
2025-12-14 20:55:19 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.1246 | Train Acc: 0.3117 | Val Loss: 1.9923 | Val Acc: 0.2252 | LR: 0.000266 | Val F1: 0.2524
2025-12-14 20:55:19 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.1473 | Train Acc: 0.2826 | Val Loss: 1.9858 | Val Acc: 0.3020 | LR: 0.000263 | Val F1: 0.3570
2025-12-14 20:55:20 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.1418 | Train Acc: 0.2769 | Val Loss: 1.9281 | Val Acc: 0.3245 | LR: 0.000261 | Val F1: 0.3781
2025-12-14 20:55:21 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.1467 | Train Acc: 0.2208 | Val Loss: 2.0474 | Val Acc: 0.3464 | LR: 0.000258 | Val F1: 0.4137
2025-12-14 20:55:22 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.1245 | Train Acc: 0.3384 | Val Loss: 2.0052 | Val Acc: 0.2829 | LR: 0.000255 | Val F1: 0.3234
2025-12-14 20:55:23 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.1411 | Train Acc: 0.3000 | Val Loss: 2.0087 | Val Acc: 0.3490 | LR: 0.000252 | Val F1: 0.4004
2025-12-14 20:55:23 | training | train_model | INFO     | Early stopping triggered at epoch 24 due to no improvement in validation F1.
2025-12-14 20:55:23 | training | train_model | INFO     | Training for Hierarchical LSTM completed in 19.7s. Best validation F1: 0.4919
2025-12-14 20:55:23 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/hierarchical_v1/best_model.pth
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 20:55:23 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 20:55:23 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 20:55:23 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 20:55:23 | training | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 20:55:23 | training | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 20:55:23 | training | log_config | INFO     |   patience                       : 15
2025-12-14 20:55:23 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 20:55:23 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_config | INFO     |   d_model                        : 128
2025-12-14 20:55:23 | training | log_config | INFO     |   nhead                          : 4
2025-12-14 20:55:23 | training | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 20:55:23 | training | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 20:55:23 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 20:55:23 | training | log_config | INFO     |   learnable_pe                   : False
2025-12-14 20:55:23 | training | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 20:55:23 | training | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 20:55:23 | training | log_config | INFO     |   batch_size                     : 8
2025-12-14 20:55:23 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 20:55:23 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 20:55:23 | training | log_model_summary | INFO     | 
2025-12-14 20:55:23 | training | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 20:55:23 | training | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 20:55:23 | training | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 20:55:23 | training | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 20:55:23 | training | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 20:55:23 | training | log_model_summary | INFO     | 
2025-12-14 20:55:23 | training | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 20:55:23 | training | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 20:55:23 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 20:55:23 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:55:25 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.7402 | Train Acc: 0.1217 | Val Loss: 2.1374 | Val Acc: 0.2666 | LR: 0.000930 | Val F1: 0.3008
2025-12-14 20:55:26 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.4959 | Train Acc: 0.1478 | Val Loss: 2.1870 | Val Acc: 0.0619 | LR: 0.000930 | Val F1: 0.0309
2025-12-14 20:55:27 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.4098 | Train Acc: 0.1467 | Val Loss: 2.2735 | Val Acc: 0.0118 | LR: 0.000929 | Val F1: 0.0091
2025-12-14 20:55:29 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.3952 | Train Acc: 0.1592 | Val Loss: 2.2967 | Val Acc: 0.5253 | LR: 0.000928 | Val F1: 0.5448
2025-12-14 20:55:30 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.3772 | Train Acc: 0.2387 | Val Loss: 2.0897 | Val Acc: 0.5460 | LR: 0.000926 | Val F1: 0.5629
2025-12-14 20:55:31 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.3351 | Train Acc: 0.2571 | Val Loss: 2.0613 | Val Acc: 0.0963 | LR: 0.000924 | Val F1: 0.0456
2025-12-14 20:55:32 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.3608 | Train Acc: 0.2400 | Val Loss: 2.2677 | Val Acc: 0.4595 | LR: 0.000922 | Val F1: 0.5228
2025-12-14 20:55:33 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.3280 | Train Acc: 0.2571 | Val Loss: 2.0224 | Val Acc: 0.5381 | LR: 0.000919 | Val F1: 0.5652
2025-12-14 20:55:35 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.3256 | Train Acc: 0.2328 | Val Loss: 2.3645 | Val Acc: 0.4981 | LR: 0.000915 | Val F1: 0.5473
2025-12-14 20:55:36 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.3346 | Train Acc: 0.2884 | Val Loss: 2.2241 | Val Acc: 0.0946 | LR: 0.000912 | Val F1: 0.0606
2025-12-14 20:55:37 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.3079 | Train Acc: 0.2946 | Val Loss: 2.1268 | Val Acc: 0.3738 | LR: 0.000907 | Val F1: 0.4477
2025-12-14 20:55:39 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.2992 | Train Acc: 0.3063 | Val Loss: 2.1457 | Val Acc: 0.0781 | LR: 0.000903 | Val F1: 0.0599
2025-12-14 20:55:40 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.2994 | Train Acc: 0.3100 | Val Loss: 2.1047 | Val Acc: 0.1685 | LR: 0.000897 | Val F1: 0.1742
2025-12-14 20:55:42 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.2842 | Train Acc: 0.2777 | Val Loss: 2.0855 | Val Acc: 0.3612 | LR: 0.000892 | Val F1: 0.4269
2025-12-14 20:55:43 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.2711 | Train Acc: 0.3368 | Val Loss: 2.1809 | Val Acc: 0.0872 | LR: 0.000886 | Val F1: 0.1002
2025-12-14 20:55:44 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.2780 | Train Acc: 0.2718 | Val Loss: 2.0283 | Val Acc: 0.2968 | LR: 0.000879 | Val F1: 0.3705
2025-12-14 20:55:45 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.2919 | Train Acc: 0.3768 | Val Loss: 1.9468 | Val Acc: 0.4935 | LR: 0.000873 | Val F1: 0.5291
2025-12-14 20:55:47 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.2734 | Train Acc: 0.4133 | Val Loss: 2.1056 | Val Acc: 0.3358 | LR: 0.000865 | Val F1: 0.4299
2025-12-14 20:55:48 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.2774 | Train Acc: 0.2876 | Val Loss: 2.1090 | Val Acc: 0.5342 | LR: 0.000858 | Val F1: 0.5828
2025-12-14 20:55:50 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.2601 | Train Acc: 0.4002 | Val Loss: 2.0991 | Val Acc: 0.4229 | LR: 0.000850 | Val F1: 0.4993
2025-12-14 20:55:51 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.2497 | Train Acc: 0.3157 | Val Loss: 2.0718 | Val Acc: 0.3020 | LR: 0.000841 | Val F1: 0.3878
2025-12-14 20:55:52 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.2580 | Train Acc: 0.4132 | Val Loss: 1.9739 | Val Acc: 0.4622 | LR: 0.000833 | Val F1: 0.5270
2025-12-14 20:55:54 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.2483 | Train Acc: 0.3064 | Val Loss: 2.0120 | Val Acc: 0.5478 | LR: 0.000823 | Val F1: 0.5733
2025-12-14 20:55:55 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.2631 | Train Acc: 0.3683 | Val Loss: 2.0258 | Val Acc: 0.4151 | LR: 0.000814 | Val F1: 0.4849
2025-12-14 20:55:57 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 2.2470 | Train Acc: 0.3918 | Val Loss: 2.0717 | Val Acc: 0.3485 | LR: 0.000804 | Val F1: 0.4231
2025-12-14 20:55:58 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 2.2429 | Train Acc: 0.3449 | Val Loss: 2.1144 | Val Acc: 0.5780 | LR: 0.000794 | Val F1: 0.6053
2025-12-14 20:56:00 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 2.2375 | Train Acc: 0.4032 | Val Loss: 2.0546 | Val Acc: 0.5498 | LR: 0.000783 | Val F1: 0.5905
2025-12-14 20:56:01 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 2.2391 | Train Acc: 0.4200 | Val Loss: 2.0188 | Val Acc: 0.3359 | LR: 0.000773 | Val F1: 0.4361
2025-12-14 20:56:03 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 2.2327 | Train Acc: 0.2907 | Val Loss: 1.9870 | Val Acc: 0.3590 | LR: 0.000762 | Val F1: 0.4444
2025-12-14 20:56:04 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 2.2386 | Train Acc: 0.3917 | Val Loss: 1.9732 | Val Acc: 0.5076 | LR: 0.000750 | Val F1: 0.5643
2025-12-14 20:56:05 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 2.2174 | Train Acc: 0.3499 | Val Loss: 1.9999 | Val Acc: 0.4157 | LR: 0.000739 | Val F1: 0.4706
2025-12-14 20:56:07 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 2.2348 | Train Acc: 0.3402 | Val Loss: 1.9940 | Val Acc: 0.5273 | LR: 0.000727 | Val F1: 0.5529
2025-12-14 20:56:08 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 2.2364 | Train Acc: 0.3426 | Val Loss: 1.9889 | Val Acc: 0.3642 | LR: 0.000714 | Val F1: 0.4165
2025-12-14 20:56:10 | training | log_epoch | INFO     | Epoch [  34/100] | Train Loss: 2.2109 | Train Acc: 0.3684 | Val Loss: 1.9684 | Val Acc: 0.4154 | LR: 0.000702 | Val F1: 0.4584
2025-12-14 20:56:11 | training | log_epoch | INFO     | Epoch [  35/100] | Train Loss: 2.2285 | Train Acc: 0.3570 | Val Loss: 2.0073 | Val Acc: 0.4233 | LR: 0.000689 | Val F1: 0.4902
2025-12-14 20:56:12 | training | log_epoch | INFO     | Epoch [  36/100] | Train Loss: 2.2225 | Train Acc: 0.3888 | Val Loss: 2.0480 | Val Acc: 0.4255 | LR: 0.000676 | Val F1: 0.5028
2025-12-14 20:56:14 | training | log_epoch | INFO     | Epoch [  37/100] | Train Loss: 2.2083 | Train Acc: 0.3957 | Val Loss: 2.0777 | Val Acc: 0.6352 | LR: 0.000663 | Val F1: 0.6329
2025-12-14 20:56:15 | training | log_epoch | INFO     | Epoch [  38/100] | Train Loss: 2.2149 | Train Acc: 0.3912 | Val Loss: 2.1173 | Val Acc: 0.2447 | LR: 0.000650 | Val F1: 0.3120
2025-12-14 20:56:17 | training | log_epoch | INFO     | Epoch [  39/100] | Train Loss: 2.1937 | Train Acc: 0.2805 | Val Loss: 2.1149 | Val Acc: 0.4516 | LR: 0.000636 | Val F1: 0.5272
2025-12-14 20:56:18 | training | log_epoch | INFO     | Epoch [  40/100] | Train Loss: 2.2146 | Train Acc: 0.4470 | Val Loss: 2.0805 | Val Acc: 0.3289 | LR: 0.000623 | Val F1: 0.4160
2025-12-14 20:56:19 | training | log_epoch | INFO     | Epoch [  41/100] | Train Loss: 2.2278 | Train Acc: 0.3690 | Val Loss: 2.0339 | Val Acc: 0.4866 | LR: 0.000609 | Val F1: 0.5495
2025-12-14 20:56:21 | training | log_epoch | INFO     | Epoch [  42/100] | Train Loss: 2.1858 | Train Acc: 0.3544 | Val Loss: 2.0835 | Val Acc: 0.6259 | LR: 0.000595 | Val F1: 0.6304
2025-12-14 20:56:22 | training | log_epoch | INFO     | Epoch [  43/100] | Train Loss: 2.1987 | Train Acc: 0.4072 | Val Loss: 2.0043 | Val Acc: 0.3992 | LR: 0.000581 | Val F1: 0.4716
2025-12-14 20:56:24 | training | log_epoch | INFO     | Epoch [  44/100] | Train Loss: 2.2131 | Train Acc: 0.4252 | Val Loss: 2.0462 | Val Acc: 0.4686 | LR: 0.000567 | Val F1: 0.5226
2025-12-14 20:56:25 | training | log_epoch | INFO     | Epoch [  45/100] | Train Loss: 2.1985 | Train Acc: 0.3814 | Val Loss: 2.0696 | Val Acc: 0.4043 | LR: 0.000553 | Val F1: 0.4848
2025-12-14 20:56:26 | training | log_epoch | INFO     | Epoch [  46/100] | Train Loss: 2.1972 | Train Acc: 0.3801 | Val Loss: 2.0398 | Val Acc: 0.3849 | LR: 0.000538 | Val F1: 0.4580
2025-12-14 20:56:28 | training | log_epoch | INFO     | Epoch [  47/100] | Train Loss: 2.2099 | Train Acc: 0.4078 | Val Loss: 2.0713 | Val Acc: 0.4547 | LR: 0.000524 | Val F1: 0.5302
2025-12-14 20:56:29 | training | log_epoch | INFO     | Epoch [  48/100] | Train Loss: 2.1921 | Train Acc: 0.3917 | Val Loss: 2.0124 | Val Acc: 0.3331 | LR: 0.000509 | Val F1: 0.3998
2025-12-14 20:56:30 | training | log_epoch | INFO     | Epoch [  49/100] | Train Loss: 2.1792 | Train Acc: 0.3631 | Val Loss: 2.0387 | Val Acc: 0.5998 | LR: 0.000495 | Val F1: 0.6184
2025-12-14 20:56:32 | training | log_epoch | INFO     | Epoch [  50/100] | Train Loss: 2.1819 | Train Acc: 0.4221 | Val Loss: 2.0264 | Val Acc: 0.4905 | LR: 0.000480 | Val F1: 0.5345
2025-12-14 20:56:33 | training | log_epoch | INFO     | Epoch [  51/100] | Train Loss: 2.1827 | Train Acc: 0.3483 | Val Loss: 2.0645 | Val Acc: 0.4030 | LR: 0.000466 | Val F1: 0.4793
2025-12-14 20:56:34 | training | log_epoch | INFO     | Epoch [  52/100] | Train Loss: 2.1825 | Train Acc: 0.3688 | Val Loss: 2.0803 | Val Acc: 0.4099 | LR: 0.000451 | Val F1: 0.4910
2025-12-14 20:56:34 | training | train_model | INFO     | Early stopping triggered at epoch 52 due to no improvement in validation F1.
2025-12-14 20:56:34 | training | train_model | INFO     | Training for Transformer completed in 71.2s. Best validation F1: 0.6329
2025-12-14 20:56:34 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 20:56:34 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:56:34 | training | log_header | INFO     | TRAINING: STATISTICAL BASELINE
2025-12-14 20:56:34 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:56:35 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:56:35 | training | log_header | INFO     | BASELINE TEST SET METRICS
2025-12-14 20:56:35 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:56:35 | training | log_evaluation | INFO     |   accuracy                  : 0.4997
2025-12-14 20:56:35 | training | log_evaluation | INFO     |   f1_weighted               : 0.4644
2025-12-14 20:56:35 | training | log_evaluation | INFO     |   f1_macro                  : 0.1069
2025-12-14 20:56:35 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:56:35 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:56:35 | training | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 20:56:35 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:56:35 | training | train_main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 20:56:35 | training | train_main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 20:56:35 | training | train_main | INFO     | LSTM_v2 (SeqLabeling)     0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 20:56:35 | training | train_main | INFO     | Hierarchical LSTM         0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 20:56:35 | training | train_main | INFO     | Statistical Baseline      0.4997       0.4644          0.1069       0.0000           0.0000            
2025-12-14 20:56:35 | training | train_main | INFO     | Transformer               0.1037       0.0195          0.0269       1.0000           1.0000            
2025-12-14 20:56:35 | training | train_main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_205454.json
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | CONFIGURATION
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 20:58:37 | training | log_config | INFO     |   Epochs                         : 100
2025-12-14 20:58:37 | training | log_config | INFO     |   Device                         : cuda
2025-12-14 20:58:37 | training | log_config | INFO     |   Log Level                      : INFO
2025-12-14 20:58:37 | training | log_config | INFO     |   Augmentation                   : True
2025-12-14 20:58:37 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 20:58:37 | training | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 20:58:37 | training | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 20:58:37 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 20:58:37 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 20:58:37 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 20:58:37 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 20:58:37 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 20:58:37 | training | log_config | INFO     |   patience                       : 15
2025-12-14 20:58:37 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 20:58:37 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_config | INFO     |   hidden_size                    : 96
2025-12-14 20:58:37 | training | log_config | INFO     |   num_layers                     : 2
2025-12-14 20:58:37 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 20:58:37 | training | log_config | INFO     |   bidirectional                  : True
2025-12-14 20:58:37 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 20:58:37 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 20:58:37 | training | log_config | INFO     |   batch_size                     : 16
2025-12-14 20:58:37 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 20:58:37 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 20:58:37 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 20:58:37 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 20:58:37 | training | log_model_summary | INFO     | 
2025-12-14 20:58:37 | training | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 20:58:37 | training | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 20:58:37 | training | log_model_summary | INFO     | 
2025-12-14 20:58:37 | training | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 20:58:37 | training | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 20:58:37 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 20:58:37 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:58:39 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 20:58:39 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 20:58:39 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 20:58:40 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 20:58:40 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 20:58:40 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 20:58:40 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 20:58:40 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 20:58:41 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 20:58:41 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 20:58:41 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 20:58:41 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 20:58:41 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 20:58:42 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 20:58:42 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 20:58:42 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 20:58:42 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 20:58:43 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 20:58:43 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 20:58:43 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 20:58:43 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 20:58:43 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 20:58:44 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 20:58:44 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 20:58:44 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 20:58:44 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 20:58:44 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 20:58:45 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 20:58:45 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 20:58:45 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 20:58:45 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 20:58:46 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 20:58:46 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 20:58:46 | training | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 20:58:46 | training | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 7.5s. Best validation F1: 0.6429
2025-12-14 20:58:46 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 20:58:46 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 20:58:46 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 20:58:46 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_header | INFO     | TRAINING: HIERARCHICAL LSTM
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_header | INFO     | HIERARCHICAL LSTM CONFIG
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 20:58:46 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 20:58:46 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 20:58:46 | training | log_config | INFO     |   patience                       : 15
2025-12-14 20:58:46 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 20:58:46 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_header | INFO     | HIERARCHICAL LSTM BEST HYPERPARAMETERS
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 20:58:46 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 20:58:46 | training | log_config | INFO     |   batch_size                     : 32
2025-12-14 20:58:46 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 20:58:46 | training | log_config | INFO     |   grad_clip                      : 0.5
2025-12-14 20:58:46 | training | log_config | INFO     |   s1_hidden_size                 : 192
2025-12-14 20:58:46 | training | log_config | INFO     |   s1_num_layers                  : 2
2025-12-14 20:58:46 | training | log_config | INFO     |   s1_dropout                     : 0.5
2025-12-14 20:58:46 | training | log_config | INFO     |   s1_bidirectional               : True
2025-12-14 20:58:46 | training | log_config | INFO     |   s2_hidden_size                 : 64
2025-12-14 20:58:46 | training | log_config | INFO     |   s2_num_layers                  : 3
2025-12-14 20:58:46 | training | log_config | INFO     |   s2_dropout                     : 0.3
2025-12-14 20:58:46 | training | log_config | INFO     |   s2_bidirectional               : False
2025-12-14 20:58:46 | training | log_config | INFO     |   use_class_weights              : True
2025-12-14 20:58:46 | training | log_config | INFO     |   class_weight_scale             : 0.6
2025-12-14 20:58:46 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_model_summary | INFO     | Model: HierarchicalClassifier
2025-12-14 20:58:46 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 20:58:46 | training | log_model_summary | INFO     | 
2025-12-14 20:58:46 | training | log_model_summary | INFO     |   stage1                    :  1,193,091 params ( 1,193,091 trainable)
2025-12-14 20:58:46 | training | log_model_summary | INFO     |   stage2                    :     84,675 params (    84,675 trainable)
2025-12-14 20:58:46 | training | log_model_summary | INFO     | 
2025-12-14 20:58:46 | training | log_model_summary | INFO     | Total parameters          :  1,277,766
2025-12-14 20:58:46 | training | log_model_summary | INFO     | Trainable parameters      :  1,277,766
2025-12-14 20:58:46 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 20:58:46 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:58:47 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.3949 | Train Acc: 0.2390 | Val Loss: 1.9860 | Val Acc: 0.2101 | LR: 0.000288 | Val F1: 0.1914
2025-12-14 20:58:47 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.3152 | Train Acc: 0.2117 | Val Loss: 2.0607 | Val Acc: 0.2404 | LR: 0.000288 | Val F1: 0.2729
2025-12-14 20:58:48 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.3001 | Train Acc: 0.1852 | Val Loss: 1.9873 | Val Acc: 0.2655 | LR: 0.000288 | Val F1: 0.3146
2025-12-14 20:58:49 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.2574 | Train Acc: 0.1779 | Val Loss: 1.9836 | Val Acc: 0.2182 | LR: 0.000287 | Val F1: 0.2417
2025-12-14 20:58:50 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.2528 | Train Acc: 0.2352 | Val Loss: 1.9733 | Val Acc: 0.2745 | LR: 0.000287 | Val F1: 0.3110
2025-12-14 20:58:51 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.2219 | Train Acc: 0.2438 | Val Loss: 2.0731 | Val Acc: 0.1727 | LR: 0.000286 | Val F1: 0.1778
2025-12-14 20:58:51 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.2270 | Train Acc: 0.2021 | Val Loss: 2.0306 | Val Acc: 0.3899 | LR: 0.000285 | Val F1: 0.4496
2025-12-14 20:58:52 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.2329 | Train Acc: 0.2830 | Val Loss: 1.9791 | Val Acc: 0.1543 | LR: 0.000285 | Val F1: 0.1320
2025-12-14 20:58:53 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.2236 | Train Acc: 0.2172 | Val Loss: 2.0190 | Val Acc: 0.4240 | LR: 0.000283 | Val F1: 0.4919
2025-12-14 20:58:54 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.2004 | Train Acc: 0.3239 | Val Loss: 1.9722 | Val Acc: 0.1903 | LR: 0.000282 | Val F1: 0.2052
2025-12-14 20:58:55 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.2156 | Train Acc: 0.2425 | Val Loss: 2.0374 | Val Acc: 0.2116 | LR: 0.000281 | Val F1: 0.2280
2025-12-14 20:58:55 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.1825 | Train Acc: 0.2420 | Val Loss: 2.0586 | Val Acc: 0.1573 | LR: 0.000280 | Val F1: 0.1609
2025-12-14 20:58:56 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.1836 | Train Acc: 0.2435 | Val Loss: 1.9628 | Val Acc: 0.2378 | LR: 0.000278 | Val F1: 0.2691
2025-12-14 20:58:57 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.1835 | Train Acc: 0.2686 | Val Loss: 1.9564 | Val Acc: 0.4023 | LR: 0.000276 | Val F1: 0.4581
2025-12-14 20:58:58 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.1889 | Train Acc: 0.2617 | Val Loss: 1.9553 | Val Acc: 0.1901 | LR: 0.000274 | Val F1: 0.2030
2025-12-14 20:58:59 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.1606 | Train Acc: 0.2830 | Val Loss: 2.0343 | Val Acc: 0.2225 | LR: 0.000272 | Val F1: 0.2518
2025-12-14 20:58:59 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.1493 | Train Acc: 0.3194 | Val Loss: 2.0108 | Val Acc: 0.1785 | LR: 0.000270 | Val F1: 0.1895
2025-12-14 20:59:00 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.1637 | Train Acc: 0.2176 | Val Loss: 2.0132 | Val Acc: 0.3490 | LR: 0.000268 | Val F1: 0.4048
2025-12-14 20:59:01 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.1246 | Train Acc: 0.3117 | Val Loss: 1.9923 | Val Acc: 0.2252 | LR: 0.000266 | Val F1: 0.2524
2025-12-14 20:59:02 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.1473 | Train Acc: 0.2826 | Val Loss: 1.9858 | Val Acc: 0.3020 | LR: 0.000263 | Val F1: 0.3570
2025-12-14 20:59:03 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.1418 | Train Acc: 0.2769 | Val Loss: 1.9281 | Val Acc: 0.3245 | LR: 0.000261 | Val F1: 0.3781
2025-12-14 20:59:04 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.1467 | Train Acc: 0.2208 | Val Loss: 2.0474 | Val Acc: 0.3464 | LR: 0.000258 | Val F1: 0.4137
2025-12-14 20:59:05 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.1245 | Train Acc: 0.3384 | Val Loss: 2.0052 | Val Acc: 0.2829 | LR: 0.000255 | Val F1: 0.3234
2025-12-14 20:59:05 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.1411 | Train Acc: 0.3000 | Val Loss: 2.0087 | Val Acc: 0.3490 | LR: 0.000252 | Val F1: 0.4004
2025-12-14 20:59:05 | training | train_model | INFO     | Early stopping triggered at epoch 24 due to no improvement in validation F1.
2025-12-14 20:59:05 | training | train_model | INFO     | Training for Hierarchical LSTM completed in 19.6s. Best validation F1: 0.4919
2025-12-14 20:59:05 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/hierarchical_v1/best_model.pth
2025-12-14 20:59:05 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:05 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 20:59:05 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:05 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 20:59:06 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 20:59:06 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 20:59:06 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 20:59:06 | training | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 20:59:06 | training | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 20:59:06 | training | log_config | INFO     |   patience                       : 15
2025-12-14 20:59:06 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 20:59:06 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_config | INFO     |   d_model                        : 128
2025-12-14 20:59:06 | training | log_config | INFO     |   nhead                          : 4
2025-12-14 20:59:06 | training | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 20:59:06 | training | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 20:59:06 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 20:59:06 | training | log_config | INFO     |   learnable_pe                   : False
2025-12-14 20:59:06 | training | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 20:59:06 | training | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 20:59:06 | training | log_config | INFO     |   batch_size                     : 8
2025-12-14 20:59:06 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 20:59:06 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 20:59:06 | training | log_model_summary | INFO     | 
2025-12-14 20:59:06 | training | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 20:59:06 | training | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 20:59:06 | training | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 20:59:06 | training | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 20:59:06 | training | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 20:59:06 | training | log_model_summary | INFO     | 
2025-12-14 20:59:06 | training | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 20:59:06 | training | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 20:59:06 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 20:59:06 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:59:07 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.7402 | Train Acc: 0.1217 | Val Loss: 2.1374 | Val Acc: 0.2666 | LR: 0.000930 | Val F1: 0.3008
2025-12-14 20:59:08 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.4959 | Train Acc: 0.1478 | Val Loss: 2.1870 | Val Acc: 0.0619 | LR: 0.000930 | Val F1: 0.0309
2025-12-14 20:59:09 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.4098 | Train Acc: 0.1467 | Val Loss: 2.2735 | Val Acc: 0.0118 | LR: 0.000929 | Val F1: 0.0091
2025-12-14 20:59:11 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.3952 | Train Acc: 0.1592 | Val Loss: 2.2967 | Val Acc: 0.5253 | LR: 0.000928 | Val F1: 0.5448
2025-12-14 20:59:12 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.3772 | Train Acc: 0.2387 | Val Loss: 2.0897 | Val Acc: 0.5460 | LR: 0.000926 | Val F1: 0.5629
2025-12-14 20:59:14 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.3351 | Train Acc: 0.2571 | Val Loss: 2.0613 | Val Acc: 0.0963 | LR: 0.000924 | Val F1: 0.0456
2025-12-14 20:59:15 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.3608 | Train Acc: 0.2400 | Val Loss: 2.2677 | Val Acc: 0.4595 | LR: 0.000922 | Val F1: 0.5228
2025-12-14 20:59:16 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.3280 | Train Acc: 0.2571 | Val Loss: 2.0224 | Val Acc: 0.5381 | LR: 0.000919 | Val F1: 0.5652
2025-12-14 20:59:17 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.3256 | Train Acc: 0.2328 | Val Loss: 2.3645 | Val Acc: 0.4981 | LR: 0.000915 | Val F1: 0.5473
2025-12-14 20:59:18 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.3346 | Train Acc: 0.2884 | Val Loss: 2.2241 | Val Acc: 0.0946 | LR: 0.000912 | Val F1: 0.0606
2025-12-14 20:59:20 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.3079 | Train Acc: 0.2946 | Val Loss: 2.1268 | Val Acc: 0.3738 | LR: 0.000907 | Val F1: 0.4477
2025-12-14 20:59:21 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.2992 | Train Acc: 0.3063 | Val Loss: 2.1457 | Val Acc: 0.0781 | LR: 0.000903 | Val F1: 0.0599
2025-12-14 20:59:22 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.2994 | Train Acc: 0.3100 | Val Loss: 2.1047 | Val Acc: 0.1685 | LR: 0.000897 | Val F1: 0.1742
2025-12-14 20:59:24 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.2842 | Train Acc: 0.2777 | Val Loss: 2.0855 | Val Acc: 0.3612 | LR: 0.000892 | Val F1: 0.4269
2025-12-14 20:59:25 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.2711 | Train Acc: 0.3368 | Val Loss: 2.1809 | Val Acc: 0.0872 | LR: 0.000886 | Val F1: 0.1002
2025-12-14 20:59:27 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.2780 | Train Acc: 0.2718 | Val Loss: 2.0283 | Val Acc: 0.2968 | LR: 0.000879 | Val F1: 0.3705
2025-12-14 20:59:28 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.2919 | Train Acc: 0.3768 | Val Loss: 1.9468 | Val Acc: 0.4935 | LR: 0.000873 | Val F1: 0.5291
2025-12-14 20:59:30 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.2734 | Train Acc: 0.4133 | Val Loss: 2.1056 | Val Acc: 0.3358 | LR: 0.000865 | Val F1: 0.4299
2025-12-14 20:59:31 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.2774 | Train Acc: 0.2876 | Val Loss: 2.1090 | Val Acc: 0.5342 | LR: 0.000858 | Val F1: 0.5828
2025-12-14 20:59:32 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.2601 | Train Acc: 0.4002 | Val Loss: 2.0991 | Val Acc: 0.4229 | LR: 0.000850 | Val F1: 0.4993
2025-12-14 20:59:33 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.2497 | Train Acc: 0.3157 | Val Loss: 2.0718 | Val Acc: 0.3020 | LR: 0.000841 | Val F1: 0.3878
2025-12-14 20:59:35 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.2580 | Train Acc: 0.4132 | Val Loss: 1.9739 | Val Acc: 0.4622 | LR: 0.000833 | Val F1: 0.5270
2025-12-14 20:59:36 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.2483 | Train Acc: 0.3064 | Val Loss: 2.0120 | Val Acc: 0.5478 | LR: 0.000823 | Val F1: 0.5733
2025-12-14 20:59:37 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.2631 | Train Acc: 0.3683 | Val Loss: 2.0258 | Val Acc: 0.4151 | LR: 0.000814 | Val F1: 0.4849
2025-12-14 20:59:39 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 2.2470 | Train Acc: 0.3918 | Val Loss: 2.0717 | Val Acc: 0.3485 | LR: 0.000804 | Val F1: 0.4231
2025-12-14 20:59:40 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 2.2429 | Train Acc: 0.3449 | Val Loss: 2.1144 | Val Acc: 0.5780 | LR: 0.000794 | Val F1: 0.6053
2025-12-14 20:59:41 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 2.2375 | Train Acc: 0.4032 | Val Loss: 2.0546 | Val Acc: 0.5498 | LR: 0.000783 | Val F1: 0.5905
2025-12-14 20:59:42 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 2.2391 | Train Acc: 0.4200 | Val Loss: 2.0188 | Val Acc: 0.3359 | LR: 0.000773 | Val F1: 0.4361
2025-12-14 20:59:43 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 2.2327 | Train Acc: 0.2907 | Val Loss: 1.9870 | Val Acc: 0.3590 | LR: 0.000762 | Val F1: 0.4444
2025-12-14 20:59:45 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 2.2386 | Train Acc: 0.3917 | Val Loss: 1.9732 | Val Acc: 0.5076 | LR: 0.000750 | Val F1: 0.5643
2025-12-14 20:59:46 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 2.2174 | Train Acc: 0.3499 | Val Loss: 1.9999 | Val Acc: 0.4157 | LR: 0.000739 | Val F1: 0.4706
2025-12-14 20:59:47 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 2.2348 | Train Acc: 0.3402 | Val Loss: 1.9940 | Val Acc: 0.5273 | LR: 0.000727 | Val F1: 0.5529
2025-12-14 20:59:49 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 2.2364 | Train Acc: 0.3426 | Val Loss: 1.9889 | Val Acc: 0.3642 | LR: 0.000714 | Val F1: 0.4165
2025-12-14 20:59:50 | training | log_epoch | INFO     | Epoch [  34/100] | Train Loss: 2.2109 | Train Acc: 0.3684 | Val Loss: 1.9684 | Val Acc: 0.4154 | LR: 0.000702 | Val F1: 0.4584
2025-12-14 20:59:51 | training | log_epoch | INFO     | Epoch [  35/100] | Train Loss: 2.2285 | Train Acc: 0.3570 | Val Loss: 2.0073 | Val Acc: 0.4233 | LR: 0.000689 | Val F1: 0.4902
2025-12-14 20:59:53 | training | log_epoch | INFO     | Epoch [  36/100] | Train Loss: 2.2225 | Train Acc: 0.3888 | Val Loss: 2.0480 | Val Acc: 0.4255 | LR: 0.000676 | Val F1: 0.5028
2025-12-14 20:59:54 | training | log_epoch | INFO     | Epoch [  37/100] | Train Loss: 2.2083 | Train Acc: 0.3957 | Val Loss: 2.0777 | Val Acc: 0.6352 | LR: 0.000663 | Val F1: 0.6329
2025-12-14 20:59:56 | training | log_epoch | INFO     | Epoch [  38/100] | Train Loss: 2.2149 | Train Acc: 0.3912 | Val Loss: 2.1173 | Val Acc: 0.2447 | LR: 0.000650 | Val F1: 0.3120
2025-12-14 20:59:57 | training | log_epoch | INFO     | Epoch [  39/100] | Train Loss: 2.1937 | Train Acc: 0.2805 | Val Loss: 2.1149 | Val Acc: 0.4516 | LR: 0.000636 | Val F1: 0.5272
2025-12-14 20:59:58 | training | log_epoch | INFO     | Epoch [  40/100] | Train Loss: 2.2146 | Train Acc: 0.4470 | Val Loss: 2.0805 | Val Acc: 0.3289 | LR: 0.000623 | Val F1: 0.4160
2025-12-14 21:00:00 | training | log_epoch | INFO     | Epoch [  41/100] | Train Loss: 2.2278 | Train Acc: 0.3690 | Val Loss: 2.0339 | Val Acc: 0.4866 | LR: 0.000609 | Val F1: 0.5495
2025-12-14 21:00:01 | training | log_epoch | INFO     | Epoch [  42/100] | Train Loss: 2.1858 | Train Acc: 0.3544 | Val Loss: 2.0835 | Val Acc: 0.6259 | LR: 0.000595 | Val F1: 0.6304
2025-12-14 21:00:02 | training | log_epoch | INFO     | Epoch [  43/100] | Train Loss: 2.1987 | Train Acc: 0.4072 | Val Loss: 2.0043 | Val Acc: 0.3992 | LR: 0.000581 | Val F1: 0.4716
2025-12-14 21:00:04 | training | log_epoch | INFO     | Epoch [  44/100] | Train Loss: 2.2131 | Train Acc: 0.4252 | Val Loss: 2.0462 | Val Acc: 0.4686 | LR: 0.000567 | Val F1: 0.5226
2025-12-14 21:00:05 | training | log_epoch | INFO     | Epoch [  45/100] | Train Loss: 2.1985 | Train Acc: 0.3814 | Val Loss: 2.0696 | Val Acc: 0.4043 | LR: 0.000553 | Val F1: 0.4848
2025-12-14 21:00:06 | training | log_epoch | INFO     | Epoch [  46/100] | Train Loss: 2.1972 | Train Acc: 0.3801 | Val Loss: 2.0399 | Val Acc: 0.3849 | LR: 0.000538 | Val F1: 0.4580
2025-12-14 21:00:08 | training | log_epoch | INFO     | Epoch [  47/100] | Train Loss: 2.2099 | Train Acc: 0.4077 | Val Loss: 2.0713 | Val Acc: 0.4548 | LR: 0.000524 | Val F1: 0.5303
2025-12-14 21:00:09 | training | log_epoch | INFO     | Epoch [  48/100] | Train Loss: 2.1921 | Train Acc: 0.3916 | Val Loss: 2.0124 | Val Acc: 0.3331 | LR: 0.000509 | Val F1: 0.3998
2025-12-14 21:00:10 | training | log_epoch | INFO     | Epoch [  49/100] | Train Loss: 2.1792 | Train Acc: 0.3631 | Val Loss: 2.0387 | Val Acc: 0.5998 | LR: 0.000495 | Val F1: 0.6184
2025-12-14 21:00:12 | training | log_epoch | INFO     | Epoch [  50/100] | Train Loss: 2.1819 | Train Acc: 0.4221 | Val Loss: 2.0264 | Val Acc: 0.4905 | LR: 0.000480 | Val F1: 0.5345
2025-12-14 21:00:13 | training | log_epoch | INFO     | Epoch [  51/100] | Train Loss: 2.1827 | Train Acc: 0.3483 | Val Loss: 2.0645 | Val Acc: 0.4029 | LR: 0.000466 | Val F1: 0.4792
2025-12-14 21:00:14 | training | log_epoch | INFO     | Epoch [  52/100] | Train Loss: 2.1825 | Train Acc: 0.3688 | Val Loss: 2.0803 | Val Acc: 0.4099 | LR: 0.000451 | Val F1: 0.4910
2025-12-14 21:00:14 | training | train_model | INFO     | Early stopping triggered at epoch 52 due to no improvement in validation F1.
2025-12-14 21:00:14 | training | train_model | INFO     | Training for Transformer completed in 68.8s. Best validation F1: 0.6329
2025-12-14 21:00:14 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 21:00:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 21:00:14 | training | log_header | INFO     | TRAINING: STATISTICAL BASELINE
2025-12-14 21:00:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 21:00:15 | training | log_separator | INFO     | ======================================================================
2025-12-14 21:00:15 | training | log_header | INFO     | BASELINE TEST SET METRICS
2025-12-14 21:00:15 | training | log_separator | INFO     | ======================================================================
2025-12-14 21:00:15 | training | log_evaluation | INFO     |   accuracy                  : 0.4997
2025-12-14 21:00:15 | training | log_evaluation | INFO     |   f1_weighted               : 0.4644
2025-12-14 21:00:15 | training | log_evaluation | INFO     |   f1_macro                  : 0.1069
2025-12-14 21:00:15 | training | log_separator | INFO     | ======================================================================
2025-12-14 21:00:15 | training | log_separator | INFO     | ======================================================================
2025-12-14 21:00:15 | training | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 21:00:15 | training | log_separator | INFO     | ======================================================================
2025-12-14 21:00:15 | training | train_main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 21:00:15 | training | train_main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 21:00:15 | training | train_main | INFO     | LSTM_v2 (SeqLabeling)     0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 21:00:15 | training | train_main | INFO     | Hierarchical LSTM         0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 21:00:15 | training | train_main | INFO     | Statistical Baseline      0.4997       0.4644          0.1069       0.0000           0.0000            
2025-12-14 21:00:15 | training | train_main | INFO     | Transformer               0.1037       0.0195          0.0269       1.0000           1.0000            
2025-12-14 21:00:15 | training | train_main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_205837.json
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | CONFIGURATION
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 22:09:59 | training | log_config | INFO     |   Epochs                         : 100
2025-12-14 22:09:59 | training | log_config | INFO     |   Device                         : cuda
2025-12-14 22:09:59 | training | log_config | INFO     |   Log Level                      : INFO
2025-12-14 22:09:59 | training | log_config | INFO     |   Augmentation                   : True
2025-12-14 22:09:59 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 22:09:59 | training | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:09:59 | training | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:09:59 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:09:59 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:09:59 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:09:59 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 22:09:59 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 22:09:59 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:09:59 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:09:59 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_config | INFO     |   hidden_size                    : 96
2025-12-14 22:09:59 | training | log_config | INFO     |   num_layers                     : 2
2025-12-14 22:09:59 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 22:09:59 | training | log_config | INFO     |   bidirectional                  : True
2025-12-14 22:09:59 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 22:09:59 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 22:09:59 | training | log_config | INFO     |   batch_size                     : 16
2025-12-14 22:09:59 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 22:09:59 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 22:09:59 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 22:09:59 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:09:59 | training | log_model_summary | INFO     | 
2025-12-14 22:09:59 | training | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 22:09:59 | training | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 22:09:59 | training | log_model_summary | INFO     | 
2025-12-14 22:09:59 | training | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 22:09:59 | training | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 22:09:59 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:09:59 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:10:01 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 22:10:01 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 22:10:01 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 22:10:02 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 22:10:02 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 22:10:03 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 22:10:03 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 22:10:03 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 22:10:04 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 22:10:04 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 22:10:04 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 22:10:05 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 22:10:05 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 22:10:06 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 22:10:06 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 22:10:06 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 22:10:07 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 22:10:07 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 22:10:08 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 22:10:08 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 22:10:09 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 22:10:09 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 22:10:10 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 22:10:10 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 22:10:11 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 22:10:11 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 22:10:11 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 22:10:12 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 22:10:12 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 22:10:13 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 22:10:13 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 22:10:14 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 22:10:14 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 22:10:14 | training | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 22:10:14 | training | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 14.3s. Best validation F1: 0.6429
2025-12-14 22:10:14 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:10:14 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:10:14 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:10:14 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_header | INFO     | TRAINING: HIERARCHICAL LSTM
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_header | INFO     | HIERARCHICAL LSTM CONFIG
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:10:14 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 22:10:14 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 22:10:14 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:10:14 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:10:14 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_header | INFO     | HIERARCHICAL LSTM BEST HYPERPARAMETERS
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 22:10:14 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 22:10:14 | training | log_config | INFO     |   batch_size                     : 32
2025-12-14 22:10:14 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 22:10:14 | training | log_config | INFO     |   grad_clip                      : 0.5
2025-12-14 22:10:14 | training | log_config | INFO     |   s1_hidden_size                 : 192
2025-12-14 22:10:14 | training | log_config | INFO     |   s1_num_layers                  : 2
2025-12-14 22:10:14 | training | log_config | INFO     |   s1_dropout                     : 0.5
2025-12-14 22:10:14 | training | log_config | INFO     |   s1_bidirectional               : True
2025-12-14 22:10:14 | training | log_config | INFO     |   s2_hidden_size                 : 64
2025-12-14 22:10:14 | training | log_config | INFO     |   s2_num_layers                  : 3
2025-12-14 22:10:14 | training | log_config | INFO     |   s2_dropout                     : 0.3
2025-12-14 22:10:14 | training | log_config | INFO     |   s2_bidirectional               : False
2025-12-14 22:10:14 | training | log_config | INFO     |   use_class_weights              : True
2025-12-14 22:10:14 | training | log_config | INFO     |   class_weight_scale             : 0.6
2025-12-14 22:10:14 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_model_summary | INFO     | Model: HierarchicalClassifier
2025-12-14 22:10:14 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:10:14 | training | log_model_summary | INFO     | 
2025-12-14 22:10:14 | training | log_model_summary | INFO     |   stage1                    :  1,193,091 params ( 1,193,091 trainable)
2025-12-14 22:10:14 | training | log_model_summary | INFO     |   stage2                    :     84,675 params (    84,675 trainable)
2025-12-14 22:10:14 | training | log_model_summary | INFO     | 
2025-12-14 22:10:14 | training | log_model_summary | INFO     | Total parameters          :  1,277,766
2025-12-14 22:10:14 | training | log_model_summary | INFO     | Trainable parameters      :  1,277,766
2025-12-14 22:10:14 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:10:14 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:10:16 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.3949 | Train Acc: 0.2390 | Val Loss: 1.9860 | Val Acc: 0.2101 | LR: 0.000288 | Val F1: 0.1914
2025-12-14 22:10:17 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.3152 | Train Acc: 0.2117 | Val Loss: 2.0607 | Val Acc: 0.2404 | LR: 0.000288 | Val F1: 0.2729
2025-12-14 22:10:18 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.3001 | Train Acc: 0.1852 | Val Loss: 1.9873 | Val Acc: 0.2655 | LR: 0.000288 | Val F1: 0.3146
2025-12-14 22:10:20 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.2574 | Train Acc: 0.1779 | Val Loss: 1.9836 | Val Acc: 0.2182 | LR: 0.000287 | Val F1: 0.2417
2025-12-14 22:10:21 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.2528 | Train Acc: 0.2352 | Val Loss: 1.9733 | Val Acc: 0.2745 | LR: 0.000287 | Val F1: 0.3110
2025-12-14 22:10:22 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.2219 | Train Acc: 0.2438 | Val Loss: 2.0731 | Val Acc: 0.1727 | LR: 0.000286 | Val F1: 0.1778
2025-12-14 22:10:23 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.2270 | Train Acc: 0.2021 | Val Loss: 2.0306 | Val Acc: 0.3899 | LR: 0.000285 | Val F1: 0.4496
2025-12-14 22:10:25 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.2329 | Train Acc: 0.2830 | Val Loss: 1.9791 | Val Acc: 0.1543 | LR: 0.000285 | Val F1: 0.1320
2025-12-14 22:10:26 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.2236 | Train Acc: 0.2172 | Val Loss: 2.0190 | Val Acc: 0.4240 | LR: 0.000283 | Val F1: 0.4919
2025-12-14 22:10:27 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.2004 | Train Acc: 0.3239 | Val Loss: 1.9722 | Val Acc: 0.1903 | LR: 0.000282 | Val F1: 0.2052
2025-12-14 22:10:29 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.2156 | Train Acc: 0.2425 | Val Loss: 2.0374 | Val Acc: 0.2116 | LR: 0.000281 | Val F1: 0.2280
2025-12-14 22:10:30 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.1825 | Train Acc: 0.2420 | Val Loss: 2.0586 | Val Acc: 0.1573 | LR: 0.000280 | Val F1: 0.1609
2025-12-14 22:10:31 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.1836 | Train Acc: 0.2435 | Val Loss: 1.9628 | Val Acc: 0.2378 | LR: 0.000278 | Val F1: 0.2691
2025-12-14 22:10:32 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.1835 | Train Acc: 0.2686 | Val Loss: 1.9564 | Val Acc: 0.4023 | LR: 0.000276 | Val F1: 0.4581
2025-12-14 22:10:34 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.1889 | Train Acc: 0.2617 | Val Loss: 1.9553 | Val Acc: 0.1901 | LR: 0.000274 | Val F1: 0.2030
2025-12-14 22:10:35 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.1606 | Train Acc: 0.2830 | Val Loss: 2.0343 | Val Acc: 0.2225 | LR: 0.000272 | Val F1: 0.2518
2025-12-14 22:10:36 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.1493 | Train Acc: 0.3194 | Val Loss: 2.0108 | Val Acc: 0.1785 | LR: 0.000270 | Val F1: 0.1895
2025-12-14 22:10:38 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.1637 | Train Acc: 0.2176 | Val Loss: 2.0132 | Val Acc: 0.3490 | LR: 0.000268 | Val F1: 0.4048
2025-12-14 22:10:39 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.1246 | Train Acc: 0.3117 | Val Loss: 1.9923 | Val Acc: 0.2252 | LR: 0.000266 | Val F1: 0.2524
2025-12-14 22:10:40 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.1473 | Train Acc: 0.2826 | Val Loss: 1.9858 | Val Acc: 0.3020 | LR: 0.000263 | Val F1: 0.3570
2025-12-14 22:10:41 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.1418 | Train Acc: 0.2769 | Val Loss: 1.9281 | Val Acc: 0.3245 | LR: 0.000261 | Val F1: 0.3781
2025-12-14 22:10:43 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.1467 | Train Acc: 0.2208 | Val Loss: 2.0474 | Val Acc: 0.3464 | LR: 0.000258 | Val F1: 0.4137
2025-12-14 22:10:44 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.1245 | Train Acc: 0.3384 | Val Loss: 2.0052 | Val Acc: 0.2829 | LR: 0.000255 | Val F1: 0.3234
2025-12-14 22:10:45 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.1411 | Train Acc: 0.3000 | Val Loss: 2.0087 | Val Acc: 0.3490 | LR: 0.000252 | Val F1: 0.4004
2025-12-14 22:10:45 | training | train_model | INFO     | Early stopping triggered at epoch 24 due to no improvement in validation F1.
2025-12-14 22:10:45 | training | train_model | INFO     | Training for Hierarchical LSTM completed in 31.0s. Best validation F1: 0.4919
2025-12-14 22:10:45 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/hierarchical_v1/best_model.pth
2025-12-14 22:10:45 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:45 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:10:45 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:45 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:10:45 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:10:45 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:10:45 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:10:46 | training | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 22:10:46 | training | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 22:10:46 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:10:46 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:10:46 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_config | INFO     |   d_model                        : 128
2025-12-14 22:10:46 | training | log_config | INFO     |   nhead                          : 4
2025-12-14 22:10:46 | training | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 22:10:46 | training | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 22:10:46 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 22:10:46 | training | log_config | INFO     |   learnable_pe                   : False
2025-12-14 22:10:46 | training | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 22:10:46 | training | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 22:10:46 | training | log_config | INFO     |   batch_size                     : 8
2025-12-14 22:10:46 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 22:10:46 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:10:46 | training | log_model_summary | INFO     | 
2025-12-14 22:10:46 | training | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 22:10:46 | training | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 22:10:46 | training | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 22:10:46 | training | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 22:10:46 | training | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 22:10:46 | training | log_model_summary | INFO     | 
2025-12-14 22:10:46 | training | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 22:10:46 | training | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 22:10:46 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:10:46 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:10:47 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.7402 | Train Acc: 0.1217 | Val Loss: 2.1374 | Val Acc: 0.2666 | LR: 0.000930 | Val F1: 0.3008
2025-12-14 22:10:49 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.4959 | Train Acc: 0.1478 | Val Loss: 2.1870 | Val Acc: 0.0619 | LR: 0.000930 | Val F1: 0.0309
2025-12-14 22:10:51 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.4098 | Train Acc: 0.1467 | Val Loss: 2.2735 | Val Acc: 0.0118 | LR: 0.000929 | Val F1: 0.0091
2025-12-14 22:10:52 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.3952 | Train Acc: 0.1592 | Val Loss: 2.2967 | Val Acc: 0.5253 | LR: 0.000928 | Val F1: 0.5448
2025-12-14 22:10:54 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.3772 | Train Acc: 0.2387 | Val Loss: 2.0897 | Val Acc: 0.5460 | LR: 0.000926 | Val F1: 0.5629
2025-12-14 22:10:56 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.3351 | Train Acc: 0.2571 | Val Loss: 2.0613 | Val Acc: 0.0963 | LR: 0.000924 | Val F1: 0.0456
2025-12-14 22:10:57 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.3608 | Train Acc: 0.2400 | Val Loss: 2.2677 | Val Acc: 0.4595 | LR: 0.000922 | Val F1: 0.5228
2025-12-14 22:10:59 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.3280 | Train Acc: 0.2571 | Val Loss: 2.0224 | Val Acc: 0.5381 | LR: 0.000919 | Val F1: 0.5652
2025-12-14 22:11:01 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.3256 | Train Acc: 0.2328 | Val Loss: 2.3645 | Val Acc: 0.4981 | LR: 0.000915 | Val F1: 0.5473
2025-12-14 22:11:02 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.3346 | Train Acc: 0.2884 | Val Loss: 2.2241 | Val Acc: 0.0946 | LR: 0.000912 | Val F1: 0.0606
2025-12-14 22:11:04 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.3079 | Train Acc: 0.2946 | Val Loss: 2.1268 | Val Acc: 0.3738 | LR: 0.000907 | Val F1: 0.4477
2025-12-14 22:11:06 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.2992 | Train Acc: 0.3063 | Val Loss: 2.1457 | Val Acc: 0.0781 | LR: 0.000903 | Val F1: 0.0599
2025-12-14 22:11:08 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.2994 | Train Acc: 0.3100 | Val Loss: 2.1047 | Val Acc: 0.1685 | LR: 0.000897 | Val F1: 0.1742
2025-12-14 22:11:09 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.2842 | Train Acc: 0.2777 | Val Loss: 2.0855 | Val Acc: 0.3612 | LR: 0.000892 | Val F1: 0.4269
2025-12-14 22:11:10 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.2711 | Train Acc: 0.3368 | Val Loss: 2.1809 | Val Acc: 0.0872 | LR: 0.000886 | Val F1: 0.1002
2025-12-14 22:11:11 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.2780 | Train Acc: 0.2718 | Val Loss: 2.0283 | Val Acc: 0.2968 | LR: 0.000879 | Val F1: 0.3705
2025-12-14 22:11:13 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.2919 | Train Acc: 0.3768 | Val Loss: 1.9468 | Val Acc: 0.4935 | LR: 0.000873 | Val F1: 0.5291
2025-12-14 22:11:14 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.2734 | Train Acc: 0.4133 | Val Loss: 2.1056 | Val Acc: 0.3358 | LR: 0.000865 | Val F1: 0.4299
2025-12-14 22:11:15 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.2774 | Train Acc: 0.2876 | Val Loss: 2.1090 | Val Acc: 0.5342 | LR: 0.000858 | Val F1: 0.5828
2025-12-14 22:11:16 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.2601 | Train Acc: 0.4002 | Val Loss: 2.0991 | Val Acc: 0.4229 | LR: 0.000850 | Val F1: 0.4993
2025-12-14 22:11:17 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.2497 | Train Acc: 0.3157 | Val Loss: 2.0718 | Val Acc: 0.3020 | LR: 0.000841 | Val F1: 0.3878
2025-12-14 22:11:18 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.2580 | Train Acc: 0.4133 | Val Loss: 1.9739 | Val Acc: 0.4622 | LR: 0.000833 | Val F1: 0.5270
2025-12-14 22:11:20 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.2483 | Train Acc: 0.3064 | Val Loss: 2.0120 | Val Acc: 0.5478 | LR: 0.000823 | Val F1: 0.5733
2025-12-14 22:11:21 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.2631 | Train Acc: 0.3683 | Val Loss: 2.0258 | Val Acc: 0.4151 | LR: 0.000814 | Val F1: 0.4849
2025-12-14 22:11:22 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 2.2470 | Train Acc: 0.3918 | Val Loss: 2.0717 | Val Acc: 0.3485 | LR: 0.000804 | Val F1: 0.4231
2025-12-14 22:11:23 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 2.2429 | Train Acc: 0.3449 | Val Loss: 2.1144 | Val Acc: 0.5780 | LR: 0.000794 | Val F1: 0.6053
2025-12-14 22:11:25 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 2.2375 | Train Acc: 0.4032 | Val Loss: 2.0546 | Val Acc: 0.5498 | LR: 0.000783 | Val F1: 0.5905
2025-12-14 22:11:26 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 2.2391 | Train Acc: 0.4200 | Val Loss: 2.0188 | Val Acc: 0.3359 | LR: 0.000773 | Val F1: 0.4361
2025-12-14 22:11:27 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 2.2327 | Train Acc: 0.2907 | Val Loss: 1.9870 | Val Acc: 0.3590 | LR: 0.000762 | Val F1: 0.4444
2025-12-14 22:11:28 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 2.2386 | Train Acc: 0.3917 | Val Loss: 1.9732 | Val Acc: 0.5076 | LR: 0.000750 | Val F1: 0.5643
2025-12-14 22:11:29 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 2.2174 | Train Acc: 0.3499 | Val Loss: 1.9999 | Val Acc: 0.4157 | LR: 0.000739 | Val F1: 0.4706
2025-12-14 22:11:31 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 2.2348 | Train Acc: 0.3402 | Val Loss: 1.9940 | Val Acc: 0.5273 | LR: 0.000727 | Val F1: 0.5529
2025-12-14 22:11:32 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 2.2364 | Train Acc: 0.3426 | Val Loss: 1.9889 | Val Acc: 0.3642 | LR: 0.000714 | Val F1: 0.4165
2025-12-14 22:11:33 | training | log_epoch | INFO     | Epoch [  34/100] | Train Loss: 2.2109 | Train Acc: 0.3684 | Val Loss: 1.9684 | Val Acc: 0.4154 | LR: 0.000702 | Val F1: 0.4584
2025-12-14 22:11:35 | training | log_epoch | INFO     | Epoch [  35/100] | Train Loss: 2.2285 | Train Acc: 0.3570 | Val Loss: 2.0073 | Val Acc: 0.4233 | LR: 0.000689 | Val F1: 0.4902
2025-12-14 22:11:36 | training | log_epoch | INFO     | Epoch [  36/100] | Train Loss: 2.2225 | Train Acc: 0.3888 | Val Loss: 2.0480 | Val Acc: 0.4255 | LR: 0.000676 | Val F1: 0.5028
2025-12-14 22:11:38 | training | log_epoch | INFO     | Epoch [  37/100] | Train Loss: 2.2083 | Train Acc: 0.3957 | Val Loss: 2.0777 | Val Acc: 0.6352 | LR: 0.000663 | Val F1: 0.6329
2025-12-14 22:11:39 | training | log_epoch | INFO     | Epoch [  38/100] | Train Loss: 2.2149 | Train Acc: 0.3912 | Val Loss: 2.1173 | Val Acc: 0.2447 | LR: 0.000650 | Val F1: 0.3120
2025-12-14 22:11:41 | training | log_epoch | INFO     | Epoch [  39/100] | Train Loss: 2.1937 | Train Acc: 0.2806 | Val Loss: 2.1149 | Val Acc: 0.4516 | LR: 0.000636 | Val F1: 0.5272
2025-12-14 22:11:42 | training | log_epoch | INFO     | Epoch [  40/100] | Train Loss: 2.2146 | Train Acc: 0.4470 | Val Loss: 2.0805 | Val Acc: 0.3289 | LR: 0.000623 | Val F1: 0.4160
2025-12-14 22:11:44 | training | log_epoch | INFO     | Epoch [  41/100] | Train Loss: 2.2278 | Train Acc: 0.3691 | Val Loss: 2.0339 | Val Acc: 0.4866 | LR: 0.000609 | Val F1: 0.5495
2025-12-14 22:11:45 | training | log_epoch | INFO     | Epoch [  42/100] | Train Loss: 2.1858 | Train Acc: 0.3544 | Val Loss: 2.0835 | Val Acc: 0.6259 | LR: 0.000595 | Val F1: 0.6304
2025-12-14 22:11:47 | training | log_epoch | INFO     | Epoch [  43/100] | Train Loss: 2.1987 | Train Acc: 0.4072 | Val Loss: 2.0043 | Val Acc: 0.3995 | LR: 0.000581 | Val F1: 0.4717
2025-12-14 22:11:48 | training | log_epoch | INFO     | Epoch [  44/100] | Train Loss: 2.2131 | Train Acc: 0.4252 | Val Loss: 2.0462 | Val Acc: 0.4686 | LR: 0.000567 | Val F1: 0.5226
2025-12-14 22:11:50 | training | log_epoch | INFO     | Epoch [  45/100] | Train Loss: 2.1985 | Train Acc: 0.3813 | Val Loss: 2.0696 | Val Acc: 0.4043 | LR: 0.000553 | Val F1: 0.4848
2025-12-14 22:11:51 | training | log_epoch | INFO     | Epoch [  46/100] | Train Loss: 2.1972 | Train Acc: 0.3801 | Val Loss: 2.0399 | Val Acc: 0.3846 | LR: 0.000538 | Val F1: 0.4578
2025-12-14 22:11:52 | training | log_epoch | INFO     | Epoch [  47/100] | Train Loss: 2.2099 | Train Acc: 0.4079 | Val Loss: 2.0713 | Val Acc: 0.4544 | LR: 0.000524 | Val F1: 0.5302
2025-12-14 22:11:54 | training | log_epoch | INFO     | Epoch [  48/100] | Train Loss: 2.1923 | Train Acc: 0.3918 | Val Loss: 2.0124 | Val Acc: 0.3307 | LR: 0.000509 | Val F1: 0.3972
2025-12-14 22:11:55 | training | log_epoch | INFO     | Epoch [  49/100] | Train Loss: 2.1793 | Train Acc: 0.3632 | Val Loss: 2.0390 | Val Acc: 0.5998 | LR: 0.000495 | Val F1: 0.6186
2025-12-14 22:11:56 | training | log_epoch | INFO     | Epoch [  50/100] | Train Loss: 2.1819 | Train Acc: 0.4222 | Val Loss: 2.0264 | Val Acc: 0.4900 | LR: 0.000480 | Val F1: 0.5341
2025-12-14 22:11:58 | training | log_epoch | INFO     | Epoch [  51/100] | Train Loss: 2.1827 | Train Acc: 0.3482 | Val Loss: 2.0647 | Val Acc: 0.4014 | LR: 0.000466 | Val F1: 0.4779
2025-12-14 22:11:59 | training | log_epoch | INFO     | Epoch [  52/100] | Train Loss: 2.1825 | Train Acc: 0.3686 | Val Loss: 2.0805 | Val Acc: 0.4089 | LR: 0.000451 | Val F1: 0.4901
2025-12-14 22:11:59 | training | train_model | INFO     | Early stopping triggered at epoch 52 due to no improvement in validation F1.
2025-12-14 22:11:59 | training | train_model | INFO     | Training for Transformer completed in 73.2s. Best validation F1: 0.6329
2025-12-14 22:11:59 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 22:11:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:11:59 | training | log_header | INFO     | TRAINING: STATISTICAL BASELINE
2025-12-14 22:11:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:12:00 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:12:00 | training | log_header | INFO     | BASELINE TEST SET METRICS
2025-12-14 22:12:00 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:12:00 | training | log_evaluation | INFO     |   accuracy                  : 0.4997
2025-12-14 22:12:00 | training | log_evaluation | INFO     |   f1_weighted               : 0.4644
2025-12-14 22:12:00 | training | log_evaluation | INFO     |   f1_macro                  : 0.1069
2025-12-14 22:12:00 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:12:00 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:12:00 | training | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 22:12:00 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:12:00 | training | train_main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 22:12:00 | training | train_main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 22:12:00 | training | train_main | INFO     | LSTM_v2 (SeqLabeling)     0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 22:12:00 | training | train_main | INFO     | Hierarchical LSTM         0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 22:12:00 | training | train_main | INFO     | Statistical Baseline      0.4997       0.4644          0.1069       0.0000           0.0000            
2025-12-14 22:12:00 | training | train_main | INFO     | Transformer               0.1037       0.0195          0.0269       1.0000           1.0000            
2025-12-14 22:12:00 | training | train_main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_220959.json
