Unable to find image 'dl-project:latest' locally
docker: Error response from daemon: pull access denied for dl-project, repository does not exist or may require 'docker login': denied: requested access to the resource is denied

Run 'docker run --help' for more information
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | CONFIGURATION
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   Epochs                         : 100
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   Device                         : cuda
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   Log Level                      : INFO
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   Augmentation                   : True
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | DATA LOADING
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | load_data | INFO     | Loading data from /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/data
2025-12-14 19:18:06 | __main__ | load_data | INFO     | Data loaded successfully. X shape: (300, 256, 4), Y shape: (300, 256)
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | DATA PREPARATION
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | prepare_dataloaders | INFO     | Created new stratified split: train=210, val=45, test=45
2025-12-14 19:18:06 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler.
2025-12-14 19:18:06 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:18:06 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:18:06 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   hidden_size                    : 96
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   num_layers                     : 2
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   dropout                        : 0.5
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   bidirectional                  : True
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   batch_size                     : 16
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   scheduler                      : cosine
2025-12-14 19:18:06 | __main__ | log_config | INFO     |   use_class_weights              : False
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 19:18:06 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:18:06 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:18:07 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 19:18:08 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 19:18:08 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 19:18:08 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 19:18:08 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 19:18:09 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 19:18:09 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 19:18:09 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 19:18:09 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 19:18:10 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 19:18:10 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 19:18:10 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 19:18:10 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 19:18:11 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 19:18:11 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 19:18:11 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 19:18:11 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 19:18:12 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 19:18:12 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 19:18:12 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 19:18:12 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 19:18:12 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 19:18:13 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 19:18:13 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 19:18:13 | __main__ | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 19:18:13 | __main__ | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 19:18:14 | __main__ | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 19:18:14 | __main__ | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 19:18:14 | __main__ | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 19:18:14 | __main__ | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 19:18:15 | __main__ | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 19:18:15 | __main__ | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 19:18:15 | __main__ | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 19:18:15 | __main__ | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 19:18:15 | __main__ | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 8.4s. Best validation F1: 0.6429
2025-12-14 19:18:15 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:15 | __main__ | log_header | INFO     | EVALUATION: LSTM_V2 (SEQLABELING)
2025-12-14 19:18:15 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:15 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:15 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) TEST SET METRICS
2025-12-14 19:18:15 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:15 | __main__ | log_evaluation | INFO     |   accuracy                  : 0.6703
2025-12-14 19:18:15 | __main__ | log_evaluation | INFO     |   f1_weighted               : 0.5380
2025-12-14 19:18:15 | __main__ | log_evaluation | INFO     |   f1_macro                  : 0.1147
2025-12-14 19:18:15 | __main__ | log_evaluation | INFO     |   detection_rate            : 0.0000
2025-12-14 19:18:15 | __main__ | log_evaluation | INFO     |   false_alarm_rate          : 0.0000
2025-12-14 19:18:15 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | Confusion Matrix:
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     |                 0        1        2        3        4        5        6
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 0           7722        0        0        0        0        0        0
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 1           1195        0        0        0        0        0        0
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 2            742        0        0        0        0        0        0
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 3            342        0        0        0        0        0        0
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 4            966        0        0        0        0        0        0
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 5            510        0        0        0        0        0        0
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 6             43        0        0        0        0        0        0
2025-12-14 19:18:15 | __main__ | log_confusion_matrix | INFO     | 
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     | Classification Report:
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |                 precision    recall  f1-score   support
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |   
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |              0       0.67      1.00      0.80      7722
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |              1       0.00      0.00      0.00      1195
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |              2       0.00      0.00      0.00       742
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |              3       0.00      0.00      0.00       342
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |              4       0.00      0.00      0.00       966
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |              5       0.00      0.00      0.00       510
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |              6       0.00      0.00      0.00        43
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |   
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |       accuracy                           0.67     11520
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |      macro avg       0.10      0.14      0.11     11520
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |   weighted avg       0.45      0.67      0.54     11520
2025-12-14 19:18:15 | __main__ | log_class_report | INFO     |   
2025-12-14 19:18:15 | __main__ | main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 19:18:15 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:15 | __main__ | log_header | INFO     | DATA PREPARATION
2025-12-14 19:18:15 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:18:15 | __main__ | prepare_dataloaders | INFO     | Created new stratified split: train=210, val=45, test=45
2025-12-14 19:18:15 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler.
2025-12-14 19:18:15 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:18:15 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:18:15 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | CONFIGURATION
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   Epochs                         : 100
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   Device                         : cuda
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   Log Level                      : INFO
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   Augmentation                   : True
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | DATA LOADING
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | load_data | INFO     | Loading data from /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/data
2025-12-14 19:20:10 | __main__ | load_data | INFO     | Data loaded successfully. X shape: (300, 256, 4), Y shape: (300, 256)
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | DATA PREPARATION
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | prepare_dataloaders | INFO     | Created new stratified split: train=210, val=45, test=45
2025-12-14 19:20:10 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler.
2025-12-14 19:20:10 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:20:10 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:20:10 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   hidden_size                    : 96
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   num_layers                     : 2
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   dropout                        : 0.5
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   bidirectional                  : True
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   batch_size                     : 16
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   scheduler                      : cosine
2025-12-14 19:20:10 | __main__ | log_config | INFO     |   use_class_weights              : False
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 19:20:10 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:20:10 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:12 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 19:20:12 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 19:20:12 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 19:20:13 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 19:20:13 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 19:20:13 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 19:20:13 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 19:20:14 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 19:20:14 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 19:20:14 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 19:20:14 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 19:20:15 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 19:20:15 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 19:20:15 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 19:20:15 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 19:20:16 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 19:20:16 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 19:20:16 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 19:20:16 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 19:20:17 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 19:20:17 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 19:20:17 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 19:20:17 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 19:20:18 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 19:20:18 | __main__ | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 19:20:18 | __main__ | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 19:20:18 | __main__ | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 19:20:19 | __main__ | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 19:20:19 | __main__ | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 19:20:19 | __main__ | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 19:20:19 | __main__ | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 19:20:20 | __main__ | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 19:20:20 | __main__ | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 19:20:20 | __main__ | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 19:20:20 | __main__ | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 8.4s. Best validation F1: 0.6429
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_header | INFO     | EVALUATION: LSTM_V2 (SEQLABELING)
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) TEST SET METRICS
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_evaluation | INFO     |   accuracy                  : 0.6703
2025-12-14 19:20:20 | __main__ | log_evaluation | INFO     |   f1_weighted               : 0.5380
2025-12-14 19:20:20 | __main__ | log_evaluation | INFO     |   f1_macro                  : 0.1147
2025-12-14 19:20:20 | __main__ | log_evaluation | INFO     |   detection_rate            : 0.0000
2025-12-14 19:20:20 | __main__ | log_evaluation | INFO     |   false_alarm_rate          : 0.0000
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | Confusion Matrix:
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     |                 0        1        2        3        4        5        6
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 0           7722        0        0        0        0        0        0
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 1           1195        0        0        0        0        0        0
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 2            742        0        0        0        0        0        0
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 3            342        0        0        0        0        0        0
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 4            966        0        0        0        0        0        0
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 5            510        0        0        0        0        0        0
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 6             43        0        0        0        0        0        0
2025-12-14 19:20:20 | __main__ | log_confusion_matrix | INFO     | 
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     | Classification Report:
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |                 precision    recall  f1-score   support
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |   
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |              0       0.67      1.00      0.80      7722
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |              1       0.00      0.00      0.00      1195
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |              2       0.00      0.00      0.00       742
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |              3       0.00      0.00      0.00       342
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |              4       0.00      0.00      0.00       966
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |              5       0.00      0.00      0.00       510
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |              6       0.00      0.00      0.00        43
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |   
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |       accuracy                           0.67     11520
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |      macro avg       0.10      0.14      0.11     11520
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |   weighted avg       0.45      0.67      0.54     11520
2025-12-14 19:20:20 | __main__ | log_class_report | INFO     |   
2025-12-14 19:20:20 | __main__ | main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_header | INFO     | DATA PREPARATION
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | prepare_dataloaders | INFO     | Created new stratified split: train=210, val=45, test=45
2025-12-14 19:20:20 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler.
2025-12-14 19:20:20 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:20:20 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:20:20 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_header | INFO     | TRAINING: HIERARCHICAL LSTM
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_header | INFO     | HIERARCHICAL LSTM CONFIG
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_header | INFO     | HIERARCHICAL LSTM BEST HYPERPARAMETERS
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   batch_size                     : 32
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   scheduler                      : cosine
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   grad_clip                      : 0.5
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s1_hidden_size                 : 192
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s1_num_layers                  : 2
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s1_dropout                     : 0.5
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s1_bidirectional               : True
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s2_hidden_size                 : 64
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s2_num_layers                  : 3
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s2_dropout                     : 0.3
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   s2_bidirectional               : False
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   use_class_weights              : True
2025-12-14 19:20:20 | __main__ | log_config | INFO     |   class_weight_scale             : 0.6
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     | Model: HierarchicalClassifier
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     |   stage1                    :  1,193,091 params ( 1,193,091 trainable)
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     |   stage2                    :     84,675 params (    84,675 trainable)
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     | Total parameters          :  1,277,766
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     | Trainable parameters      :  1,277,766
2025-12-14 19:20:20 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:20:20 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:21 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.3949 | Train Acc: 0.2390 | Val Loss: 1.9860 | Val Acc: 0.2101 | LR: 0.000288 | Val F1: 0.1914
2025-12-14 19:20:22 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.3152 | Train Acc: 0.2117 | Val Loss: 2.0607 | Val Acc: 0.2404 | LR: 0.000288 | Val F1: 0.2729
2025-12-14 19:20:22 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.3001 | Train Acc: 0.1852 | Val Loss: 1.9873 | Val Acc: 0.2655 | LR: 0.000288 | Val F1: 0.3146
2025-12-14 19:20:23 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.2574 | Train Acc: 0.1779 | Val Loss: 1.9836 | Val Acc: 0.2182 | LR: 0.000287 | Val F1: 0.2417
2025-12-14 19:20:24 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.2528 | Train Acc: 0.2352 | Val Loss: 1.9733 | Val Acc: 0.2745 | LR: 0.000287 | Val F1: 0.3110
2025-12-14 19:20:25 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.2219 | Train Acc: 0.2438 | Val Loss: 2.0731 | Val Acc: 0.1727 | LR: 0.000286 | Val F1: 0.1778
2025-12-14 19:20:26 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.2270 | Train Acc: 0.2021 | Val Loss: 2.0306 | Val Acc: 0.3899 | LR: 0.000285 | Val F1: 0.4496
2025-12-14 19:20:27 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.2329 | Train Acc: 0.2830 | Val Loss: 1.9791 | Val Acc: 0.1543 | LR: 0.000285 | Val F1: 0.1320
2025-12-14 19:20:28 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.2236 | Train Acc: 0.2172 | Val Loss: 2.0190 | Val Acc: 0.4240 | LR: 0.000283 | Val F1: 0.4919
2025-12-14 19:20:29 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.2004 | Train Acc: 0.3239 | Val Loss: 1.9722 | Val Acc: 0.1903 | LR: 0.000282 | Val F1: 0.2052
2025-12-14 19:20:30 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.2156 | Train Acc: 0.2425 | Val Loss: 2.0374 | Val Acc: 0.2116 | LR: 0.000281 | Val F1: 0.2280
2025-12-14 19:20:30 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.1825 | Train Acc: 0.2420 | Val Loss: 2.0586 | Val Acc: 0.1573 | LR: 0.000280 | Val F1: 0.1609
2025-12-14 19:20:31 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.1836 | Train Acc: 0.2435 | Val Loss: 1.9628 | Val Acc: 0.2378 | LR: 0.000278 | Val F1: 0.2691
2025-12-14 19:20:32 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.1835 | Train Acc: 0.2686 | Val Loss: 1.9564 | Val Acc: 0.4023 | LR: 0.000276 | Val F1: 0.4581
2025-12-14 19:20:33 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.1889 | Train Acc: 0.2617 | Val Loss: 1.9553 | Val Acc: 0.1901 | LR: 0.000274 | Val F1: 0.2030
2025-12-14 19:20:34 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.1606 | Train Acc: 0.2830 | Val Loss: 2.0343 | Val Acc: 0.2225 | LR: 0.000272 | Val F1: 0.2518
2025-12-14 19:20:34 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.1493 | Train Acc: 0.3194 | Val Loss: 2.0108 | Val Acc: 0.1785 | LR: 0.000270 | Val F1: 0.1895
2025-12-14 19:20:35 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.1637 | Train Acc: 0.2176 | Val Loss: 2.0132 | Val Acc: 0.3490 | LR: 0.000268 | Val F1: 0.4048
2025-12-14 19:20:36 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.1246 | Train Acc: 0.3117 | Val Loss: 1.9923 | Val Acc: 0.2252 | LR: 0.000266 | Val F1: 0.2524
2025-12-14 19:20:37 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.1473 | Train Acc: 0.2826 | Val Loss: 1.9858 | Val Acc: 0.3020 | LR: 0.000263 | Val F1: 0.3570
2025-12-14 19:20:38 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.1418 | Train Acc: 0.2769 | Val Loss: 1.9281 | Val Acc: 0.3245 | LR: 0.000261 | Val F1: 0.3781
2025-12-14 19:20:39 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.1467 | Train Acc: 0.2208 | Val Loss: 2.0474 | Val Acc: 0.3464 | LR: 0.000258 | Val F1: 0.4137
2025-12-14 19:20:39 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.1245 | Train Acc: 0.3384 | Val Loss: 2.0052 | Val Acc: 0.2829 | LR: 0.000255 | Val F1: 0.3234
2025-12-14 19:20:40 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.1411 | Train Acc: 0.3000 | Val Loss: 2.0087 | Val Acc: 0.3490 | LR: 0.000252 | Val F1: 0.4004
2025-12-14 19:20:40 | __main__ | train_model | INFO     | Early stopping triggered at epoch 24 due to no improvement in validation F1.
2025-12-14 19:20:40 | __main__ | train_model | INFO     | Training for Hierarchical LSTM completed in 20.4s. Best validation F1: 0.4919
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_header | INFO     | EVALUATION: HIERARCHICAL LSTM
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_header | INFO     | HIERARCHICAL LSTM TEST SET METRICS
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_evaluation | INFO     |   accuracy                  : 0.6703
2025-12-14 19:20:40 | __main__ | log_evaluation | INFO     |   f1_weighted               : 0.5380
2025-12-14 19:20:40 | __main__ | log_evaluation | INFO     |   f1_macro                  : 0.1147
2025-12-14 19:20:40 | __main__ | log_evaluation | INFO     |   detection_rate            : 0.0000
2025-12-14 19:20:40 | __main__ | log_evaluation | INFO     |   false_alarm_rate          : 0.0000
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | Confusion Matrix:
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     |                 0        1        2        3        4        5        6
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 0           7722        0        0        0        0        0        0
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 1           1195        0        0        0        0        0        0
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 2            742        0        0        0        0        0        0
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 3            342        0        0        0        0        0        0
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 4            966        0        0        0        0        0        0
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 5            510        0        0        0        0        0        0
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 6             43        0        0        0        0        0        0
2025-12-14 19:20:40 | __main__ | log_confusion_matrix | INFO     | 
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     | Classification Report:
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |                 precision    recall  f1-score   support
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |   
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |              0       0.67      1.00      0.80      7722
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |              1       0.00      0.00      0.00      1195
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |              2       0.00      0.00      0.00       742
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |              3       0.00      0.00      0.00       342
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |              4       0.00      0.00      0.00       966
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |              5       0.00      0.00      0.00       510
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |              6       0.00      0.00      0.00        43
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |   
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |       accuracy                           0.67     11520
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |      macro avg       0.10      0.14      0.11     11520
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |   weighted avg       0.45      0.67      0.54     11520
2025-12-14 19:20:40 | __main__ | log_class_report | INFO     |   
2025-12-14 19:20:40 | __main__ | main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/hierarchical_v1/best_model.pth
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_header | INFO     | DATA PREPARATION
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | prepare_dataloaders | INFO     | Created new stratified split: train=210, val=45, test=45
2025-12-14 19:20:40 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler.
2025-12-14 19:20:40 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:20:40 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:20:40 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   d_model                        : 128
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   nhead                          : 4
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   dropout                        : 0.5
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   learnable_pe                   : False
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 19:20:40 | __main__ | log_config | INFO     |   batch_size                     : 8
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 19:20:40 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:20:40 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:20:42 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.7402 | Train Acc: 0.1217 | Val Loss: 2.1374 | Val Acc: 0.2666 | LR: 0.000930 | Val F1: 0.3008
2025-12-14 19:20:43 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.4959 | Train Acc: 0.1478 | Val Loss: 2.1870 | Val Acc: 0.0619 | LR: 0.000930 | Val F1: 0.0309
2025-12-14 19:20:44 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.4098 | Train Acc: 0.1467 | Val Loss: 2.2735 | Val Acc: 0.0118 | LR: 0.000929 | Val F1: 0.0091
2025-12-14 19:20:46 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.3952 | Train Acc: 0.1592 | Val Loss: 2.2967 | Val Acc: 0.5253 | LR: 0.000928 | Val F1: 0.5448
2025-12-14 19:20:47 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.3772 | Train Acc: 0.2387 | Val Loss: 2.0897 | Val Acc: 0.5460 | LR: 0.000926 | Val F1: 0.5629
2025-12-14 19:20:49 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.3351 | Train Acc: 0.2571 | Val Loss: 2.0613 | Val Acc: 0.0963 | LR: 0.000924 | Val F1: 0.0456
2025-12-14 19:20:50 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.3608 | Train Acc: 0.2400 | Val Loss: 2.2677 | Val Acc: 0.4595 | LR: 0.000922 | Val F1: 0.5228
2025-12-14 19:20:51 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.3280 | Train Acc: 0.2571 | Val Loss: 2.0224 | Val Acc: 0.5381 | LR: 0.000919 | Val F1: 0.5652
2025-12-14 19:20:53 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.3256 | Train Acc: 0.2328 | Val Loss: 2.3645 | Val Acc: 0.4981 | LR: 0.000915 | Val F1: 0.5473
2025-12-14 19:20:54 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.3346 | Train Acc: 0.2884 | Val Loss: 2.2241 | Val Acc: 0.0946 | LR: 0.000912 | Val F1: 0.0606
2025-12-14 19:20:56 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.3079 | Train Acc: 0.2946 | Val Loss: 2.1268 | Val Acc: 0.3738 | LR: 0.000907 | Val F1: 0.4477
2025-12-14 19:20:57 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.2992 | Train Acc: 0.3063 | Val Loss: 2.1457 | Val Acc: 0.0781 | LR: 0.000903 | Val F1: 0.0599
2025-12-14 19:20:58 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.2994 | Train Acc: 0.3100 | Val Loss: 2.1047 | Val Acc: 0.1685 | LR: 0.000897 | Val F1: 0.1742
2025-12-14 19:20:59 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.2842 | Train Acc: 0.2777 | Val Loss: 2.0855 | Val Acc: 0.3612 | LR: 0.000892 | Val F1: 0.4269
2025-12-14 19:21:00 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.2711 | Train Acc: 0.3368 | Val Loss: 2.1809 | Val Acc: 0.0872 | LR: 0.000886 | Val F1: 0.1002
2025-12-14 19:21:02 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.2780 | Train Acc: 0.2718 | Val Loss: 2.0283 | Val Acc: 0.2968 | LR: 0.000879 | Val F1: 0.3705
2025-12-14 19:21:03 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.2919 | Train Acc: 0.3768 | Val Loss: 1.9468 | Val Acc: 0.4935 | LR: 0.000873 | Val F1: 0.5291
2025-12-14 19:21:05 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.2734 | Train Acc: 0.4133 | Val Loss: 2.1056 | Val Acc: 0.3358 | LR: 0.000865 | Val F1: 0.4299
2025-12-14 19:21:05 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.2774 | Train Acc: 0.2876 | Val Loss: 2.1090 | Val Acc: 0.5342 | LR: 0.000858 | Val F1: 0.5828
2025-12-14 19:21:07 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.2601 | Train Acc: 0.4002 | Val Loss: 2.0991 | Val Acc: 0.4229 | LR: 0.000850 | Val F1: 0.4993
2025-12-14 19:21:08 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.2497 | Train Acc: 0.3157 | Val Loss: 2.0718 | Val Acc: 0.3020 | LR: 0.000841 | Val F1: 0.3878
2025-12-14 19:21:10 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.2580 | Train Acc: 0.4132 | Val Loss: 1.9739 | Val Acc: 0.4622 | LR: 0.000833 | Val F1: 0.5270
2025-12-14 19:21:11 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.2483 | Train Acc: 0.3064 | Val Loss: 2.0120 | Val Acc: 0.5478 | LR: 0.000823 | Val F1: 0.5733
2025-12-14 19:21:12 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.2631 | Train Acc: 0.3683 | Val Loss: 2.0258 | Val Acc: 0.4151 | LR: 0.000814 | Val F1: 0.4849
2025-12-14 19:21:14 | __main__ | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 2.2470 | Train Acc: 0.3918 | Val Loss: 2.0717 | Val Acc: 0.3485 | LR: 0.000804 | Val F1: 0.4231
2025-12-14 19:21:15 | __main__ | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 2.2429 | Train Acc: 0.3449 | Val Loss: 2.1144 | Val Acc: 0.5780 | LR: 0.000794 | Val F1: 0.6053
2025-12-14 19:21:16 | __main__ | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 2.2375 | Train Acc: 0.4032 | Val Loss: 2.0546 | Val Acc: 0.5498 | LR: 0.000783 | Val F1: 0.5905
2025-12-14 19:21:17 | __main__ | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 2.2391 | Train Acc: 0.4200 | Val Loss: 2.0188 | Val Acc: 0.3359 | LR: 0.000773 | Val F1: 0.4361
2025-12-14 19:21:19 | __main__ | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 2.2327 | Train Acc: 0.2907 | Val Loss: 1.9870 | Val Acc: 0.3590 | LR: 0.000762 | Val F1: 0.4444
2025-12-14 19:21:20 | __main__ | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 2.2386 | Train Acc: 0.3917 | Val Loss: 1.9732 | Val Acc: 0.5076 | LR: 0.000750 | Val F1: 0.5643
2025-12-14 19:21:22 | __main__ | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 2.2174 | Train Acc: 0.3499 | Val Loss: 1.9999 | Val Acc: 0.4157 | LR: 0.000739 | Val F1: 0.4706
2025-12-14 19:21:23 | __main__ | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 2.2348 | Train Acc: 0.3402 | Val Loss: 1.9940 | Val Acc: 0.5273 | LR: 0.000727 | Val F1: 0.5529
2025-12-14 19:21:24 | __main__ | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 2.2364 | Train Acc: 0.3426 | Val Loss: 1.9889 | Val Acc: 0.3642 | LR: 0.000714 | Val F1: 0.4165
2025-12-14 19:21:25 | __main__ | log_epoch | INFO     | Epoch [  34/100] | Train Loss: 2.2109 | Train Acc: 0.3684 | Val Loss: 1.9684 | Val Acc: 0.4154 | LR: 0.000702 | Val F1: 0.4584
2025-12-14 19:21:26 | __main__ | log_epoch | INFO     | Epoch [  35/100] | Train Loss: 2.2285 | Train Acc: 0.3570 | Val Loss: 2.0073 | Val Acc: 0.4233 | LR: 0.000689 | Val F1: 0.4902
2025-12-14 19:21:27 | __main__ | log_epoch | INFO     | Epoch [  36/100] | Train Loss: 2.2225 | Train Acc: 0.3888 | Val Loss: 2.0480 | Val Acc: 0.4255 | LR: 0.000676 | Val F1: 0.5028
2025-12-14 19:21:28 | __main__ | log_epoch | INFO     | Epoch [  37/100] | Train Loss: 2.2083 | Train Acc: 0.3957 | Val Loss: 2.0777 | Val Acc: 0.6352 | LR: 0.000663 | Val F1: 0.6329
2025-12-14 19:21:30 | __main__ | log_epoch | INFO     | Epoch [  38/100] | Train Loss: 2.2149 | Train Acc: 0.3912 | Val Loss: 2.1173 | Val Acc: 0.2447 | LR: 0.000650 | Val F1: 0.3120
2025-12-14 19:21:31 | __main__ | log_epoch | INFO     | Epoch [  39/100] | Train Loss: 2.1937 | Train Acc: 0.2806 | Val Loss: 2.1149 | Val Acc: 0.4516 | LR: 0.000636 | Val F1: 0.5272
2025-12-14 19:21:31 | __main__ | log_epoch | INFO     | Epoch [  40/100] | Train Loss: 2.2146 | Train Acc: 0.4470 | Val Loss: 2.0805 | Val Acc: 0.3289 | LR: 0.000623 | Val F1: 0.4160
2025-12-14 19:21:32 | __main__ | log_epoch | INFO     | Epoch [  41/100] | Train Loss: 2.2278 | Train Acc: 0.3691 | Val Loss: 2.0339 | Val Acc: 0.4866 | LR: 0.000609 | Val F1: 0.5495
2025-12-14 19:21:33 | __main__ | log_epoch | INFO     | Epoch [  42/100] | Train Loss: 2.1858 | Train Acc: 0.3544 | Val Loss: 2.0835 | Val Acc: 0.6259 | LR: 0.000595 | Val F1: 0.6304
2025-12-14 19:21:34 | __main__ | log_epoch | INFO     | Epoch [  43/100] | Train Loss: 2.1987 | Train Acc: 0.4072 | Val Loss: 2.0043 | Val Acc: 0.3995 | LR: 0.000581 | Val F1: 0.4717
2025-12-14 19:21:35 | __main__ | log_epoch | INFO     | Epoch [  44/100] | Train Loss: 2.2131 | Train Acc: 0.4252 | Val Loss: 2.0462 | Val Acc: 0.4686 | LR: 0.000567 | Val F1: 0.5226
2025-12-14 19:21:37 | __main__ | log_epoch | INFO     | Epoch [  45/100] | Train Loss: 2.1985 | Train Acc: 0.3813 | Val Loss: 2.0696 | Val Acc: 0.4043 | LR: 0.000553 | Val F1: 0.4848
2025-12-14 19:21:38 | __main__ | log_epoch | INFO     | Epoch [  46/100] | Train Loss: 2.1972 | Train Acc: 0.3800 | Val Loss: 2.0399 | Val Acc: 0.3847 | LR: 0.000538 | Val F1: 0.4579
2025-12-14 19:21:39 | __main__ | log_epoch | INFO     | Epoch [  47/100] | Train Loss: 2.2099 | Train Acc: 0.4079 | Val Loss: 2.0713 | Val Acc: 0.4546 | LR: 0.000524 | Val F1: 0.5303
2025-12-14 19:21:40 | __main__ | log_epoch | INFO     | Epoch [  48/100] | Train Loss: 2.1922 | Train Acc: 0.3917 | Val Loss: 2.0124 | Val Acc: 0.3316 | LR: 0.000509 | Val F1: 0.3982
2025-12-14 19:21:41 | __main__ | log_epoch | INFO     | Epoch [  49/100] | Train Loss: 2.1793 | Train Acc: 0.3632 | Val Loss: 2.0389 | Val Acc: 0.5997 | LR: 0.000495 | Val F1: 0.6185
2025-12-14 19:21:42 | __main__ | log_epoch | INFO     | Epoch [  50/100] | Train Loss: 2.1819 | Train Acc: 0.4222 | Val Loss: 2.0264 | Val Acc: 0.4901 | LR: 0.000480 | Val F1: 0.5341
2025-12-14 19:21:43 | __main__ | log_epoch | INFO     | Epoch [  51/100] | Train Loss: 2.1827 | Train Acc: 0.3482 | Val Loss: 2.0647 | Val Acc: 0.4015 | LR: 0.000466 | Val F1: 0.4780
2025-12-14 19:21:44 | __main__ | log_epoch | INFO     | Epoch [  52/100] | Train Loss: 2.1825 | Train Acc: 0.3686 | Val Loss: 2.0804 | Val Acc: 0.4089 | LR: 0.000451 | Val F1: 0.4902
2025-12-14 19:21:44 | __main__ | train_model | INFO     | Early stopping triggered at epoch 52 due to no improvement in validation F1.
2025-12-14 19:21:44 | __main__ | train_model | INFO     | Training for Transformer completed in 64.0s. Best validation F1: 0.6329
2025-12-14 19:21:44 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:44 | __main__ | log_header | INFO     | EVALUATION: TRANSFORMER
2025-12-14 19:21:44 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:44 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:44 | __main__ | log_header | INFO     | TRANSFORMER TEST SET METRICS
2025-12-14 19:21:44 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:44 | __main__ | log_evaluation | INFO     |   accuracy                  : 0.1037
2025-12-14 19:21:44 | __main__ | log_evaluation | INFO     |   f1_weighted               : 0.0195
2025-12-14 19:21:44 | __main__ | log_evaluation | INFO     |   f1_macro                  : 0.0269
2025-12-14 19:21:44 | __main__ | log_evaluation | INFO     |   detection_rate            : 1.0000
2025-12-14 19:21:44 | __main__ | log_evaluation | INFO     |   false_alarm_rate          : 1.0000
2025-12-14 19:21:44 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | Confusion Matrix:
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     |                 0        1        2        3        4        5        6
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 0              0     7722        0        0        0        0        0
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 1              0     1195        0        0        0        0        0
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 2              0      742        0        0        0        0        0
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 3              0      342        0        0        0        0        0
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 4              0      966        0        0        0        0        0
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 5              0      510        0        0        0        0        0
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 6              0       43        0        0        0        0        0
2025-12-14 19:21:44 | __main__ | log_confusion_matrix | INFO     | 
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     | Classification Report:
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |                 precision    recall  f1-score   support
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |   
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |              0       0.00      0.00      0.00      7722
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |              1       0.10      1.00      0.19      1195
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |              2       0.00      0.00      0.00       742
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |              3       0.00      0.00      0.00       342
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |              4       0.00      0.00      0.00       966
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |              5       0.00      0.00      0.00       510
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |              6       0.00      0.00      0.00        43
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |   
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |       accuracy                           0.10     11520
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |      macro avg       0.01      0.14      0.03     11520
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |   weighted avg       0.01      0.10      0.02     11520
2025-12-14 19:21:44 | __main__ | log_class_report | INFO     |   
2025-12-14 19:21:44 | __main__ | main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 19:21:44 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:44 | __main__ | log_header | INFO     | TRAINING: STATISTICAL BASELINE
2025-12-14 19:21:44 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:45 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:45 | __main__ | log_header | INFO     | BASELINE TEST SET METRICS
2025-12-14 19:21:45 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:45 | __main__ | log_evaluation | INFO     |   accuracy                  : 0.4997
2025-12-14 19:21:45 | __main__ | log_evaluation | INFO     |   f1_weighted               : 0.4644
2025-12-14 19:21:45 | __main__ | log_evaluation | INFO     |   f1_macro                  : 0.1069
2025-12-14 19:21:45 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:45 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:45 | __main__ | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 19:21:45 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:21:45 | __main__ | main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 19:21:45 | __main__ | main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 19:21:45 | __main__ | main | INFO     | LSTM_v2 (SeqLabeling)     0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 19:21:45 | __main__ | main | INFO     | Hierarchical LSTM         0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 19:21:45 | __main__ | main | INFO     | Statistical Baseline      0.4997       0.4644          0.1069       0.0000           0.0000            
2025-12-14 19:21:45 | __main__ | main | INFO     | Transformer               0.1037       0.0195          0.0269       1.0000           1.0000            
2025-12-14 19:21:45 | __main__ | main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_192010.json
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | CONFIGURATION
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   Epochs                         : 100
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   Device                         : cuda
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   Log Level                      : INFO
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   Augmentation                   : True
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 19:52:03 | __main__ | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 19:52:03 | __main__ | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 19:52:03 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:52:03 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:52:03 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   hidden_size                    : 96
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   num_layers                     : 2
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   dropout                        : 0.5
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   bidirectional                  : True
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   batch_size                     : 16
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   scheduler                      : cosine
2025-12-14 19:52:03 | __main__ | log_config | INFO     |   use_class_weights              : False
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 19:52:03 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:52:03 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:52:05 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 19:52:05 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 19:52:05 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 19:52:05 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 19:52:06 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 19:52:06 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 19:52:06 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 19:52:07 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 19:52:07 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 19:52:07 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 19:52:07 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 19:52:08 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 19:52:08 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 19:52:08 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 19:52:08 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 19:52:08 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 19:52:09 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 19:52:09 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 19:52:09 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 19:52:09 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 19:52:09 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 19:52:10 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 19:52:10 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 19:52:10 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 19:52:10 | __main__ | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 19:52:11 | __main__ | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 19:52:11 | __main__ | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 19:52:11 | __main__ | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 19:52:11 | __main__ | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 19:52:12 | __main__ | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 19:52:12 | __main__ | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 19:52:12 | __main__ | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 19:52:12 | __main__ | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 19:52:12 | __main__ | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 19:52:12 | __main__ | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 8.3s. Best validation F1: 0.6429
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | CONFIGURATION
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   Epochs                         : 100
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   Device                         : cuda
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   Log Level                      : INFO
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   Augmentation                   : True
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 19:54:51 | __main__ | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 19:54:51 | __main__ | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 19:54:51 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:54:51 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:54:51 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   hidden_size                    : 96
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   num_layers                     : 2
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   dropout                        : 0.5
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   bidirectional                  : True
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   batch_size                     : 16
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   scheduler                      : cosine
2025-12-14 19:54:51 | __main__ | log_config | INFO     |   use_class_weights              : False
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 19:54:51 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:54:51 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:54:52 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 19:54:53 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 19:54:53 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 19:54:53 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 19:54:53 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 19:54:54 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 19:54:54 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 19:54:54 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 19:54:54 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 19:54:55 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 19:54:55 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 19:54:55 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 19:54:55 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 19:54:56 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 19:54:56 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 19:54:56 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 19:54:56 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 19:54:57 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 19:54:57 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 19:54:57 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 19:54:57 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 19:54:58 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 19:54:58 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 19:54:58 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 19:54:58 | __main__ | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 19:54:58 | __main__ | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 19:54:59 | __main__ | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 19:54:59 | __main__ | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 19:54:59 | __main__ | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 19:54:59 | __main__ | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 19:55:00 | __main__ | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 19:55:00 | __main__ | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 19:55:00 | __main__ | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 19:55:00 | __main__ | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 19:55:00 | __main__ | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 8.3s. Best validation F1: 0.6429
2025-12-14 19:55:00 | __main__ | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 19:55:00 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:55:00 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:55:00 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_header | INFO     | TRAINING: HIERARCHICAL LSTM
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_header | INFO     | HIERARCHICAL LSTM CONFIG
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_header | INFO     | HIERARCHICAL LSTM BEST HYPERPARAMETERS
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   batch_size                     : 32
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   scheduler                      : cosine
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   grad_clip                      : 0.5
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s1_hidden_size                 : 192
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s1_num_layers                  : 2
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s1_dropout                     : 0.5
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s1_bidirectional               : True
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s2_hidden_size                 : 64
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s2_num_layers                  : 3
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s2_dropout                     : 0.3
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   s2_bidirectional               : False
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   use_class_weights              : True
2025-12-14 19:55:00 | __main__ | log_config | INFO     |   class_weight_scale             : 0.6
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     | Model: HierarchicalClassifier
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     |   stage1                    :  1,193,091 params ( 1,193,091 trainable)
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     |   stage2                    :     84,675 params (    84,675 trainable)
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     | Total parameters          :  1,277,766
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     | Trainable parameters      :  1,277,766
2025-12-14 19:55:00 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:55:00 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:55:01 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.3949 | Train Acc: 0.2390 | Val Loss: 1.9860 | Val Acc: 0.2101 | LR: 0.000288 | Val F1: 0.1914
2025-12-14 19:55:02 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.3152 | Train Acc: 0.2117 | Val Loss: 2.0607 | Val Acc: 0.2404 | LR: 0.000288 | Val F1: 0.2729
2025-12-14 19:55:03 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.3001 | Train Acc: 0.1852 | Val Loss: 1.9873 | Val Acc: 0.2655 | LR: 0.000288 | Val F1: 0.3146
2025-12-14 19:55:04 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.2574 | Train Acc: 0.1779 | Val Loss: 1.9836 | Val Acc: 0.2182 | LR: 0.000287 | Val F1: 0.2417
2025-12-14 19:55:05 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.2528 | Train Acc: 0.2352 | Val Loss: 1.9733 | Val Acc: 0.2745 | LR: 0.000287 | Val F1: 0.3110
2025-12-14 19:55:06 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.2219 | Train Acc: 0.2438 | Val Loss: 2.0731 | Val Acc: 0.1727 | LR: 0.000286 | Val F1: 0.1778
2025-12-14 19:55:07 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.2270 | Train Acc: 0.2021 | Val Loss: 2.0306 | Val Acc: 0.3899 | LR: 0.000285 | Val F1: 0.4496
2025-12-14 19:55:08 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.2329 | Train Acc: 0.2830 | Val Loss: 1.9791 | Val Acc: 0.1543 | LR: 0.000285 | Val F1: 0.1320
2025-12-14 19:55:08 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.2236 | Train Acc: 0.2172 | Val Loss: 2.0190 | Val Acc: 0.4240 | LR: 0.000283 | Val F1: 0.4919
2025-12-14 19:55:09 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.2004 | Train Acc: 0.3239 | Val Loss: 1.9722 | Val Acc: 0.1903 | LR: 0.000282 | Val F1: 0.2052
2025-12-14 19:55:10 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.2156 | Train Acc: 0.2425 | Val Loss: 2.0374 | Val Acc: 0.2116 | LR: 0.000281 | Val F1: 0.2280
2025-12-14 19:55:11 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.1825 | Train Acc: 0.2420 | Val Loss: 2.0586 | Val Acc: 0.1573 | LR: 0.000280 | Val F1: 0.1609
2025-12-14 19:55:12 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.1836 | Train Acc: 0.2435 | Val Loss: 1.9628 | Val Acc: 0.2378 | LR: 0.000278 | Val F1: 0.2691
2025-12-14 19:55:13 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.1835 | Train Acc: 0.2686 | Val Loss: 1.9564 | Val Acc: 0.4023 | LR: 0.000276 | Val F1: 0.4581
2025-12-14 19:55:14 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.1889 | Train Acc: 0.2617 | Val Loss: 1.9553 | Val Acc: 0.1901 | LR: 0.000274 | Val F1: 0.2030
2025-12-14 19:55:15 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.1606 | Train Acc: 0.2830 | Val Loss: 2.0343 | Val Acc: 0.2225 | LR: 0.000272 | Val F1: 0.2518
2025-12-14 19:55:16 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.1493 | Train Acc: 0.3194 | Val Loss: 2.0108 | Val Acc: 0.1785 | LR: 0.000270 | Val F1: 0.1895
2025-12-14 19:55:17 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.1637 | Train Acc: 0.2176 | Val Loss: 2.0132 | Val Acc: 0.3490 | LR: 0.000268 | Val F1: 0.4048
2025-12-14 19:55:17 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.1246 | Train Acc: 0.3117 | Val Loss: 1.9923 | Val Acc: 0.2252 | LR: 0.000266 | Val F1: 0.2524
2025-12-14 19:55:18 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.1473 | Train Acc: 0.2826 | Val Loss: 1.9858 | Val Acc: 0.3020 | LR: 0.000263 | Val F1: 0.3570
2025-12-14 19:55:19 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.1418 | Train Acc: 0.2769 | Val Loss: 1.9281 | Val Acc: 0.3245 | LR: 0.000261 | Val F1: 0.3781
2025-12-14 19:55:20 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.1467 | Train Acc: 0.2208 | Val Loss: 2.0474 | Val Acc: 0.3464 | LR: 0.000258 | Val F1: 0.4137
2025-12-14 19:55:21 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.1245 | Train Acc: 0.3384 | Val Loss: 2.0052 | Val Acc: 0.2829 | LR: 0.000255 | Val F1: 0.3234
2025-12-14 19:55:22 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.1411 | Train Acc: 0.3000 | Val Loss: 2.0087 | Val Acc: 0.3490 | LR: 0.000252 | Val F1: 0.4004
2025-12-14 19:55:22 | __main__ | train_model | INFO     | Early stopping triggered at epoch 24 due to no improvement in validation F1.
2025-12-14 19:55:22 | __main__ | train_model | INFO     | Training for Hierarchical LSTM completed in 21.4s. Best validation F1: 0.4919
2025-12-14 19:55:22 | __main__ | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/hierarchical_v1/best_model.pth
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 19:55:22 | __main__ | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 19:55:22 | __main__ | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 19:55:22 | __main__ | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   epochs                         : 100
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   patience                       : 15
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   d_model                        : 128
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   nhead                          : 4
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   dropout                        : 0.5
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   learnable_pe                   : False
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 19:55:22 | __main__ | log_config | INFO     |   batch_size                     : 8
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     | 
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 19:55:22 | __main__ | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 19:55:22 | __main__ | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 19:55:23 | __main__ | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.7402 | Train Acc: 0.1217 | Val Loss: 2.1374 | Val Acc: 0.2666 | LR: 0.000930 | Val F1: 0.3008
2025-12-14 19:55:24 | __main__ | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.4959 | Train Acc: 0.1478 | Val Loss: 2.1870 | Val Acc: 0.0619 | LR: 0.000930 | Val F1: 0.0309
2025-12-14 19:55:26 | __main__ | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.4098 | Train Acc: 0.1467 | Val Loss: 2.2735 | Val Acc: 0.0118 | LR: 0.000929 | Val F1: 0.0091
2025-12-14 19:55:27 | __main__ | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.3952 | Train Acc: 0.1592 | Val Loss: 2.2967 | Val Acc: 0.5253 | LR: 0.000928 | Val F1: 0.5448
2025-12-14 19:55:29 | __main__ | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.3772 | Train Acc: 0.2387 | Val Loss: 2.0897 | Val Acc: 0.5460 | LR: 0.000926 | Val F1: 0.5629
2025-12-14 19:55:30 | __main__ | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.3351 | Train Acc: 0.2571 | Val Loss: 2.0613 | Val Acc: 0.0963 | LR: 0.000924 | Val F1: 0.0456
2025-12-14 19:55:31 | __main__ | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.3608 | Train Acc: 0.2400 | Val Loss: 2.2677 | Val Acc: 0.4595 | LR: 0.000922 | Val F1: 0.5228
2025-12-14 19:55:32 | __main__ | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.3280 | Train Acc: 0.2571 | Val Loss: 2.0224 | Val Acc: 0.5381 | LR: 0.000919 | Val F1: 0.5652
2025-12-14 19:55:34 | __main__ | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.3256 | Train Acc: 0.2328 | Val Loss: 2.3645 | Val Acc: 0.4981 | LR: 0.000915 | Val F1: 0.5473
2025-12-14 19:55:35 | __main__ | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.3346 | Train Acc: 0.2884 | Val Loss: 2.2241 | Val Acc: 0.0946 | LR: 0.000912 | Val F1: 0.0606
2025-12-14 19:55:37 | __main__ | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.3079 | Train Acc: 0.2946 | Val Loss: 2.1268 | Val Acc: 0.3738 | LR: 0.000907 | Val F1: 0.4477
2025-12-14 19:55:37 | __main__ | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.2992 | Train Acc: 0.3063 | Val Loss: 2.1457 | Val Acc: 0.0781 | LR: 0.000903 | Val F1: 0.0599
2025-12-14 19:55:38 | __main__ | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.2994 | Train Acc: 0.3100 | Val Loss: 2.1047 | Val Acc: 0.1685 | LR: 0.000897 | Val F1: 0.1742
2025-12-14 19:55:40 | __main__ | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.2842 | Train Acc: 0.2777 | Val Loss: 2.0855 | Val Acc: 0.3612 | LR: 0.000892 | Val F1: 0.4269
2025-12-14 19:55:41 | __main__ | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.2711 | Train Acc: 0.3368 | Val Loss: 2.1809 | Val Acc: 0.0872 | LR: 0.000886 | Val F1: 0.1002
2025-12-14 19:55:43 | __main__ | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.2780 | Train Acc: 0.2718 | Val Loss: 2.0283 | Val Acc: 0.2968 | LR: 0.000879 | Val F1: 0.3705
2025-12-14 19:55:44 | __main__ | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.2919 | Train Acc: 0.3768 | Val Loss: 1.9468 | Val Acc: 0.4935 | LR: 0.000873 | Val F1: 0.5291
2025-12-14 19:55:45 | __main__ | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.2734 | Train Acc: 0.4133 | Val Loss: 2.1056 | Val Acc: 0.3358 | LR: 0.000865 | Val F1: 0.4299
2025-12-14 19:55:47 | __main__ | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.2774 | Train Acc: 0.2876 | Val Loss: 2.1090 | Val Acc: 0.5342 | LR: 0.000858 | Val F1: 0.5828
2025-12-14 19:55:48 | __main__ | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.2601 | Train Acc: 0.4002 | Val Loss: 2.0991 | Val Acc: 0.4229 | LR: 0.000850 | Val F1: 0.4993
2025-12-14 19:55:50 | __main__ | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.2497 | Train Acc: 0.3157 | Val Loss: 2.0718 | Val Acc: 0.3020 | LR: 0.000841 | Val F1: 0.3878
2025-12-14 19:55:51 | __main__ | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.2580 | Train Acc: 0.4132 | Val Loss: 1.9739 | Val Acc: 0.4622 | LR: 0.000833 | Val F1: 0.5270
2025-12-14 19:55:52 | __main__ | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.2483 | Train Acc: 0.3064 | Val Loss: 2.0120 | Val Acc: 0.5478 | LR: 0.000823 | Val F1: 0.5733
2025-12-14 19:55:54 | __main__ | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.2631 | Train Acc: 0.3683 | Val Loss: 2.0258 | Val Acc: 0.4151 | LR: 0.000814 | Val F1: 0.4849
2025-12-14 19:55:55 | __main__ | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 2.2470 | Train Acc: 0.3918 | Val Loss: 2.0717 | Val Acc: 0.3485 | LR: 0.000804 | Val F1: 0.4231
2025-12-14 19:55:56 | __main__ | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 2.2429 | Train Acc: 0.3449 | Val Loss: 2.1144 | Val Acc: 0.5780 | LR: 0.000794 | Val F1: 0.6053
2025-12-14 19:55:58 | __main__ | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 2.2375 | Train Acc: 0.4032 | Val Loss: 2.0546 | Val Acc: 0.5498 | LR: 0.000783 | Val F1: 0.5905
2025-12-14 19:55:59 | __main__ | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 2.2391 | Train Acc: 0.4200 | Val Loss: 2.0188 | Val Acc: 0.3359 | LR: 0.000773 | Val F1: 0.4361
2025-12-14 19:56:00 | __main__ | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 2.2327 | Train Acc: 0.2907 | Val Loss: 1.9870 | Val Acc: 0.3590 | LR: 0.000762 | Val F1: 0.4444
2025-12-14 19:56:02 | __main__ | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 2.2386 | Train Acc: 0.3917 | Val Loss: 1.9732 | Val Acc: 0.5076 | LR: 0.000750 | Val F1: 0.5643
2025-12-14 19:56:03 | __main__ | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 2.2174 | Train Acc: 0.3499 | Val Loss: 1.9999 | Val Acc: 0.4157 | LR: 0.000739 | Val F1: 0.4706
2025-12-14 19:56:04 | __main__ | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 2.2348 | Train Acc: 0.3402 | Val Loss: 1.9940 | Val Acc: 0.5273 | LR: 0.000727 | Val F1: 0.5529
2025-12-14 19:56:06 | __main__ | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 2.2364 | Train Acc: 0.3426 | Val Loss: 1.9889 | Val Acc: 0.3642 | LR: 0.000714 | Val F1: 0.4165
2025-12-14 19:56:07 | __main__ | log_epoch | INFO     | Epoch [  34/100] | Train Loss: 2.2109 | Train Acc: 0.3684 | Val Loss: 1.9684 | Val Acc: 0.4154 | LR: 0.000702 | Val F1: 0.4584
2025-12-14 19:56:09 | __main__ | log_epoch | INFO     | Epoch [  35/100] | Train Loss: 2.2285 | Train Acc: 0.3570 | Val Loss: 2.0073 | Val Acc: 0.4233 | LR: 0.000689 | Val F1: 0.4902
2025-12-14 19:56:10 | __main__ | log_epoch | INFO     | Epoch [  36/100] | Train Loss: 2.2225 | Train Acc: 0.3888 | Val Loss: 2.0480 | Val Acc: 0.4255 | LR: 0.000676 | Val F1: 0.5028
2025-12-14 19:56:11 | __main__ | log_epoch | INFO     | Epoch [  37/100] | Train Loss: 2.2083 | Train Acc: 0.3957 | Val Loss: 2.0777 | Val Acc: 0.6352 | LR: 0.000663 | Val F1: 0.6329
2025-12-14 19:56:12 | __main__ | log_epoch | INFO     | Epoch [  38/100] | Train Loss: 2.2149 | Train Acc: 0.3912 | Val Loss: 2.1173 | Val Acc: 0.2447 | LR: 0.000650 | Val F1: 0.3120
2025-12-14 19:56:13 | __main__ | log_epoch | INFO     | Epoch [  39/100] | Train Loss: 2.1937 | Train Acc: 0.2805 | Val Loss: 2.1149 | Val Acc: 0.4516 | LR: 0.000636 | Val F1: 0.5272
2025-12-14 19:56:14 | __main__ | log_epoch | INFO     | Epoch [  40/100] | Train Loss: 2.2146 | Train Acc: 0.4470 | Val Loss: 2.0805 | Val Acc: 0.3288 | LR: 0.000623 | Val F1: 0.4160
2025-12-14 19:56:16 | __main__ | log_epoch | INFO     | Epoch [  41/100] | Train Loss: 2.2278 | Train Acc: 0.3690 | Val Loss: 2.0339 | Val Acc: 0.4866 | LR: 0.000609 | Val F1: 0.5495
2025-12-14 19:56:17 | __main__ | log_epoch | INFO     | Epoch [  42/100] | Train Loss: 2.1858 | Train Acc: 0.3544 | Val Loss: 2.0835 | Val Acc: 0.6259 | LR: 0.000595 | Val F1: 0.6304
2025-12-14 19:56:18 | __main__ | log_epoch | INFO     | Epoch [  43/100] | Train Loss: 2.1987 | Train Acc: 0.4072 | Val Loss: 2.0043 | Val Acc: 0.3991 | LR: 0.000581 | Val F1: 0.4715
2025-12-14 19:56:20 | __main__ | log_epoch | INFO     | Epoch [  44/100] | Train Loss: 2.2131 | Train Acc: 0.4252 | Val Loss: 2.0462 | Val Acc: 0.4686 | LR: 0.000567 | Val F1: 0.5226
2025-12-14 19:56:21 | __main__ | log_epoch | INFO     | Epoch [  45/100] | Train Loss: 2.1985 | Train Acc: 0.3814 | Val Loss: 2.0696 | Val Acc: 0.4043 | LR: 0.000553 | Val F1: 0.4849
2025-12-14 19:56:22 | __main__ | log_epoch | INFO     | Epoch [  46/100] | Train Loss: 2.1971 | Train Acc: 0.3801 | Val Loss: 2.0398 | Val Acc: 0.3849 | LR: 0.000538 | Val F1: 0.4580
2025-12-14 19:56:24 | __main__ | log_epoch | INFO     | Epoch [  47/100] | Train Loss: 2.2098 | Train Acc: 0.4077 | Val Loss: 2.0713 | Val Acc: 0.4549 | LR: 0.000524 | Val F1: 0.5303
2025-12-14 19:56:25 | __main__ | log_epoch | INFO     | Epoch [  48/100] | Train Loss: 2.1918 | Train Acc: 0.3916 | Val Loss: 2.0126 | Val Acc: 0.3363 | LR: 0.000509 | Val F1: 0.4035
2025-12-14 19:56:26 | __main__ | log_epoch | INFO     | Epoch [  49/100] | Train Loss: 2.1791 | Train Acc: 0.3631 | Val Loss: 2.0384 | Val Acc: 0.6001 | LR: 0.000495 | Val F1: 0.6187
2025-12-14 19:56:27 | __main__ | log_epoch | INFO     | Epoch [  50/100] | Train Loss: 2.1819 | Train Acc: 0.4220 | Val Loss: 2.0265 | Val Acc: 0.4913 | LR: 0.000480 | Val F1: 0.5351
2025-12-14 19:56:28 | __main__ | log_epoch | INFO     | Epoch [  51/100] | Train Loss: 2.1828 | Train Acc: 0.3485 | Val Loss: 2.0643 | Val Acc: 0.4053 | LR: 0.000466 | Val F1: 0.4814
2025-12-14 19:56:29 | __main__ | log_epoch | INFO     | Epoch [  52/100] | Train Loss: 2.1827 | Train Acc: 0.3690 | Val Loss: 2.0801 | Val Acc: 0.4111 | LR: 0.000451 | Val F1: 0.4920
2025-12-14 19:56:29 | __main__ | train_model | INFO     | Early stopping triggered at epoch 52 due to no improvement in validation F1.
2025-12-14 19:56:29 | __main__ | train_model | INFO     | Training for Transformer completed in 67.7s. Best validation F1: 0.6329
2025-12-14 19:56:30 | __main__ | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 19:56:30 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:56:30 | __main__ | log_header | INFO     | TRAINING: STATISTICAL BASELINE
2025-12-14 19:56:30 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:56:30 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:56:30 | __main__ | log_header | INFO     | BASELINE TEST SET METRICS
2025-12-14 19:56:30 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:56:30 | __main__ | log_evaluation | INFO     |   accuracy                  : 0.4997
2025-12-14 19:56:30 | __main__ | log_evaluation | INFO     |   f1_weighted               : 0.4644
2025-12-14 19:56:30 | __main__ | log_evaluation | INFO     |   f1_macro                  : 0.1069
2025-12-14 19:56:30 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:56:30 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:56:30 | __main__ | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 19:56:30 | __main__ | log_separator | INFO     | ======================================================================
2025-12-14 19:56:30 | __main__ | train_main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 19:56:30 | __main__ | train_main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 19:56:30 | __main__ | train_main | INFO     | LSTM_v2 (SeqLabeling)     0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 19:56:30 | __main__ | train_main | INFO     | Hierarchical LSTM         0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 19:56:30 | __main__ | train_main | INFO     | Statistical Baseline      0.4997       0.4644          0.1069       0.0000           0.0000            
2025-12-14 19:56:30 | __main__ | train_main | INFO     | Transformer               0.1037       0.0195          0.0269       1.0000           1.0000            
2025-12-14 19:56:30 | __main__ | train_main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_195451.json
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | CONFIGURATION
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 20:54:54 | training | log_config | INFO     |   Epochs                         : 100
2025-12-14 20:54:54 | training | log_config | INFO     |   Device                         : cuda
2025-12-14 20:54:54 | training | log_config | INFO     |   Log Level                      : INFO
2025-12-14 20:54:54 | training | log_config | INFO     |   Augmentation                   : True
2025-12-14 20:54:54 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 20:54:54 | training | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 20:54:54 | training | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 20:54:54 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 20:54:54 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 20:54:54 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 20:54:54 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 20:54:54 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 20:54:54 | training | log_config | INFO     |   patience                       : 15
2025-12-14 20:54:54 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 20:54:54 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_config | INFO     |   hidden_size                    : 96
2025-12-14 20:54:54 | training | log_config | INFO     |   num_layers                     : 2
2025-12-14 20:54:54 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 20:54:54 | training | log_config | INFO     |   bidirectional                  : True
2025-12-14 20:54:54 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 20:54:54 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 20:54:54 | training | log_config | INFO     |   batch_size                     : 16
2025-12-14 20:54:54 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 20:54:54 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 20:54:54 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 20:54:54 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:54:54 | training | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 20:54:54 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 20:54:54 | training | log_model_summary | INFO     | 
2025-12-14 20:54:54 | training | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 20:54:54 | training | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 20:54:54 | training | log_model_summary | INFO     | 
2025-12-14 20:54:54 | training | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 20:54:54 | training | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 20:54:54 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 20:54:54 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:54:55 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 20:54:55 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 20:54:56 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 20:54:56 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 20:54:56 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 20:54:56 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 20:54:57 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 20:54:57 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 20:54:57 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 20:54:58 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 20:54:58 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 20:54:58 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 20:54:58 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 20:54:59 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 20:54:59 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 20:54:59 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 20:54:59 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 20:54:59 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 20:55:00 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 20:55:00 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 20:55:00 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 20:55:00 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 20:55:01 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 20:55:01 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 20:55:01 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 20:55:01 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 20:55:02 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 20:55:02 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 20:55:02 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 20:55:02 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 20:55:03 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 20:55:03 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 20:55:03 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 20:55:03 | training | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 20:55:03 | training | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 8.5s. Best validation F1: 0.6429
2025-12-14 20:55:03 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 20:55:03 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 20:55:03 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 20:55:03 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_header | INFO     | TRAINING: HIERARCHICAL LSTM
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_header | INFO     | HIERARCHICAL LSTM CONFIG
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 20:55:03 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 20:55:03 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 20:55:03 | training | log_config | INFO     |   patience                       : 15
2025-12-14 20:55:03 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 20:55:03 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_header | INFO     | HIERARCHICAL LSTM BEST HYPERPARAMETERS
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 20:55:03 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 20:55:03 | training | log_config | INFO     |   batch_size                     : 32
2025-12-14 20:55:03 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 20:55:03 | training | log_config | INFO     |   grad_clip                      : 0.5
2025-12-14 20:55:03 | training | log_config | INFO     |   s1_hidden_size                 : 192
2025-12-14 20:55:03 | training | log_config | INFO     |   s1_num_layers                  : 2
2025-12-14 20:55:03 | training | log_config | INFO     |   s1_dropout                     : 0.5
2025-12-14 20:55:03 | training | log_config | INFO     |   s1_bidirectional               : True
2025-12-14 20:55:03 | training | log_config | INFO     |   s2_hidden_size                 : 64
2025-12-14 20:55:03 | training | log_config | INFO     |   s2_num_layers                  : 3
2025-12-14 20:55:03 | training | log_config | INFO     |   s2_dropout                     : 0.3
2025-12-14 20:55:03 | training | log_config | INFO     |   s2_bidirectional               : False
2025-12-14 20:55:03 | training | log_config | INFO     |   use_class_weights              : True
2025-12-14 20:55:03 | training | log_config | INFO     |   class_weight_scale             : 0.6
2025-12-14 20:55:03 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 20:55:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:03 | training | log_model_summary | INFO     | Model: HierarchicalClassifier
2025-12-14 20:55:03 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 20:55:03 | training | log_model_summary | INFO     | 
2025-12-14 20:55:03 | training | log_model_summary | INFO     |   stage1                    :  1,193,091 params ( 1,193,091 trainable)
2025-12-14 20:55:03 | training | log_model_summary | INFO     |   stage2                    :     84,675 params (    84,675 trainable)
2025-12-14 20:55:03 | training | log_model_summary | INFO     | 
2025-12-14 20:55:03 | training | log_model_summary | INFO     | Total parameters          :  1,277,766
2025-12-14 20:55:03 | training | log_model_summary | INFO     | Trainable parameters      :  1,277,766
2025-12-14 20:55:03 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 20:55:03 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:55:04 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.3949 | Train Acc: 0.2390 | Val Loss: 1.9860 | Val Acc: 0.2101 | LR: 0.000288 | Val F1: 0.1914
2025-12-14 20:55:05 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.3152 | Train Acc: 0.2117 | Val Loss: 2.0607 | Val Acc: 0.2404 | LR: 0.000288 | Val F1: 0.2729
2025-12-14 20:55:06 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.3001 | Train Acc: 0.1852 | Val Loss: 1.9873 | Val Acc: 0.2655 | LR: 0.000288 | Val F1: 0.3146
2025-12-14 20:55:06 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.2574 | Train Acc: 0.1779 | Val Loss: 1.9836 | Val Acc: 0.2182 | LR: 0.000287 | Val F1: 0.2417
2025-12-14 20:55:07 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.2528 | Train Acc: 0.2352 | Val Loss: 1.9733 | Val Acc: 0.2745 | LR: 0.000287 | Val F1: 0.3110
2025-12-14 20:55:08 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.2219 | Train Acc: 0.2438 | Val Loss: 2.0731 | Val Acc: 0.1727 | LR: 0.000286 | Val F1: 0.1778
2025-12-14 20:55:09 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.2270 | Train Acc: 0.2021 | Val Loss: 2.0306 | Val Acc: 0.3899 | LR: 0.000285 | Val F1: 0.4496
2025-12-14 20:55:09 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.2329 | Train Acc: 0.2830 | Val Loss: 1.9791 | Val Acc: 0.1543 | LR: 0.000285 | Val F1: 0.1320
2025-12-14 20:55:10 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.2236 | Train Acc: 0.2172 | Val Loss: 2.0190 | Val Acc: 0.4240 | LR: 0.000283 | Val F1: 0.4919
2025-12-14 20:55:11 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.2004 | Train Acc: 0.3239 | Val Loss: 1.9722 | Val Acc: 0.1903 | LR: 0.000282 | Val F1: 0.2052
2025-12-14 20:55:12 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.2156 | Train Acc: 0.2425 | Val Loss: 2.0374 | Val Acc: 0.2116 | LR: 0.000281 | Val F1: 0.2280
2025-12-14 20:55:13 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.1825 | Train Acc: 0.2420 | Val Loss: 2.0586 | Val Acc: 0.1573 | LR: 0.000280 | Val F1: 0.1609
2025-12-14 20:55:14 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.1836 | Train Acc: 0.2435 | Val Loss: 1.9628 | Val Acc: 0.2378 | LR: 0.000278 | Val F1: 0.2691
2025-12-14 20:55:14 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.1835 | Train Acc: 0.2686 | Val Loss: 1.9564 | Val Acc: 0.4023 | LR: 0.000276 | Val F1: 0.4581
2025-12-14 20:55:15 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.1889 | Train Acc: 0.2617 | Val Loss: 1.9553 | Val Acc: 0.1901 | LR: 0.000274 | Val F1: 0.2030
2025-12-14 20:55:16 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.1606 | Train Acc: 0.2830 | Val Loss: 2.0343 | Val Acc: 0.2225 | LR: 0.000272 | Val F1: 0.2518
2025-12-14 20:55:17 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.1493 | Train Acc: 0.3194 | Val Loss: 2.0108 | Val Acc: 0.1785 | LR: 0.000270 | Val F1: 0.1895
2025-12-14 20:55:18 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.1637 | Train Acc: 0.2176 | Val Loss: 2.0132 | Val Acc: 0.3490 | LR: 0.000268 | Val F1: 0.4048
2025-12-14 20:55:19 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.1246 | Train Acc: 0.3117 | Val Loss: 1.9923 | Val Acc: 0.2252 | LR: 0.000266 | Val F1: 0.2524
2025-12-14 20:55:19 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.1473 | Train Acc: 0.2826 | Val Loss: 1.9858 | Val Acc: 0.3020 | LR: 0.000263 | Val F1: 0.3570
2025-12-14 20:55:20 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.1418 | Train Acc: 0.2769 | Val Loss: 1.9281 | Val Acc: 0.3245 | LR: 0.000261 | Val F1: 0.3781
2025-12-14 20:55:21 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.1467 | Train Acc: 0.2208 | Val Loss: 2.0474 | Val Acc: 0.3464 | LR: 0.000258 | Val F1: 0.4137
2025-12-14 20:55:22 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.1245 | Train Acc: 0.3384 | Val Loss: 2.0052 | Val Acc: 0.2829 | LR: 0.000255 | Val F1: 0.3234
2025-12-14 20:55:23 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.1411 | Train Acc: 0.3000 | Val Loss: 2.0087 | Val Acc: 0.3490 | LR: 0.000252 | Val F1: 0.4004
2025-12-14 20:55:23 | training | train_model | INFO     | Early stopping triggered at epoch 24 due to no improvement in validation F1.
2025-12-14 20:55:23 | training | train_model | INFO     | Training for Hierarchical LSTM completed in 19.7s. Best validation F1: 0.4919
2025-12-14 20:55:23 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/hierarchical_v1/best_model.pth
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 20:55:23 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 20:55:23 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 20:55:23 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 20:55:23 | training | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 20:55:23 | training | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 20:55:23 | training | log_config | INFO     |   patience                       : 15
2025-12-14 20:55:23 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 20:55:23 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_config | INFO     |   d_model                        : 128
2025-12-14 20:55:23 | training | log_config | INFO     |   nhead                          : 4
2025-12-14 20:55:23 | training | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 20:55:23 | training | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 20:55:23 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 20:55:23 | training | log_config | INFO     |   learnable_pe                   : False
2025-12-14 20:55:23 | training | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 20:55:23 | training | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 20:55:23 | training | log_config | INFO     |   batch_size                     : 8
2025-12-14 20:55:23 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 20:55:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:55:23 | training | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 20:55:23 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 20:55:23 | training | log_model_summary | INFO     | 
2025-12-14 20:55:23 | training | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 20:55:23 | training | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 20:55:23 | training | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 20:55:23 | training | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 20:55:23 | training | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 20:55:23 | training | log_model_summary | INFO     | 
2025-12-14 20:55:23 | training | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 20:55:23 | training | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 20:55:23 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 20:55:23 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:55:25 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.7402 | Train Acc: 0.1217 | Val Loss: 2.1374 | Val Acc: 0.2666 | LR: 0.000930 | Val F1: 0.3008
2025-12-14 20:55:26 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.4959 | Train Acc: 0.1478 | Val Loss: 2.1870 | Val Acc: 0.0619 | LR: 0.000930 | Val F1: 0.0309
2025-12-14 20:55:27 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.4098 | Train Acc: 0.1467 | Val Loss: 2.2735 | Val Acc: 0.0118 | LR: 0.000929 | Val F1: 0.0091
2025-12-14 20:55:29 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.3952 | Train Acc: 0.1592 | Val Loss: 2.2967 | Val Acc: 0.5253 | LR: 0.000928 | Val F1: 0.5448
2025-12-14 20:55:30 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.3772 | Train Acc: 0.2387 | Val Loss: 2.0897 | Val Acc: 0.5460 | LR: 0.000926 | Val F1: 0.5629
2025-12-14 20:55:31 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.3351 | Train Acc: 0.2571 | Val Loss: 2.0613 | Val Acc: 0.0963 | LR: 0.000924 | Val F1: 0.0456
2025-12-14 20:55:32 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.3608 | Train Acc: 0.2400 | Val Loss: 2.2677 | Val Acc: 0.4595 | LR: 0.000922 | Val F1: 0.5228
2025-12-14 20:55:33 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.3280 | Train Acc: 0.2571 | Val Loss: 2.0224 | Val Acc: 0.5381 | LR: 0.000919 | Val F1: 0.5652
2025-12-14 20:55:35 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.3256 | Train Acc: 0.2328 | Val Loss: 2.3645 | Val Acc: 0.4981 | LR: 0.000915 | Val F1: 0.5473
2025-12-14 20:55:36 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.3346 | Train Acc: 0.2884 | Val Loss: 2.2241 | Val Acc: 0.0946 | LR: 0.000912 | Val F1: 0.0606
2025-12-14 20:55:37 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.3079 | Train Acc: 0.2946 | Val Loss: 2.1268 | Val Acc: 0.3738 | LR: 0.000907 | Val F1: 0.4477
2025-12-14 20:55:39 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.2992 | Train Acc: 0.3063 | Val Loss: 2.1457 | Val Acc: 0.0781 | LR: 0.000903 | Val F1: 0.0599
2025-12-14 20:55:40 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.2994 | Train Acc: 0.3100 | Val Loss: 2.1047 | Val Acc: 0.1685 | LR: 0.000897 | Val F1: 0.1742
2025-12-14 20:55:42 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.2842 | Train Acc: 0.2777 | Val Loss: 2.0855 | Val Acc: 0.3612 | LR: 0.000892 | Val F1: 0.4269
2025-12-14 20:55:43 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.2711 | Train Acc: 0.3368 | Val Loss: 2.1809 | Val Acc: 0.0872 | LR: 0.000886 | Val F1: 0.1002
2025-12-14 20:55:44 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.2780 | Train Acc: 0.2718 | Val Loss: 2.0283 | Val Acc: 0.2968 | LR: 0.000879 | Val F1: 0.3705
2025-12-14 20:55:45 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.2919 | Train Acc: 0.3768 | Val Loss: 1.9468 | Val Acc: 0.4935 | LR: 0.000873 | Val F1: 0.5291
2025-12-14 20:55:47 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.2734 | Train Acc: 0.4133 | Val Loss: 2.1056 | Val Acc: 0.3358 | LR: 0.000865 | Val F1: 0.4299
2025-12-14 20:55:48 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.2774 | Train Acc: 0.2876 | Val Loss: 2.1090 | Val Acc: 0.5342 | LR: 0.000858 | Val F1: 0.5828
2025-12-14 20:55:50 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.2601 | Train Acc: 0.4002 | Val Loss: 2.0991 | Val Acc: 0.4229 | LR: 0.000850 | Val F1: 0.4993
2025-12-14 20:55:51 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.2497 | Train Acc: 0.3157 | Val Loss: 2.0718 | Val Acc: 0.3020 | LR: 0.000841 | Val F1: 0.3878
2025-12-14 20:55:52 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.2580 | Train Acc: 0.4132 | Val Loss: 1.9739 | Val Acc: 0.4622 | LR: 0.000833 | Val F1: 0.5270
2025-12-14 20:55:54 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.2483 | Train Acc: 0.3064 | Val Loss: 2.0120 | Val Acc: 0.5478 | LR: 0.000823 | Val F1: 0.5733
2025-12-14 20:55:55 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.2631 | Train Acc: 0.3683 | Val Loss: 2.0258 | Val Acc: 0.4151 | LR: 0.000814 | Val F1: 0.4849
2025-12-14 20:55:57 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 2.2470 | Train Acc: 0.3918 | Val Loss: 2.0717 | Val Acc: 0.3485 | LR: 0.000804 | Val F1: 0.4231
2025-12-14 20:55:58 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 2.2429 | Train Acc: 0.3449 | Val Loss: 2.1144 | Val Acc: 0.5780 | LR: 0.000794 | Val F1: 0.6053
2025-12-14 20:56:00 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 2.2375 | Train Acc: 0.4032 | Val Loss: 2.0546 | Val Acc: 0.5498 | LR: 0.000783 | Val F1: 0.5905
2025-12-14 20:56:01 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 2.2391 | Train Acc: 0.4200 | Val Loss: 2.0188 | Val Acc: 0.3359 | LR: 0.000773 | Val F1: 0.4361
2025-12-14 20:56:03 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 2.2327 | Train Acc: 0.2907 | Val Loss: 1.9870 | Val Acc: 0.3590 | LR: 0.000762 | Val F1: 0.4444
2025-12-14 20:56:04 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 2.2386 | Train Acc: 0.3917 | Val Loss: 1.9732 | Val Acc: 0.5076 | LR: 0.000750 | Val F1: 0.5643
2025-12-14 20:56:05 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 2.2174 | Train Acc: 0.3499 | Val Loss: 1.9999 | Val Acc: 0.4157 | LR: 0.000739 | Val F1: 0.4706
2025-12-14 20:56:07 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 2.2348 | Train Acc: 0.3402 | Val Loss: 1.9940 | Val Acc: 0.5273 | LR: 0.000727 | Val F1: 0.5529
2025-12-14 20:56:08 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 2.2364 | Train Acc: 0.3426 | Val Loss: 1.9889 | Val Acc: 0.3642 | LR: 0.000714 | Val F1: 0.4165
2025-12-14 20:56:10 | training | log_epoch | INFO     | Epoch [  34/100] | Train Loss: 2.2109 | Train Acc: 0.3684 | Val Loss: 1.9684 | Val Acc: 0.4154 | LR: 0.000702 | Val F1: 0.4584
2025-12-14 20:56:11 | training | log_epoch | INFO     | Epoch [  35/100] | Train Loss: 2.2285 | Train Acc: 0.3570 | Val Loss: 2.0073 | Val Acc: 0.4233 | LR: 0.000689 | Val F1: 0.4902
2025-12-14 20:56:12 | training | log_epoch | INFO     | Epoch [  36/100] | Train Loss: 2.2225 | Train Acc: 0.3888 | Val Loss: 2.0480 | Val Acc: 0.4255 | LR: 0.000676 | Val F1: 0.5028
2025-12-14 20:56:14 | training | log_epoch | INFO     | Epoch [  37/100] | Train Loss: 2.2083 | Train Acc: 0.3957 | Val Loss: 2.0777 | Val Acc: 0.6352 | LR: 0.000663 | Val F1: 0.6329
2025-12-14 20:56:15 | training | log_epoch | INFO     | Epoch [  38/100] | Train Loss: 2.2149 | Train Acc: 0.3912 | Val Loss: 2.1173 | Val Acc: 0.2447 | LR: 0.000650 | Val F1: 0.3120
2025-12-14 20:56:17 | training | log_epoch | INFO     | Epoch [  39/100] | Train Loss: 2.1937 | Train Acc: 0.2805 | Val Loss: 2.1149 | Val Acc: 0.4516 | LR: 0.000636 | Val F1: 0.5272
2025-12-14 20:56:18 | training | log_epoch | INFO     | Epoch [  40/100] | Train Loss: 2.2146 | Train Acc: 0.4470 | Val Loss: 2.0805 | Val Acc: 0.3289 | LR: 0.000623 | Val F1: 0.4160
2025-12-14 20:56:19 | training | log_epoch | INFO     | Epoch [  41/100] | Train Loss: 2.2278 | Train Acc: 0.3690 | Val Loss: 2.0339 | Val Acc: 0.4866 | LR: 0.000609 | Val F1: 0.5495
2025-12-14 20:56:21 | training | log_epoch | INFO     | Epoch [  42/100] | Train Loss: 2.1858 | Train Acc: 0.3544 | Val Loss: 2.0835 | Val Acc: 0.6259 | LR: 0.000595 | Val F1: 0.6304
2025-12-14 20:56:22 | training | log_epoch | INFO     | Epoch [  43/100] | Train Loss: 2.1987 | Train Acc: 0.4072 | Val Loss: 2.0043 | Val Acc: 0.3992 | LR: 0.000581 | Val F1: 0.4716
2025-12-14 20:56:24 | training | log_epoch | INFO     | Epoch [  44/100] | Train Loss: 2.2131 | Train Acc: 0.4252 | Val Loss: 2.0462 | Val Acc: 0.4686 | LR: 0.000567 | Val F1: 0.5226
2025-12-14 20:56:25 | training | log_epoch | INFO     | Epoch [  45/100] | Train Loss: 2.1985 | Train Acc: 0.3814 | Val Loss: 2.0696 | Val Acc: 0.4043 | LR: 0.000553 | Val F1: 0.4848
2025-12-14 20:56:26 | training | log_epoch | INFO     | Epoch [  46/100] | Train Loss: 2.1972 | Train Acc: 0.3801 | Val Loss: 2.0398 | Val Acc: 0.3849 | LR: 0.000538 | Val F1: 0.4580
2025-12-14 20:56:28 | training | log_epoch | INFO     | Epoch [  47/100] | Train Loss: 2.2099 | Train Acc: 0.4078 | Val Loss: 2.0713 | Val Acc: 0.4547 | LR: 0.000524 | Val F1: 0.5302
2025-12-14 20:56:29 | training | log_epoch | INFO     | Epoch [  48/100] | Train Loss: 2.1921 | Train Acc: 0.3917 | Val Loss: 2.0124 | Val Acc: 0.3331 | LR: 0.000509 | Val F1: 0.3998
2025-12-14 20:56:30 | training | log_epoch | INFO     | Epoch [  49/100] | Train Loss: 2.1792 | Train Acc: 0.3631 | Val Loss: 2.0387 | Val Acc: 0.5998 | LR: 0.000495 | Val F1: 0.6184
2025-12-14 20:56:32 | training | log_epoch | INFO     | Epoch [  50/100] | Train Loss: 2.1819 | Train Acc: 0.4221 | Val Loss: 2.0264 | Val Acc: 0.4905 | LR: 0.000480 | Val F1: 0.5345
2025-12-14 20:56:33 | training | log_epoch | INFO     | Epoch [  51/100] | Train Loss: 2.1827 | Train Acc: 0.3483 | Val Loss: 2.0645 | Val Acc: 0.4030 | LR: 0.000466 | Val F1: 0.4793
2025-12-14 20:56:34 | training | log_epoch | INFO     | Epoch [  52/100] | Train Loss: 2.1825 | Train Acc: 0.3688 | Val Loss: 2.0803 | Val Acc: 0.4099 | LR: 0.000451 | Val F1: 0.4910
2025-12-14 20:56:34 | training | train_model | INFO     | Early stopping triggered at epoch 52 due to no improvement in validation F1.
2025-12-14 20:56:34 | training | train_model | INFO     | Training for Transformer completed in 71.2s. Best validation F1: 0.6329
2025-12-14 20:56:34 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 20:56:34 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:56:34 | training | log_header | INFO     | TRAINING: STATISTICAL BASELINE
2025-12-14 20:56:34 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:56:35 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:56:35 | training | log_header | INFO     | BASELINE TEST SET METRICS
2025-12-14 20:56:35 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:56:35 | training | log_evaluation | INFO     |   accuracy                  : 0.4997
2025-12-14 20:56:35 | training | log_evaluation | INFO     |   f1_weighted               : 0.4644
2025-12-14 20:56:35 | training | log_evaluation | INFO     |   f1_macro                  : 0.1069
2025-12-14 20:56:35 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:56:35 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:56:35 | training | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 20:56:35 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:56:35 | training | train_main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 20:56:35 | training | train_main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 20:56:35 | training | train_main | INFO     | LSTM_v2 (SeqLabeling)     0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 20:56:35 | training | train_main | INFO     | Hierarchical LSTM         0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 20:56:35 | training | train_main | INFO     | Statistical Baseline      0.4997       0.4644          0.1069       0.0000           0.0000            
2025-12-14 20:56:35 | training | train_main | INFO     | Transformer               0.1037       0.0195          0.0269       1.0000           1.0000            
2025-12-14 20:56:35 | training | train_main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_205454.json
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | CONFIGURATION
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 20:58:37 | training | log_config | INFO     |   Epochs                         : 100
2025-12-14 20:58:37 | training | log_config | INFO     |   Device                         : cuda
2025-12-14 20:58:37 | training | log_config | INFO     |   Log Level                      : INFO
2025-12-14 20:58:37 | training | log_config | INFO     |   Augmentation                   : True
2025-12-14 20:58:37 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 20:58:37 | training | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 20:58:37 | training | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 20:58:37 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 20:58:37 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 20:58:37 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 20:58:37 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 20:58:37 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 20:58:37 | training | log_config | INFO     |   patience                       : 15
2025-12-14 20:58:37 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 20:58:37 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_config | INFO     |   hidden_size                    : 96
2025-12-14 20:58:37 | training | log_config | INFO     |   num_layers                     : 2
2025-12-14 20:58:37 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 20:58:37 | training | log_config | INFO     |   bidirectional                  : True
2025-12-14 20:58:37 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 20:58:37 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 20:58:37 | training | log_config | INFO     |   batch_size                     : 16
2025-12-14 20:58:37 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 20:58:37 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 20:58:37 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 20:58:37 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:37 | training | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 20:58:37 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 20:58:37 | training | log_model_summary | INFO     | 
2025-12-14 20:58:37 | training | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 20:58:37 | training | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 20:58:37 | training | log_model_summary | INFO     | 
2025-12-14 20:58:37 | training | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 20:58:37 | training | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 20:58:37 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 20:58:37 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:58:39 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 20:58:39 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 20:58:39 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 20:58:40 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 20:58:40 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 20:58:40 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 20:58:40 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 20:58:40 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 20:58:41 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 20:58:41 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 20:58:41 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 20:58:41 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 20:58:41 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 20:58:42 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 20:58:42 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 20:58:42 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 20:58:42 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 20:58:43 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 20:58:43 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 20:58:43 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 20:58:43 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 20:58:43 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 20:58:44 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 20:58:44 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 20:58:44 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 20:58:44 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 20:58:44 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 20:58:45 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 20:58:45 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 20:58:45 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 20:58:45 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 20:58:46 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 20:58:46 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 20:58:46 | training | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 20:58:46 | training | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 7.5s. Best validation F1: 0.6429
2025-12-14 20:58:46 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 20:58:46 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 20:58:46 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 20:58:46 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_header | INFO     | TRAINING: HIERARCHICAL LSTM
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_header | INFO     | HIERARCHICAL LSTM CONFIG
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 20:58:46 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 20:58:46 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 20:58:46 | training | log_config | INFO     |   patience                       : 15
2025-12-14 20:58:46 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 20:58:46 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_header | INFO     | HIERARCHICAL LSTM BEST HYPERPARAMETERS
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 20:58:46 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 20:58:46 | training | log_config | INFO     |   batch_size                     : 32
2025-12-14 20:58:46 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 20:58:46 | training | log_config | INFO     |   grad_clip                      : 0.5
2025-12-14 20:58:46 | training | log_config | INFO     |   s1_hidden_size                 : 192
2025-12-14 20:58:46 | training | log_config | INFO     |   s1_num_layers                  : 2
2025-12-14 20:58:46 | training | log_config | INFO     |   s1_dropout                     : 0.5
2025-12-14 20:58:46 | training | log_config | INFO     |   s1_bidirectional               : True
2025-12-14 20:58:46 | training | log_config | INFO     |   s2_hidden_size                 : 64
2025-12-14 20:58:46 | training | log_config | INFO     |   s2_num_layers                  : 3
2025-12-14 20:58:46 | training | log_config | INFO     |   s2_dropout                     : 0.3
2025-12-14 20:58:46 | training | log_config | INFO     |   s2_bidirectional               : False
2025-12-14 20:58:46 | training | log_config | INFO     |   use_class_weights              : True
2025-12-14 20:58:46 | training | log_config | INFO     |   class_weight_scale             : 0.6
2025-12-14 20:58:46 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 20:58:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:58:46 | training | log_model_summary | INFO     | Model: HierarchicalClassifier
2025-12-14 20:58:46 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 20:58:46 | training | log_model_summary | INFO     | 
2025-12-14 20:58:46 | training | log_model_summary | INFO     |   stage1                    :  1,193,091 params ( 1,193,091 trainable)
2025-12-14 20:58:46 | training | log_model_summary | INFO     |   stage2                    :     84,675 params (    84,675 trainable)
2025-12-14 20:58:46 | training | log_model_summary | INFO     | 
2025-12-14 20:58:46 | training | log_model_summary | INFO     | Total parameters          :  1,277,766
2025-12-14 20:58:46 | training | log_model_summary | INFO     | Trainable parameters      :  1,277,766
2025-12-14 20:58:46 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 20:58:46 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:58:47 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.3949 | Train Acc: 0.2390 | Val Loss: 1.9860 | Val Acc: 0.2101 | LR: 0.000288 | Val F1: 0.1914
2025-12-14 20:58:47 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.3152 | Train Acc: 0.2117 | Val Loss: 2.0607 | Val Acc: 0.2404 | LR: 0.000288 | Val F1: 0.2729
2025-12-14 20:58:48 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.3001 | Train Acc: 0.1852 | Val Loss: 1.9873 | Val Acc: 0.2655 | LR: 0.000288 | Val F1: 0.3146
2025-12-14 20:58:49 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.2574 | Train Acc: 0.1779 | Val Loss: 1.9836 | Val Acc: 0.2182 | LR: 0.000287 | Val F1: 0.2417
2025-12-14 20:58:50 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.2528 | Train Acc: 0.2352 | Val Loss: 1.9733 | Val Acc: 0.2745 | LR: 0.000287 | Val F1: 0.3110
2025-12-14 20:58:51 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.2219 | Train Acc: 0.2438 | Val Loss: 2.0731 | Val Acc: 0.1727 | LR: 0.000286 | Val F1: 0.1778
2025-12-14 20:58:51 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.2270 | Train Acc: 0.2021 | Val Loss: 2.0306 | Val Acc: 0.3899 | LR: 0.000285 | Val F1: 0.4496
2025-12-14 20:58:52 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.2329 | Train Acc: 0.2830 | Val Loss: 1.9791 | Val Acc: 0.1543 | LR: 0.000285 | Val F1: 0.1320
2025-12-14 20:58:53 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.2236 | Train Acc: 0.2172 | Val Loss: 2.0190 | Val Acc: 0.4240 | LR: 0.000283 | Val F1: 0.4919
2025-12-14 20:58:54 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.2004 | Train Acc: 0.3239 | Val Loss: 1.9722 | Val Acc: 0.1903 | LR: 0.000282 | Val F1: 0.2052
2025-12-14 20:58:55 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.2156 | Train Acc: 0.2425 | Val Loss: 2.0374 | Val Acc: 0.2116 | LR: 0.000281 | Val F1: 0.2280
2025-12-14 20:58:55 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.1825 | Train Acc: 0.2420 | Val Loss: 2.0586 | Val Acc: 0.1573 | LR: 0.000280 | Val F1: 0.1609
2025-12-14 20:58:56 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.1836 | Train Acc: 0.2435 | Val Loss: 1.9628 | Val Acc: 0.2378 | LR: 0.000278 | Val F1: 0.2691
2025-12-14 20:58:57 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.1835 | Train Acc: 0.2686 | Val Loss: 1.9564 | Val Acc: 0.4023 | LR: 0.000276 | Val F1: 0.4581
2025-12-14 20:58:58 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.1889 | Train Acc: 0.2617 | Val Loss: 1.9553 | Val Acc: 0.1901 | LR: 0.000274 | Val F1: 0.2030
2025-12-14 20:58:59 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.1606 | Train Acc: 0.2830 | Val Loss: 2.0343 | Val Acc: 0.2225 | LR: 0.000272 | Val F1: 0.2518
2025-12-14 20:58:59 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.1493 | Train Acc: 0.3194 | Val Loss: 2.0108 | Val Acc: 0.1785 | LR: 0.000270 | Val F1: 0.1895
2025-12-14 20:59:00 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.1637 | Train Acc: 0.2176 | Val Loss: 2.0132 | Val Acc: 0.3490 | LR: 0.000268 | Val F1: 0.4048
2025-12-14 20:59:01 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.1246 | Train Acc: 0.3117 | Val Loss: 1.9923 | Val Acc: 0.2252 | LR: 0.000266 | Val F1: 0.2524
2025-12-14 20:59:02 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.1473 | Train Acc: 0.2826 | Val Loss: 1.9858 | Val Acc: 0.3020 | LR: 0.000263 | Val F1: 0.3570
2025-12-14 20:59:03 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.1418 | Train Acc: 0.2769 | Val Loss: 1.9281 | Val Acc: 0.3245 | LR: 0.000261 | Val F1: 0.3781
2025-12-14 20:59:04 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.1467 | Train Acc: 0.2208 | Val Loss: 2.0474 | Val Acc: 0.3464 | LR: 0.000258 | Val F1: 0.4137
2025-12-14 20:59:05 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.1245 | Train Acc: 0.3384 | Val Loss: 2.0052 | Val Acc: 0.2829 | LR: 0.000255 | Val F1: 0.3234
2025-12-14 20:59:05 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.1411 | Train Acc: 0.3000 | Val Loss: 2.0087 | Val Acc: 0.3490 | LR: 0.000252 | Val F1: 0.4004
2025-12-14 20:59:05 | training | train_model | INFO     | Early stopping triggered at epoch 24 due to no improvement in validation F1.
2025-12-14 20:59:05 | training | train_model | INFO     | Training for Hierarchical LSTM completed in 19.6s. Best validation F1: 0.4919
2025-12-14 20:59:05 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/hierarchical_v1/best_model.pth
2025-12-14 20:59:05 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:05 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 20:59:05 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:05 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 20:59:06 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 20:59:06 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 20:59:06 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 20:59:06 | training | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 20:59:06 | training | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 20:59:06 | training | log_config | INFO     |   patience                       : 15
2025-12-14 20:59:06 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 20:59:06 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_config | INFO     |   d_model                        : 128
2025-12-14 20:59:06 | training | log_config | INFO     |   nhead                          : 4
2025-12-14 20:59:06 | training | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 20:59:06 | training | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 20:59:06 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 20:59:06 | training | log_config | INFO     |   learnable_pe                   : False
2025-12-14 20:59:06 | training | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 20:59:06 | training | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 20:59:06 | training | log_config | INFO     |   batch_size                     : 8
2025-12-14 20:59:06 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 20:59:06 | training | log_separator | INFO     | ======================================================================
2025-12-14 20:59:06 | training | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 20:59:06 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 20:59:06 | training | log_model_summary | INFO     | 
2025-12-14 20:59:06 | training | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 20:59:06 | training | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 20:59:06 | training | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 20:59:06 | training | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 20:59:06 | training | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 20:59:06 | training | log_model_summary | INFO     | 
2025-12-14 20:59:06 | training | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 20:59:06 | training | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 20:59:06 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 20:59:06 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 20:59:07 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.7402 | Train Acc: 0.1217 | Val Loss: 2.1374 | Val Acc: 0.2666 | LR: 0.000930 | Val F1: 0.3008
2025-12-14 20:59:08 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.4959 | Train Acc: 0.1478 | Val Loss: 2.1870 | Val Acc: 0.0619 | LR: 0.000930 | Val F1: 0.0309
2025-12-14 20:59:09 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.4098 | Train Acc: 0.1467 | Val Loss: 2.2735 | Val Acc: 0.0118 | LR: 0.000929 | Val F1: 0.0091
2025-12-14 20:59:11 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.3952 | Train Acc: 0.1592 | Val Loss: 2.2967 | Val Acc: 0.5253 | LR: 0.000928 | Val F1: 0.5448
2025-12-14 20:59:12 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.3772 | Train Acc: 0.2387 | Val Loss: 2.0897 | Val Acc: 0.5460 | LR: 0.000926 | Val F1: 0.5629
2025-12-14 20:59:14 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.3351 | Train Acc: 0.2571 | Val Loss: 2.0613 | Val Acc: 0.0963 | LR: 0.000924 | Val F1: 0.0456
2025-12-14 20:59:15 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.3608 | Train Acc: 0.2400 | Val Loss: 2.2677 | Val Acc: 0.4595 | LR: 0.000922 | Val F1: 0.5228
2025-12-14 20:59:16 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.3280 | Train Acc: 0.2571 | Val Loss: 2.0224 | Val Acc: 0.5381 | LR: 0.000919 | Val F1: 0.5652
2025-12-14 20:59:17 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.3256 | Train Acc: 0.2328 | Val Loss: 2.3645 | Val Acc: 0.4981 | LR: 0.000915 | Val F1: 0.5473
2025-12-14 20:59:18 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.3346 | Train Acc: 0.2884 | Val Loss: 2.2241 | Val Acc: 0.0946 | LR: 0.000912 | Val F1: 0.0606
2025-12-14 20:59:20 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.3079 | Train Acc: 0.2946 | Val Loss: 2.1268 | Val Acc: 0.3738 | LR: 0.000907 | Val F1: 0.4477
2025-12-14 20:59:21 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.2992 | Train Acc: 0.3063 | Val Loss: 2.1457 | Val Acc: 0.0781 | LR: 0.000903 | Val F1: 0.0599
2025-12-14 20:59:22 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.2994 | Train Acc: 0.3100 | Val Loss: 2.1047 | Val Acc: 0.1685 | LR: 0.000897 | Val F1: 0.1742
2025-12-14 20:59:24 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.2842 | Train Acc: 0.2777 | Val Loss: 2.0855 | Val Acc: 0.3612 | LR: 0.000892 | Val F1: 0.4269
2025-12-14 20:59:25 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.2711 | Train Acc: 0.3368 | Val Loss: 2.1809 | Val Acc: 0.0872 | LR: 0.000886 | Val F1: 0.1002
2025-12-14 20:59:27 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.2780 | Train Acc: 0.2718 | Val Loss: 2.0283 | Val Acc: 0.2968 | LR: 0.000879 | Val F1: 0.3705
2025-12-14 20:59:28 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.2919 | Train Acc: 0.3768 | Val Loss: 1.9468 | Val Acc: 0.4935 | LR: 0.000873 | Val F1: 0.5291
2025-12-14 20:59:30 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.2734 | Train Acc: 0.4133 | Val Loss: 2.1056 | Val Acc: 0.3358 | LR: 0.000865 | Val F1: 0.4299
2025-12-14 20:59:31 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.2774 | Train Acc: 0.2876 | Val Loss: 2.1090 | Val Acc: 0.5342 | LR: 0.000858 | Val F1: 0.5828
2025-12-14 20:59:32 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.2601 | Train Acc: 0.4002 | Val Loss: 2.0991 | Val Acc: 0.4229 | LR: 0.000850 | Val F1: 0.4993
2025-12-14 20:59:33 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.2497 | Train Acc: 0.3157 | Val Loss: 2.0718 | Val Acc: 0.3020 | LR: 0.000841 | Val F1: 0.3878
2025-12-14 20:59:35 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.2580 | Train Acc: 0.4132 | Val Loss: 1.9739 | Val Acc: 0.4622 | LR: 0.000833 | Val F1: 0.5270
2025-12-14 20:59:36 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.2483 | Train Acc: 0.3064 | Val Loss: 2.0120 | Val Acc: 0.5478 | LR: 0.000823 | Val F1: 0.5733
2025-12-14 20:59:37 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.2631 | Train Acc: 0.3683 | Val Loss: 2.0258 | Val Acc: 0.4151 | LR: 0.000814 | Val F1: 0.4849
2025-12-14 20:59:39 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 2.2470 | Train Acc: 0.3918 | Val Loss: 2.0717 | Val Acc: 0.3485 | LR: 0.000804 | Val F1: 0.4231
2025-12-14 20:59:40 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 2.2429 | Train Acc: 0.3449 | Val Loss: 2.1144 | Val Acc: 0.5780 | LR: 0.000794 | Val F1: 0.6053
2025-12-14 20:59:41 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 2.2375 | Train Acc: 0.4032 | Val Loss: 2.0546 | Val Acc: 0.5498 | LR: 0.000783 | Val F1: 0.5905
2025-12-14 20:59:42 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 2.2391 | Train Acc: 0.4200 | Val Loss: 2.0188 | Val Acc: 0.3359 | LR: 0.000773 | Val F1: 0.4361
2025-12-14 20:59:43 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 2.2327 | Train Acc: 0.2907 | Val Loss: 1.9870 | Val Acc: 0.3590 | LR: 0.000762 | Val F1: 0.4444
2025-12-14 20:59:45 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 2.2386 | Train Acc: 0.3917 | Val Loss: 1.9732 | Val Acc: 0.5076 | LR: 0.000750 | Val F1: 0.5643
2025-12-14 20:59:46 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 2.2174 | Train Acc: 0.3499 | Val Loss: 1.9999 | Val Acc: 0.4157 | LR: 0.000739 | Val F1: 0.4706
2025-12-14 20:59:47 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 2.2348 | Train Acc: 0.3402 | Val Loss: 1.9940 | Val Acc: 0.5273 | LR: 0.000727 | Val F1: 0.5529
2025-12-14 20:59:49 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 2.2364 | Train Acc: 0.3426 | Val Loss: 1.9889 | Val Acc: 0.3642 | LR: 0.000714 | Val F1: 0.4165
2025-12-14 20:59:50 | training | log_epoch | INFO     | Epoch [  34/100] | Train Loss: 2.2109 | Train Acc: 0.3684 | Val Loss: 1.9684 | Val Acc: 0.4154 | LR: 0.000702 | Val F1: 0.4584
2025-12-14 20:59:51 | training | log_epoch | INFO     | Epoch [  35/100] | Train Loss: 2.2285 | Train Acc: 0.3570 | Val Loss: 2.0073 | Val Acc: 0.4233 | LR: 0.000689 | Val F1: 0.4902
2025-12-14 20:59:53 | training | log_epoch | INFO     | Epoch [  36/100] | Train Loss: 2.2225 | Train Acc: 0.3888 | Val Loss: 2.0480 | Val Acc: 0.4255 | LR: 0.000676 | Val F1: 0.5028
2025-12-14 20:59:54 | training | log_epoch | INFO     | Epoch [  37/100] | Train Loss: 2.2083 | Train Acc: 0.3957 | Val Loss: 2.0777 | Val Acc: 0.6352 | LR: 0.000663 | Val F1: 0.6329
2025-12-14 20:59:56 | training | log_epoch | INFO     | Epoch [  38/100] | Train Loss: 2.2149 | Train Acc: 0.3912 | Val Loss: 2.1173 | Val Acc: 0.2447 | LR: 0.000650 | Val F1: 0.3120
2025-12-14 20:59:57 | training | log_epoch | INFO     | Epoch [  39/100] | Train Loss: 2.1937 | Train Acc: 0.2805 | Val Loss: 2.1149 | Val Acc: 0.4516 | LR: 0.000636 | Val F1: 0.5272
2025-12-14 20:59:58 | training | log_epoch | INFO     | Epoch [  40/100] | Train Loss: 2.2146 | Train Acc: 0.4470 | Val Loss: 2.0805 | Val Acc: 0.3289 | LR: 0.000623 | Val F1: 0.4160
2025-12-14 21:00:00 | training | log_epoch | INFO     | Epoch [  41/100] | Train Loss: 2.2278 | Train Acc: 0.3690 | Val Loss: 2.0339 | Val Acc: 0.4866 | LR: 0.000609 | Val F1: 0.5495
2025-12-14 21:00:01 | training | log_epoch | INFO     | Epoch [  42/100] | Train Loss: 2.1858 | Train Acc: 0.3544 | Val Loss: 2.0835 | Val Acc: 0.6259 | LR: 0.000595 | Val F1: 0.6304
2025-12-14 21:00:02 | training | log_epoch | INFO     | Epoch [  43/100] | Train Loss: 2.1987 | Train Acc: 0.4072 | Val Loss: 2.0043 | Val Acc: 0.3992 | LR: 0.000581 | Val F1: 0.4716
2025-12-14 21:00:04 | training | log_epoch | INFO     | Epoch [  44/100] | Train Loss: 2.2131 | Train Acc: 0.4252 | Val Loss: 2.0462 | Val Acc: 0.4686 | LR: 0.000567 | Val F1: 0.5226
2025-12-14 21:00:05 | training | log_epoch | INFO     | Epoch [  45/100] | Train Loss: 2.1985 | Train Acc: 0.3814 | Val Loss: 2.0696 | Val Acc: 0.4043 | LR: 0.000553 | Val F1: 0.4848
2025-12-14 21:00:06 | training | log_epoch | INFO     | Epoch [  46/100] | Train Loss: 2.1972 | Train Acc: 0.3801 | Val Loss: 2.0399 | Val Acc: 0.3849 | LR: 0.000538 | Val F1: 0.4580
2025-12-14 21:00:08 | training | log_epoch | INFO     | Epoch [  47/100] | Train Loss: 2.2099 | Train Acc: 0.4077 | Val Loss: 2.0713 | Val Acc: 0.4548 | LR: 0.000524 | Val F1: 0.5303
2025-12-14 21:00:09 | training | log_epoch | INFO     | Epoch [  48/100] | Train Loss: 2.1921 | Train Acc: 0.3916 | Val Loss: 2.0124 | Val Acc: 0.3331 | LR: 0.000509 | Val F1: 0.3998
2025-12-14 21:00:10 | training | log_epoch | INFO     | Epoch [  49/100] | Train Loss: 2.1792 | Train Acc: 0.3631 | Val Loss: 2.0387 | Val Acc: 0.5998 | LR: 0.000495 | Val F1: 0.6184
2025-12-14 21:00:12 | training | log_epoch | INFO     | Epoch [  50/100] | Train Loss: 2.1819 | Train Acc: 0.4221 | Val Loss: 2.0264 | Val Acc: 0.4905 | LR: 0.000480 | Val F1: 0.5345
2025-12-14 21:00:13 | training | log_epoch | INFO     | Epoch [  51/100] | Train Loss: 2.1827 | Train Acc: 0.3483 | Val Loss: 2.0645 | Val Acc: 0.4029 | LR: 0.000466 | Val F1: 0.4792
2025-12-14 21:00:14 | training | log_epoch | INFO     | Epoch [  52/100] | Train Loss: 2.1825 | Train Acc: 0.3688 | Val Loss: 2.0803 | Val Acc: 0.4099 | LR: 0.000451 | Val F1: 0.4910
2025-12-14 21:00:14 | training | train_model | INFO     | Early stopping triggered at epoch 52 due to no improvement in validation F1.
2025-12-14 21:00:14 | training | train_model | INFO     | Training for Transformer completed in 68.8s. Best validation F1: 0.6329
2025-12-14 21:00:14 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 21:00:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 21:00:14 | training | log_header | INFO     | TRAINING: STATISTICAL BASELINE
2025-12-14 21:00:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 21:00:15 | training | log_separator | INFO     | ======================================================================
2025-12-14 21:00:15 | training | log_header | INFO     | BASELINE TEST SET METRICS
2025-12-14 21:00:15 | training | log_separator | INFO     | ======================================================================
2025-12-14 21:00:15 | training | log_evaluation | INFO     |   accuracy                  : 0.4997
2025-12-14 21:00:15 | training | log_evaluation | INFO     |   f1_weighted               : 0.4644
2025-12-14 21:00:15 | training | log_evaluation | INFO     |   f1_macro                  : 0.1069
2025-12-14 21:00:15 | training | log_separator | INFO     | ======================================================================
2025-12-14 21:00:15 | training | log_separator | INFO     | ======================================================================
2025-12-14 21:00:15 | training | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 21:00:15 | training | log_separator | INFO     | ======================================================================
2025-12-14 21:00:15 | training | train_main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 21:00:15 | training | train_main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 21:00:15 | training | train_main | INFO     | LSTM_v2 (SeqLabeling)     0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 21:00:15 | training | train_main | INFO     | Hierarchical LSTM         0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 21:00:15 | training | train_main | INFO     | Statistical Baseline      0.4997       0.4644          0.1069       0.0000           0.0000            
2025-12-14 21:00:15 | training | train_main | INFO     | Transformer               0.1037       0.0195          0.0269       1.0000           1.0000            
2025-12-14 21:00:15 | training | train_main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_205837.json
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | CONFIGURATION
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 22:09:59 | training | log_config | INFO     |   Epochs                         : 100
2025-12-14 22:09:59 | training | log_config | INFO     |   Device                         : cuda
2025-12-14 22:09:59 | training | log_config | INFO     |   Log Level                      : INFO
2025-12-14 22:09:59 | training | log_config | INFO     |   Augmentation                   : True
2025-12-14 22:09:59 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 22:09:59 | training | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:09:59 | training | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:09:59 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:09:59 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:09:59 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:09:59 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 22:09:59 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 22:09:59 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:09:59 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:09:59 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_config | INFO     |   hidden_size                    : 96
2025-12-14 22:09:59 | training | log_config | INFO     |   num_layers                     : 2
2025-12-14 22:09:59 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 22:09:59 | training | log_config | INFO     |   bidirectional                  : True
2025-12-14 22:09:59 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 22:09:59 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 22:09:59 | training | log_config | INFO     |   batch_size                     : 16
2025-12-14 22:09:59 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 22:09:59 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 22:09:59 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:09:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:09:59 | training | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 22:09:59 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:09:59 | training | log_model_summary | INFO     | 
2025-12-14 22:09:59 | training | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 22:09:59 | training | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 22:09:59 | training | log_model_summary | INFO     | 
2025-12-14 22:09:59 | training | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 22:09:59 | training | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 22:09:59 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:09:59 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:10:01 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 22:10:01 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 22:10:01 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 22:10:02 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 22:10:02 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 22:10:03 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 22:10:03 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 22:10:03 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 22:10:04 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 22:10:04 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 22:10:04 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 22:10:05 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 22:10:05 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 22:10:06 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 22:10:06 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 22:10:06 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 22:10:07 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 22:10:07 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 22:10:08 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 22:10:08 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 22:10:09 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 22:10:09 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 22:10:10 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 22:10:10 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 22:10:11 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 22:10:11 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 22:10:11 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 22:10:12 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 22:10:12 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 22:10:13 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 22:10:13 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 22:10:14 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 22:10:14 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 22:10:14 | training | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 22:10:14 | training | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 14.3s. Best validation F1: 0.6429
2025-12-14 22:10:14 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:10:14 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:10:14 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:10:14 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_header | INFO     | TRAINING: HIERARCHICAL LSTM
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_header | INFO     | HIERARCHICAL LSTM CONFIG
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:10:14 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 22:10:14 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 22:10:14 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:10:14 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:10:14 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_header | INFO     | HIERARCHICAL LSTM BEST HYPERPARAMETERS
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 22:10:14 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 22:10:14 | training | log_config | INFO     |   batch_size                     : 32
2025-12-14 22:10:14 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 22:10:14 | training | log_config | INFO     |   grad_clip                      : 0.5
2025-12-14 22:10:14 | training | log_config | INFO     |   s1_hidden_size                 : 192
2025-12-14 22:10:14 | training | log_config | INFO     |   s1_num_layers                  : 2
2025-12-14 22:10:14 | training | log_config | INFO     |   s1_dropout                     : 0.5
2025-12-14 22:10:14 | training | log_config | INFO     |   s1_bidirectional               : True
2025-12-14 22:10:14 | training | log_config | INFO     |   s2_hidden_size                 : 64
2025-12-14 22:10:14 | training | log_config | INFO     |   s2_num_layers                  : 3
2025-12-14 22:10:14 | training | log_config | INFO     |   s2_dropout                     : 0.3
2025-12-14 22:10:14 | training | log_config | INFO     |   s2_bidirectional               : False
2025-12-14 22:10:14 | training | log_config | INFO     |   use_class_weights              : True
2025-12-14 22:10:14 | training | log_config | INFO     |   class_weight_scale             : 0.6
2025-12-14 22:10:14 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:10:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:14 | training | log_model_summary | INFO     | Model: HierarchicalClassifier
2025-12-14 22:10:14 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:10:14 | training | log_model_summary | INFO     | 
2025-12-14 22:10:14 | training | log_model_summary | INFO     |   stage1                    :  1,193,091 params ( 1,193,091 trainable)
2025-12-14 22:10:14 | training | log_model_summary | INFO     |   stage2                    :     84,675 params (    84,675 trainable)
2025-12-14 22:10:14 | training | log_model_summary | INFO     | 
2025-12-14 22:10:14 | training | log_model_summary | INFO     | Total parameters          :  1,277,766
2025-12-14 22:10:14 | training | log_model_summary | INFO     | Trainable parameters      :  1,277,766
2025-12-14 22:10:14 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:10:14 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:10:16 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.3949 | Train Acc: 0.2390 | Val Loss: 1.9860 | Val Acc: 0.2101 | LR: 0.000288 | Val F1: 0.1914
2025-12-14 22:10:17 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.3152 | Train Acc: 0.2117 | Val Loss: 2.0607 | Val Acc: 0.2404 | LR: 0.000288 | Val F1: 0.2729
2025-12-14 22:10:18 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.3001 | Train Acc: 0.1852 | Val Loss: 1.9873 | Val Acc: 0.2655 | LR: 0.000288 | Val F1: 0.3146
2025-12-14 22:10:20 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.2574 | Train Acc: 0.1779 | Val Loss: 1.9836 | Val Acc: 0.2182 | LR: 0.000287 | Val F1: 0.2417
2025-12-14 22:10:21 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.2528 | Train Acc: 0.2352 | Val Loss: 1.9733 | Val Acc: 0.2745 | LR: 0.000287 | Val F1: 0.3110
2025-12-14 22:10:22 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.2219 | Train Acc: 0.2438 | Val Loss: 2.0731 | Val Acc: 0.1727 | LR: 0.000286 | Val F1: 0.1778
2025-12-14 22:10:23 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.2270 | Train Acc: 0.2021 | Val Loss: 2.0306 | Val Acc: 0.3899 | LR: 0.000285 | Val F1: 0.4496
2025-12-14 22:10:25 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.2329 | Train Acc: 0.2830 | Val Loss: 1.9791 | Val Acc: 0.1543 | LR: 0.000285 | Val F1: 0.1320
2025-12-14 22:10:26 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.2236 | Train Acc: 0.2172 | Val Loss: 2.0190 | Val Acc: 0.4240 | LR: 0.000283 | Val F1: 0.4919
2025-12-14 22:10:27 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.2004 | Train Acc: 0.3239 | Val Loss: 1.9722 | Val Acc: 0.1903 | LR: 0.000282 | Val F1: 0.2052
2025-12-14 22:10:29 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.2156 | Train Acc: 0.2425 | Val Loss: 2.0374 | Val Acc: 0.2116 | LR: 0.000281 | Val F1: 0.2280
2025-12-14 22:10:30 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.1825 | Train Acc: 0.2420 | Val Loss: 2.0586 | Val Acc: 0.1573 | LR: 0.000280 | Val F1: 0.1609
2025-12-14 22:10:31 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.1836 | Train Acc: 0.2435 | Val Loss: 1.9628 | Val Acc: 0.2378 | LR: 0.000278 | Val F1: 0.2691
2025-12-14 22:10:32 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.1835 | Train Acc: 0.2686 | Val Loss: 1.9564 | Val Acc: 0.4023 | LR: 0.000276 | Val F1: 0.4581
2025-12-14 22:10:34 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.1889 | Train Acc: 0.2617 | Val Loss: 1.9553 | Val Acc: 0.1901 | LR: 0.000274 | Val F1: 0.2030
2025-12-14 22:10:35 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.1606 | Train Acc: 0.2830 | Val Loss: 2.0343 | Val Acc: 0.2225 | LR: 0.000272 | Val F1: 0.2518
2025-12-14 22:10:36 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.1493 | Train Acc: 0.3194 | Val Loss: 2.0108 | Val Acc: 0.1785 | LR: 0.000270 | Val F1: 0.1895
2025-12-14 22:10:38 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.1637 | Train Acc: 0.2176 | Val Loss: 2.0132 | Val Acc: 0.3490 | LR: 0.000268 | Val F1: 0.4048
2025-12-14 22:10:39 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.1246 | Train Acc: 0.3117 | Val Loss: 1.9923 | Val Acc: 0.2252 | LR: 0.000266 | Val F1: 0.2524
2025-12-14 22:10:40 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.1473 | Train Acc: 0.2826 | Val Loss: 1.9858 | Val Acc: 0.3020 | LR: 0.000263 | Val F1: 0.3570
2025-12-14 22:10:41 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.1418 | Train Acc: 0.2769 | Val Loss: 1.9281 | Val Acc: 0.3245 | LR: 0.000261 | Val F1: 0.3781
2025-12-14 22:10:43 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.1467 | Train Acc: 0.2208 | Val Loss: 2.0474 | Val Acc: 0.3464 | LR: 0.000258 | Val F1: 0.4137
2025-12-14 22:10:44 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.1245 | Train Acc: 0.3384 | Val Loss: 2.0052 | Val Acc: 0.2829 | LR: 0.000255 | Val F1: 0.3234
2025-12-14 22:10:45 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.1411 | Train Acc: 0.3000 | Val Loss: 2.0087 | Val Acc: 0.3490 | LR: 0.000252 | Val F1: 0.4004
2025-12-14 22:10:45 | training | train_model | INFO     | Early stopping triggered at epoch 24 due to no improvement in validation F1.
2025-12-14 22:10:45 | training | train_model | INFO     | Training for Hierarchical LSTM completed in 31.0s. Best validation F1: 0.4919
2025-12-14 22:10:45 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/hierarchical_v1/best_model.pth
2025-12-14 22:10:45 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:45 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:10:45 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:45 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:10:45 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:10:45 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:10:45 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:10:46 | training | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 22:10:46 | training | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 22:10:46 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:10:46 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:10:46 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_config | INFO     |   d_model                        : 128
2025-12-14 22:10:46 | training | log_config | INFO     |   nhead                          : 4
2025-12-14 22:10:46 | training | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 22:10:46 | training | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 22:10:46 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 22:10:46 | training | log_config | INFO     |   learnable_pe                   : False
2025-12-14 22:10:46 | training | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 22:10:46 | training | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 22:10:46 | training | log_config | INFO     |   batch_size                     : 8
2025-12-14 22:10:46 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:10:46 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:10:46 | training | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 22:10:46 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:10:46 | training | log_model_summary | INFO     | 
2025-12-14 22:10:46 | training | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 22:10:46 | training | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 22:10:46 | training | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 22:10:46 | training | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 22:10:46 | training | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 22:10:46 | training | log_model_summary | INFO     | 
2025-12-14 22:10:46 | training | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 22:10:46 | training | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 22:10:46 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:10:46 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:10:47 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.7402 | Train Acc: 0.1217 | Val Loss: 2.1374 | Val Acc: 0.2666 | LR: 0.000930 | Val F1: 0.3008
2025-12-14 22:10:49 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.4959 | Train Acc: 0.1478 | Val Loss: 2.1870 | Val Acc: 0.0619 | LR: 0.000930 | Val F1: 0.0309
2025-12-14 22:10:51 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.4098 | Train Acc: 0.1467 | Val Loss: 2.2735 | Val Acc: 0.0118 | LR: 0.000929 | Val F1: 0.0091
2025-12-14 22:10:52 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.3952 | Train Acc: 0.1592 | Val Loss: 2.2967 | Val Acc: 0.5253 | LR: 0.000928 | Val F1: 0.5448
2025-12-14 22:10:54 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.3772 | Train Acc: 0.2387 | Val Loss: 2.0897 | Val Acc: 0.5460 | LR: 0.000926 | Val F1: 0.5629
2025-12-14 22:10:56 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.3351 | Train Acc: 0.2571 | Val Loss: 2.0613 | Val Acc: 0.0963 | LR: 0.000924 | Val F1: 0.0456
2025-12-14 22:10:57 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.3608 | Train Acc: 0.2400 | Val Loss: 2.2677 | Val Acc: 0.4595 | LR: 0.000922 | Val F1: 0.5228
2025-12-14 22:10:59 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.3280 | Train Acc: 0.2571 | Val Loss: 2.0224 | Val Acc: 0.5381 | LR: 0.000919 | Val F1: 0.5652
2025-12-14 22:11:01 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.3256 | Train Acc: 0.2328 | Val Loss: 2.3645 | Val Acc: 0.4981 | LR: 0.000915 | Val F1: 0.5473
2025-12-14 22:11:02 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.3346 | Train Acc: 0.2884 | Val Loss: 2.2241 | Val Acc: 0.0946 | LR: 0.000912 | Val F1: 0.0606
2025-12-14 22:11:04 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.3079 | Train Acc: 0.2946 | Val Loss: 2.1268 | Val Acc: 0.3738 | LR: 0.000907 | Val F1: 0.4477
2025-12-14 22:11:06 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.2992 | Train Acc: 0.3063 | Val Loss: 2.1457 | Val Acc: 0.0781 | LR: 0.000903 | Val F1: 0.0599
2025-12-14 22:11:08 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.2994 | Train Acc: 0.3100 | Val Loss: 2.1047 | Val Acc: 0.1685 | LR: 0.000897 | Val F1: 0.1742
2025-12-14 22:11:09 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.2842 | Train Acc: 0.2777 | Val Loss: 2.0855 | Val Acc: 0.3612 | LR: 0.000892 | Val F1: 0.4269
2025-12-14 22:11:10 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.2711 | Train Acc: 0.3368 | Val Loss: 2.1809 | Val Acc: 0.0872 | LR: 0.000886 | Val F1: 0.1002
2025-12-14 22:11:11 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.2780 | Train Acc: 0.2718 | Val Loss: 2.0283 | Val Acc: 0.2968 | LR: 0.000879 | Val F1: 0.3705
2025-12-14 22:11:13 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.2919 | Train Acc: 0.3768 | Val Loss: 1.9468 | Val Acc: 0.4935 | LR: 0.000873 | Val F1: 0.5291
2025-12-14 22:11:14 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.2734 | Train Acc: 0.4133 | Val Loss: 2.1056 | Val Acc: 0.3358 | LR: 0.000865 | Val F1: 0.4299
2025-12-14 22:11:15 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.2774 | Train Acc: 0.2876 | Val Loss: 2.1090 | Val Acc: 0.5342 | LR: 0.000858 | Val F1: 0.5828
2025-12-14 22:11:16 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.2601 | Train Acc: 0.4002 | Val Loss: 2.0991 | Val Acc: 0.4229 | LR: 0.000850 | Val F1: 0.4993
2025-12-14 22:11:17 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.2497 | Train Acc: 0.3157 | Val Loss: 2.0718 | Val Acc: 0.3020 | LR: 0.000841 | Val F1: 0.3878
2025-12-14 22:11:18 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.2580 | Train Acc: 0.4133 | Val Loss: 1.9739 | Val Acc: 0.4622 | LR: 0.000833 | Val F1: 0.5270
2025-12-14 22:11:20 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.2483 | Train Acc: 0.3064 | Val Loss: 2.0120 | Val Acc: 0.5478 | LR: 0.000823 | Val F1: 0.5733
2025-12-14 22:11:21 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.2631 | Train Acc: 0.3683 | Val Loss: 2.0258 | Val Acc: 0.4151 | LR: 0.000814 | Val F1: 0.4849
2025-12-14 22:11:22 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 2.2470 | Train Acc: 0.3918 | Val Loss: 2.0717 | Val Acc: 0.3485 | LR: 0.000804 | Val F1: 0.4231
2025-12-14 22:11:23 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 2.2429 | Train Acc: 0.3449 | Val Loss: 2.1144 | Val Acc: 0.5780 | LR: 0.000794 | Val F1: 0.6053
2025-12-14 22:11:25 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 2.2375 | Train Acc: 0.4032 | Val Loss: 2.0546 | Val Acc: 0.5498 | LR: 0.000783 | Val F1: 0.5905
2025-12-14 22:11:26 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 2.2391 | Train Acc: 0.4200 | Val Loss: 2.0188 | Val Acc: 0.3359 | LR: 0.000773 | Val F1: 0.4361
2025-12-14 22:11:27 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 2.2327 | Train Acc: 0.2907 | Val Loss: 1.9870 | Val Acc: 0.3590 | LR: 0.000762 | Val F1: 0.4444
2025-12-14 22:11:28 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 2.2386 | Train Acc: 0.3917 | Val Loss: 1.9732 | Val Acc: 0.5076 | LR: 0.000750 | Val F1: 0.5643
2025-12-14 22:11:29 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 2.2174 | Train Acc: 0.3499 | Val Loss: 1.9999 | Val Acc: 0.4157 | LR: 0.000739 | Val F1: 0.4706
2025-12-14 22:11:31 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 2.2348 | Train Acc: 0.3402 | Val Loss: 1.9940 | Val Acc: 0.5273 | LR: 0.000727 | Val F1: 0.5529
2025-12-14 22:11:32 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 2.2364 | Train Acc: 0.3426 | Val Loss: 1.9889 | Val Acc: 0.3642 | LR: 0.000714 | Val F1: 0.4165
2025-12-14 22:11:33 | training | log_epoch | INFO     | Epoch [  34/100] | Train Loss: 2.2109 | Train Acc: 0.3684 | Val Loss: 1.9684 | Val Acc: 0.4154 | LR: 0.000702 | Val F1: 0.4584
2025-12-14 22:11:35 | training | log_epoch | INFO     | Epoch [  35/100] | Train Loss: 2.2285 | Train Acc: 0.3570 | Val Loss: 2.0073 | Val Acc: 0.4233 | LR: 0.000689 | Val F1: 0.4902
2025-12-14 22:11:36 | training | log_epoch | INFO     | Epoch [  36/100] | Train Loss: 2.2225 | Train Acc: 0.3888 | Val Loss: 2.0480 | Val Acc: 0.4255 | LR: 0.000676 | Val F1: 0.5028
2025-12-14 22:11:38 | training | log_epoch | INFO     | Epoch [  37/100] | Train Loss: 2.2083 | Train Acc: 0.3957 | Val Loss: 2.0777 | Val Acc: 0.6352 | LR: 0.000663 | Val F1: 0.6329
2025-12-14 22:11:39 | training | log_epoch | INFO     | Epoch [  38/100] | Train Loss: 2.2149 | Train Acc: 0.3912 | Val Loss: 2.1173 | Val Acc: 0.2447 | LR: 0.000650 | Val F1: 0.3120
2025-12-14 22:11:41 | training | log_epoch | INFO     | Epoch [  39/100] | Train Loss: 2.1937 | Train Acc: 0.2806 | Val Loss: 2.1149 | Val Acc: 0.4516 | LR: 0.000636 | Val F1: 0.5272
2025-12-14 22:11:42 | training | log_epoch | INFO     | Epoch [  40/100] | Train Loss: 2.2146 | Train Acc: 0.4470 | Val Loss: 2.0805 | Val Acc: 0.3289 | LR: 0.000623 | Val F1: 0.4160
2025-12-14 22:11:44 | training | log_epoch | INFO     | Epoch [  41/100] | Train Loss: 2.2278 | Train Acc: 0.3691 | Val Loss: 2.0339 | Val Acc: 0.4866 | LR: 0.000609 | Val F1: 0.5495
2025-12-14 22:11:45 | training | log_epoch | INFO     | Epoch [  42/100] | Train Loss: 2.1858 | Train Acc: 0.3544 | Val Loss: 2.0835 | Val Acc: 0.6259 | LR: 0.000595 | Val F1: 0.6304
2025-12-14 22:11:47 | training | log_epoch | INFO     | Epoch [  43/100] | Train Loss: 2.1987 | Train Acc: 0.4072 | Val Loss: 2.0043 | Val Acc: 0.3995 | LR: 0.000581 | Val F1: 0.4717
2025-12-14 22:11:48 | training | log_epoch | INFO     | Epoch [  44/100] | Train Loss: 2.2131 | Train Acc: 0.4252 | Val Loss: 2.0462 | Val Acc: 0.4686 | LR: 0.000567 | Val F1: 0.5226
2025-12-14 22:11:50 | training | log_epoch | INFO     | Epoch [  45/100] | Train Loss: 2.1985 | Train Acc: 0.3813 | Val Loss: 2.0696 | Val Acc: 0.4043 | LR: 0.000553 | Val F1: 0.4848
2025-12-14 22:11:51 | training | log_epoch | INFO     | Epoch [  46/100] | Train Loss: 2.1972 | Train Acc: 0.3801 | Val Loss: 2.0399 | Val Acc: 0.3846 | LR: 0.000538 | Val F1: 0.4578
2025-12-14 22:11:52 | training | log_epoch | INFO     | Epoch [  47/100] | Train Loss: 2.2099 | Train Acc: 0.4079 | Val Loss: 2.0713 | Val Acc: 0.4544 | LR: 0.000524 | Val F1: 0.5302
2025-12-14 22:11:54 | training | log_epoch | INFO     | Epoch [  48/100] | Train Loss: 2.1923 | Train Acc: 0.3918 | Val Loss: 2.0124 | Val Acc: 0.3307 | LR: 0.000509 | Val F1: 0.3972
2025-12-14 22:11:55 | training | log_epoch | INFO     | Epoch [  49/100] | Train Loss: 2.1793 | Train Acc: 0.3632 | Val Loss: 2.0390 | Val Acc: 0.5998 | LR: 0.000495 | Val F1: 0.6186
2025-12-14 22:11:56 | training | log_epoch | INFO     | Epoch [  50/100] | Train Loss: 2.1819 | Train Acc: 0.4222 | Val Loss: 2.0264 | Val Acc: 0.4900 | LR: 0.000480 | Val F1: 0.5341
2025-12-14 22:11:58 | training | log_epoch | INFO     | Epoch [  51/100] | Train Loss: 2.1827 | Train Acc: 0.3482 | Val Loss: 2.0647 | Val Acc: 0.4014 | LR: 0.000466 | Val F1: 0.4779
2025-12-14 22:11:59 | training | log_epoch | INFO     | Epoch [  52/100] | Train Loss: 2.1825 | Train Acc: 0.3686 | Val Loss: 2.0805 | Val Acc: 0.4089 | LR: 0.000451 | Val F1: 0.4901
2025-12-14 22:11:59 | training | train_model | INFO     | Early stopping triggered at epoch 52 due to no improvement in validation F1.
2025-12-14 22:11:59 | training | train_model | INFO     | Training for Transformer completed in 73.2s. Best validation F1: 0.6329
2025-12-14 22:11:59 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 22:11:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:11:59 | training | log_header | INFO     | TRAINING: STATISTICAL BASELINE
2025-12-14 22:11:59 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:12:00 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:12:00 | training | log_header | INFO     | BASELINE TEST SET METRICS
2025-12-14 22:12:00 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:12:00 | training | log_evaluation | INFO     |   accuracy                  : 0.4997
2025-12-14 22:12:00 | training | log_evaluation | INFO     |   f1_weighted               : 0.4644
2025-12-14 22:12:00 | training | log_evaluation | INFO     |   f1_macro                  : 0.1069
2025-12-14 22:12:00 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:12:00 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:12:00 | training | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 22:12:00 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:12:00 | training | train_main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 22:12:00 | training | train_main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 22:12:00 | training | train_main | INFO     | LSTM_v2 (SeqLabeling)     0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 22:12:00 | training | train_main | INFO     | Hierarchical LSTM         0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 22:12:00 | training | train_main | INFO     | Statistical Baseline      0.4997       0.4644          0.1069       0.0000           0.0000            
2025-12-14 22:12:00 | training | train_main | INFO     | Transformer               0.1037       0.0195          0.0269       1.0000           1.0000            
2025-12-14 22:12:00 | training | train_main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_220959.json
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | log_header | INFO     | CONFIGURATION
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 22:23:23 | training | log_config | INFO     |   Epochs                         : 100
2025-12-14 22:23:23 | training | log_config | INFO     |   Device                         : cuda
2025-12-14 22:23:23 | training | log_config | INFO     |   Log Level                      : INFO
2025-12-14 22:23:23 | training | log_config | INFO     |   Augmentation                   : True
2025-12-14 22:23:23 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 22:23:23 | training | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:23:23 | training | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:23:23 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:23:23 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:23:23 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:23:23 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 22:23:23 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 22:23:23 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:23:23 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:23:23 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | log_config | INFO     |   hidden_size                    : 96
2025-12-14 22:23:23 | training | log_config | INFO     |   num_layers                     : 2
2025-12-14 22:23:23 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 22:23:23 | training | log_config | INFO     |   bidirectional                  : True
2025-12-14 22:23:23 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 22:23:23 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 22:23:23 | training | log_config | INFO     |   batch_size                     : 16
2025-12-14 22:23:23 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 22:23:23 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 22:23:23 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:23:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:23 | training | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 22:23:23 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:23:23 | training | log_model_summary | INFO     | 
2025-12-14 22:23:23 | training | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 22:23:23 | training | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 22:23:23 | training | log_model_summary | INFO     | 
2025-12-14 22:23:23 | training | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 22:23:23 | training | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 22:23:23 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:23:23 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:23:25 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 22:23:25 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 22:23:25 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 22:23:25 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 22:23:25 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 22:23:26 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 22:23:26 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 22:23:26 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 22:23:26 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 22:23:27 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 22:23:27 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 22:23:27 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 22:23:27 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 22:23:28 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 22:23:28 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 22:23:28 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 22:23:28 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 22:23:28 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 22:23:29 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 22:23:29 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 22:23:29 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 22:23:29 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 22:23:30 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 22:23:30 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 22:23:30 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 22:23:30 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 22:23:30 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 22:23:31 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 22:23:31 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 22:23:31 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 22:23:31 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 22:23:31 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 22:23:32 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 22:23:32 | training | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 22:23:32 | training | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 7.8s. Best validation F1: 0.6429
2025-12-14 22:23:32 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 22:23:32 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:32 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:23:32 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:32 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:23:32 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:23:32 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:23:32 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:23:32 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:32 | training | log_header | INFO     | TRAINING: HIERARCHICAL LSTM
2025-12-14 22:23:32 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:32 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:32 | training | log_header | INFO     | HIERARCHICAL LSTM CONFIG
2025-12-14 22:23:32 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:32 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:23:32 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 22:23:32 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 22:23:32 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:23:32 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:23:32 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:23:32 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:32 | training | log_header | INFO     | HIERARCHICAL LSTM BEST HYPERPARAMETERS
2025-12-14 22:23:32 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:32 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 22:23:32 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 22:23:32 | training | log_config | INFO     |   batch_size                     : 32
2025-12-14 22:23:32 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 22:23:32 | training | log_config | INFO     |   grad_clip                      : 0.5
2025-12-14 22:23:32 | training | log_config | INFO     |   s1_hidden_size                 : 192
2025-12-14 22:23:32 | training | log_config | INFO     |   s1_num_layers                  : 2
2025-12-14 22:23:32 | training | log_config | INFO     |   s1_dropout                     : 0.5
2025-12-14 22:23:32 | training | log_config | INFO     |   s1_bidirectional               : True
2025-12-14 22:23:32 | training | log_config | INFO     |   s2_hidden_size                 : 64
2025-12-14 22:23:32 | training | log_config | INFO     |   s2_num_layers                  : 3
2025-12-14 22:23:32 | training | log_config | INFO     |   s2_dropout                     : 0.3
2025-12-14 22:23:32 | training | log_config | INFO     |   s2_bidirectional               : False
2025-12-14 22:23:32 | training | log_config | INFO     |   use_class_weights              : True
2025-12-14 22:23:32 | training | log_config | INFO     |   class_weight_scale             : 0.6
2025-12-14 22:23:32 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:23:32 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:32 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:23:32 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:32 | training | log_model_summary | INFO     | Model: HierarchicalClassifier
2025-12-14 22:23:32 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:23:32 | training | log_model_summary | INFO     | 
2025-12-14 22:23:32 | training | log_model_summary | INFO     |   stage1                    :  1,193,091 params ( 1,193,091 trainable)
2025-12-14 22:23:32 | training | log_model_summary | INFO     |   stage2                    :     84,675 params (    84,675 trainable)
2025-12-14 22:23:32 | training | log_model_summary | INFO     | 
2025-12-14 22:23:32 | training | log_model_summary | INFO     | Total parameters          :  1,277,766
2025-12-14 22:23:32 | training | log_model_summary | INFO     | Trainable parameters      :  1,277,766
2025-12-14 22:23:32 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:23:32 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:23:33 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.3949 | Train Acc: 0.2390 | Val Loss: 1.9860 | Val Acc: 0.2101 | LR: 0.000288 | Val F1: 0.1914
2025-12-14 22:23:34 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.3152 | Train Acc: 0.2117 | Val Loss: 2.0607 | Val Acc: 0.2404 | LR: 0.000288 | Val F1: 0.2729
2025-12-14 22:23:35 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.3001 | Train Acc: 0.1852 | Val Loss: 1.9873 | Val Acc: 0.2655 | LR: 0.000288 | Val F1: 0.3146
2025-12-14 22:23:35 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.2574 | Train Acc: 0.1779 | Val Loss: 1.9836 | Val Acc: 0.2182 | LR: 0.000287 | Val F1: 0.2417
2025-12-14 22:23:36 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.2528 | Train Acc: 0.2352 | Val Loss: 1.9733 | Val Acc: 0.2745 | LR: 0.000287 | Val F1: 0.3110
2025-12-14 22:23:37 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.2219 | Train Acc: 0.2438 | Val Loss: 2.0731 | Val Acc: 0.1727 | LR: 0.000286 | Val F1: 0.1778
2025-12-14 22:23:38 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.2270 | Train Acc: 0.2021 | Val Loss: 2.0306 | Val Acc: 0.3899 | LR: 0.000285 | Val F1: 0.4496
2025-12-14 22:23:39 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.2329 | Train Acc: 0.2830 | Val Loss: 1.9791 | Val Acc: 0.1543 | LR: 0.000285 | Val F1: 0.1320
2025-12-14 22:23:40 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.2236 | Train Acc: 0.2172 | Val Loss: 2.0190 | Val Acc: 0.4240 | LR: 0.000283 | Val F1: 0.4919
2025-12-14 22:23:40 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.2004 | Train Acc: 0.3239 | Val Loss: 1.9722 | Val Acc: 0.1903 | LR: 0.000282 | Val F1: 0.2052
2025-12-14 22:23:41 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.2156 | Train Acc: 0.2425 | Val Loss: 2.0374 | Val Acc: 0.2116 | LR: 0.000281 | Val F1: 0.2280
2025-12-14 22:23:42 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.1825 | Train Acc: 0.2420 | Val Loss: 2.0586 | Val Acc: 0.1573 | LR: 0.000280 | Val F1: 0.1609
2025-12-14 22:23:43 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.1836 | Train Acc: 0.2435 | Val Loss: 1.9628 | Val Acc: 0.2378 | LR: 0.000278 | Val F1: 0.2691
2025-12-14 22:23:44 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.1835 | Train Acc: 0.2686 | Val Loss: 1.9564 | Val Acc: 0.4023 | LR: 0.000276 | Val F1: 0.4581
2025-12-14 22:23:45 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.1889 | Train Acc: 0.2617 | Val Loss: 1.9553 | Val Acc: 0.1901 | LR: 0.000274 | Val F1: 0.2030
2025-12-14 22:23:45 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.1606 | Train Acc: 0.2830 | Val Loss: 2.0343 | Val Acc: 0.2225 | LR: 0.000272 | Val F1: 0.2518
2025-12-14 22:23:46 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.1493 | Train Acc: 0.3194 | Val Loss: 2.0108 | Val Acc: 0.1785 | LR: 0.000270 | Val F1: 0.1895
2025-12-14 22:23:47 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.1637 | Train Acc: 0.2176 | Val Loss: 2.0132 | Val Acc: 0.3490 | LR: 0.000268 | Val F1: 0.4048
2025-12-14 22:23:48 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.1246 | Train Acc: 0.3117 | Val Loss: 1.9923 | Val Acc: 0.2252 | LR: 0.000266 | Val F1: 0.2524
2025-12-14 22:23:49 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.1473 | Train Acc: 0.2826 | Val Loss: 1.9858 | Val Acc: 0.3020 | LR: 0.000263 | Val F1: 0.3570
2025-12-14 22:23:50 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.1418 | Train Acc: 0.2769 | Val Loss: 1.9281 | Val Acc: 0.3245 | LR: 0.000261 | Val F1: 0.3781
2025-12-14 22:23:51 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.1467 | Train Acc: 0.2208 | Val Loss: 2.0474 | Val Acc: 0.3464 | LR: 0.000258 | Val F1: 0.4137
2025-12-14 22:23:51 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.1245 | Train Acc: 0.3384 | Val Loss: 2.0052 | Val Acc: 0.2829 | LR: 0.000255 | Val F1: 0.3234
2025-12-14 22:23:52 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.1411 | Train Acc: 0.3000 | Val Loss: 2.0087 | Val Acc: 0.3490 | LR: 0.000252 | Val F1: 0.4004
2025-12-14 22:23:52 | training | train_model | INFO     | Early stopping triggered at epoch 24 due to no improvement in validation F1.
2025-12-14 22:23:52 | training | train_model | INFO     | Training for Hierarchical LSTM completed in 20.4s. Best validation F1: 0.4919
2025-12-14 22:23:52 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/hierarchical_v1/best_model.pth
2025-12-14 22:23:52 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:52 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:23:52 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:52 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:23:52 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:23:52 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:23:52 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:23:52 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:52 | training | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 22:23:52 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:52 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:52 | training | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 22:23:52 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:52 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:23:52 | training | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 22:23:52 | training | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 22:23:52 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:23:52 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:23:52 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:23:52 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:52 | training | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 22:23:52 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:52 | training | log_config | INFO     |   d_model                        : 128
2025-12-14 22:23:52 | training | log_config | INFO     |   nhead                          : 4
2025-12-14 22:23:52 | training | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 22:23:52 | training | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 22:23:52 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 22:23:52 | training | log_config | INFO     |   learnable_pe                   : False
2025-12-14 22:23:52 | training | log_config | INFO     |   learning_rate                  : 0.00093
2025-12-14 22:23:52 | training | log_config | INFO     |   weight_decay                   : 4.4e-06
2025-12-14 22:23:52 | training | log_config | INFO     |   batch_size                     : 8
2025-12-14 22:23:52 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:23:52 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:52 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:23:52 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:23:52 | training | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 22:23:52 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:23:52 | training | log_model_summary | INFO     | 
2025-12-14 22:23:52 | training | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 22:23:52 | training | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 22:23:52 | training | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 22:23:52 | training | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 22:23:52 | training | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 22:23:52 | training | log_model_summary | INFO     | 
2025-12-14 22:23:52 | training | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 22:23:52 | training | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 22:23:52 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:23:52 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:23:53 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.7402 | Train Acc: 0.1217 | Val Loss: 2.1374 | Val Acc: 0.2666 | LR: 0.000930 | Val F1: 0.3008
2025-12-14 22:23:54 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.4959 | Train Acc: 0.1478 | Val Loss: 2.1870 | Val Acc: 0.0619 | LR: 0.000930 | Val F1: 0.0309
2025-12-14 22:23:55 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.4098 | Train Acc: 0.1467 | Val Loss: 2.2735 | Val Acc: 0.0118 | LR: 0.000929 | Val F1: 0.0091
2025-12-14 22:23:56 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.3952 | Train Acc: 0.1592 | Val Loss: 2.2967 | Val Acc: 0.5253 | LR: 0.000928 | Val F1: 0.5448
2025-12-14 22:23:57 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.3772 | Train Acc: 0.2387 | Val Loss: 2.0897 | Val Acc: 0.5460 | LR: 0.000926 | Val F1: 0.5629
2025-12-14 22:23:58 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.3351 | Train Acc: 0.2571 | Val Loss: 2.0613 | Val Acc: 0.0963 | LR: 0.000924 | Val F1: 0.0456
2025-12-14 22:23:59 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.3608 | Train Acc: 0.2400 | Val Loss: 2.2677 | Val Acc: 0.4595 | LR: 0.000922 | Val F1: 0.5228
2025-12-14 22:24:00 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.3280 | Train Acc: 0.2571 | Val Loss: 2.0224 | Val Acc: 0.5381 | LR: 0.000919 | Val F1: 0.5652
2025-12-14 22:24:01 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.3256 | Train Acc: 0.2328 | Val Loss: 2.3645 | Val Acc: 0.4981 | LR: 0.000915 | Val F1: 0.5473
2025-12-14 22:24:02 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.3346 | Train Acc: 0.2884 | Val Loss: 2.2241 | Val Acc: 0.0946 | LR: 0.000912 | Val F1: 0.0606
2025-12-14 22:24:04 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.3079 | Train Acc: 0.2946 | Val Loss: 2.1268 | Val Acc: 0.3738 | LR: 0.000907 | Val F1: 0.4477
2025-12-14 22:24:05 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.2992 | Train Acc: 0.3063 | Val Loss: 2.1457 | Val Acc: 0.0781 | LR: 0.000903 | Val F1: 0.0599
2025-12-14 22:24:06 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.2994 | Train Acc: 0.3100 | Val Loss: 2.1047 | Val Acc: 0.1685 | LR: 0.000897 | Val F1: 0.1742
2025-12-14 22:24:07 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.2842 | Train Acc: 0.2777 | Val Loss: 2.0855 | Val Acc: 0.3612 | LR: 0.000892 | Val F1: 0.4269
2025-12-14 22:24:07 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.2711 | Train Acc: 0.3368 | Val Loss: 2.1809 | Val Acc: 0.0872 | LR: 0.000886 | Val F1: 0.1002
2025-12-14 22:24:08 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.2780 | Train Acc: 0.2718 | Val Loss: 2.0283 | Val Acc: 0.2968 | LR: 0.000879 | Val F1: 0.3705
2025-12-14 22:24:09 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.2919 | Train Acc: 0.3768 | Val Loss: 1.9468 | Val Acc: 0.4935 | LR: 0.000873 | Val F1: 0.5291
2025-12-14 22:24:10 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.2734 | Train Acc: 0.4133 | Val Loss: 2.1056 | Val Acc: 0.3358 | LR: 0.000865 | Val F1: 0.4299
2025-12-14 22:24:12 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.2774 | Train Acc: 0.2876 | Val Loss: 2.1090 | Val Acc: 0.5342 | LR: 0.000858 | Val F1: 0.5828
2025-12-14 22:24:13 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.2601 | Train Acc: 0.4002 | Val Loss: 2.0991 | Val Acc: 0.4229 | LR: 0.000850 | Val F1: 0.4993
2025-12-14 22:24:14 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.2497 | Train Acc: 0.3157 | Val Loss: 2.0718 | Val Acc: 0.3020 | LR: 0.000841 | Val F1: 0.3878
2025-12-14 22:24:15 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.2580 | Train Acc: 0.4132 | Val Loss: 1.9739 | Val Acc: 0.4622 | LR: 0.000833 | Val F1: 0.5270
2025-12-14 22:24:16 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.2483 | Train Acc: 0.3064 | Val Loss: 2.0120 | Val Acc: 0.5478 | LR: 0.000823 | Val F1: 0.5733
2025-12-14 22:24:17 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.2631 | Train Acc: 0.3683 | Val Loss: 2.0258 | Val Acc: 0.4151 | LR: 0.000814 | Val F1: 0.4849
2025-12-14 22:24:18 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 2.2470 | Train Acc: 0.3918 | Val Loss: 2.0717 | Val Acc: 0.3485 | LR: 0.000804 | Val F1: 0.4231
2025-12-14 22:24:19 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 2.2429 | Train Acc: 0.3449 | Val Loss: 2.1144 | Val Acc: 0.5780 | LR: 0.000794 | Val F1: 0.6053
2025-12-14 22:24:20 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 2.2375 | Train Acc: 0.4032 | Val Loss: 2.0546 | Val Acc: 0.5498 | LR: 0.000783 | Val F1: 0.5905
2025-12-14 22:24:21 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 2.2391 | Train Acc: 0.4200 | Val Loss: 2.0188 | Val Acc: 0.3359 | LR: 0.000773 | Val F1: 0.4361
2025-12-14 22:24:22 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 2.2327 | Train Acc: 0.2907 | Val Loss: 1.9870 | Val Acc: 0.3590 | LR: 0.000762 | Val F1: 0.4444
2025-12-14 22:24:23 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 2.2386 | Train Acc: 0.3917 | Val Loss: 1.9732 | Val Acc: 0.5076 | LR: 0.000750 | Val F1: 0.5643
2025-12-14 22:24:24 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 2.2174 | Train Acc: 0.3499 | Val Loss: 1.9999 | Val Acc: 0.4157 | LR: 0.000739 | Val F1: 0.4706
2025-12-14 22:24:25 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 2.2348 | Train Acc: 0.3402 | Val Loss: 1.9940 | Val Acc: 0.5273 | LR: 0.000727 | Val F1: 0.5529
2025-12-14 22:24:26 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 2.2364 | Train Acc: 0.3426 | Val Loss: 1.9889 | Val Acc: 0.3642 | LR: 0.000714 | Val F1: 0.4165
2025-12-14 22:24:27 | training | log_epoch | INFO     | Epoch [  34/100] | Train Loss: 2.2109 | Train Acc: 0.3684 | Val Loss: 1.9684 | Val Acc: 0.4154 | LR: 0.000702 | Val F1: 0.4584
2025-12-14 22:24:29 | training | log_epoch | INFO     | Epoch [  35/100] | Train Loss: 2.2285 | Train Acc: 0.3570 | Val Loss: 2.0073 | Val Acc: 0.4233 | LR: 0.000689 | Val F1: 0.4902
2025-12-14 22:24:30 | training | log_epoch | INFO     | Epoch [  36/100] | Train Loss: 2.2225 | Train Acc: 0.3888 | Val Loss: 2.0480 | Val Acc: 0.4255 | LR: 0.000676 | Val F1: 0.5028
2025-12-14 22:24:31 | training | log_epoch | INFO     | Epoch [  37/100] | Train Loss: 2.2083 | Train Acc: 0.3957 | Val Loss: 2.0777 | Val Acc: 0.6352 | LR: 0.000663 | Val F1: 0.6329
2025-12-14 22:24:32 | training | log_epoch | INFO     | Epoch [  38/100] | Train Loss: 2.2149 | Train Acc: 0.3912 | Val Loss: 2.1173 | Val Acc: 0.2447 | LR: 0.000650 | Val F1: 0.3120
2025-12-14 22:24:33 | training | log_epoch | INFO     | Epoch [  39/100] | Train Loss: 2.1937 | Train Acc: 0.2805 | Val Loss: 2.1149 | Val Acc: 0.4516 | LR: 0.000636 | Val F1: 0.5272
2025-12-14 22:24:35 | training | log_epoch | INFO     | Epoch [  40/100] | Train Loss: 2.2146 | Train Acc: 0.4470 | Val Loss: 2.0805 | Val Acc: 0.3289 | LR: 0.000623 | Val F1: 0.4160
2025-12-14 22:24:36 | training | log_epoch | INFO     | Epoch [  41/100] | Train Loss: 2.2278 | Train Acc: 0.3691 | Val Loss: 2.0339 | Val Acc: 0.4866 | LR: 0.000609 | Val F1: 0.5495
2025-12-14 22:24:37 | training | log_epoch | INFO     | Epoch [  42/100] | Train Loss: 2.1858 | Train Acc: 0.3544 | Val Loss: 2.0835 | Val Acc: 0.6259 | LR: 0.000595 | Val F1: 0.6304
2025-12-14 22:24:38 | training | log_epoch | INFO     | Epoch [  43/100] | Train Loss: 2.1987 | Train Acc: 0.4072 | Val Loss: 2.0043 | Val Acc: 0.3995 | LR: 0.000581 | Val F1: 0.4717
2025-12-14 22:24:40 | training | log_epoch | INFO     | Epoch [  44/100] | Train Loss: 2.2131 | Train Acc: 0.4252 | Val Loss: 2.0462 | Val Acc: 0.4686 | LR: 0.000567 | Val F1: 0.5226
2025-12-14 22:24:41 | training | log_epoch | INFO     | Epoch [  45/100] | Train Loss: 2.1985 | Train Acc: 0.3813 | Val Loss: 2.0696 | Val Acc: 0.4043 | LR: 0.000553 | Val F1: 0.4848
2025-12-14 22:24:42 | training | log_epoch | INFO     | Epoch [  46/100] | Train Loss: 2.1972 | Train Acc: 0.3800 | Val Loss: 2.0399 | Val Acc: 0.3846 | LR: 0.000538 | Val F1: 0.4578
2025-12-14 22:24:43 | training | log_epoch | INFO     | Epoch [  47/100] | Train Loss: 2.2099 | Train Acc: 0.4079 | Val Loss: 2.0713 | Val Acc: 0.4544 | LR: 0.000524 | Val F1: 0.5303
2025-12-14 22:24:44 | training | log_epoch | INFO     | Epoch [  48/100] | Train Loss: 2.1923 | Train Acc: 0.3918 | Val Loss: 2.0123 | Val Acc: 0.3303 | LR: 0.000509 | Val F1: 0.3968
2025-12-14 22:24:46 | training | log_epoch | INFO     | Epoch [  49/100] | Train Loss: 2.1793 | Train Acc: 0.3632 | Val Loss: 2.0390 | Val Acc: 0.5998 | LR: 0.000495 | Val F1: 0.6186
2025-12-14 22:24:47 | training | log_epoch | INFO     | Epoch [  50/100] | Train Loss: 2.1819 | Train Acc: 0.4223 | Val Loss: 2.0264 | Val Acc: 0.4900 | LR: 0.000480 | Val F1: 0.5341
2025-12-14 22:24:48 | training | log_epoch | INFO     | Epoch [  51/100] | Train Loss: 2.1827 | Train Acc: 0.3482 | Val Loss: 2.0647 | Val Acc: 0.4011 | LR: 0.000466 | Val F1: 0.4776
2025-12-14 22:24:49 | training | log_epoch | INFO     | Epoch [  52/100] | Train Loss: 2.1825 | Train Acc: 0.3687 | Val Loss: 2.0805 | Val Acc: 0.4087 | LR: 0.000451 | Val F1: 0.4900
2025-12-14 22:24:49 | training | train_model | INFO     | Early stopping triggered at epoch 52 due to no improvement in validation F1.
2025-12-14 22:24:49 | training | train_model | INFO     | Training for Transformer completed in 56.4s. Best validation F1: 0.6329
2025-12-14 22:24:49 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 22:24:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:24:49 | training | log_header | INFO     | TRAINING: STATISTICAL BASELINE
2025-12-14 22:24:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:24:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:24:49 | training | log_header | INFO     | BASELINE TEST SET METRICS
2025-12-14 22:24:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:24:49 | training | log_evaluation | INFO     |   accuracy                  : 0.4997
2025-12-14 22:24:49 | training | log_evaluation | INFO     |   f1_weighted               : 0.4644
2025-12-14 22:24:49 | training | log_evaluation | INFO     |   f1_macro                  : 0.1069
2025-12-14 22:24:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:24:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:24:49 | training | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 22:24:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:24:49 | training | train_main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 22:24:49 | training | train_main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 22:24:49 | training | train_main | INFO     | LSTM_v2 (SeqLabeling)     0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 22:24:49 | training | train_main | INFO     | Hierarchical LSTM         0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 22:24:49 | training | train_main | INFO     | Statistical Baseline      0.4997       0.4644          0.1069       0.0000           0.0000            
2025-12-14 22:24:49 | training | train_main | INFO     | Transformer               0.1037       0.0195          0.0269       1.0000           1.0000            
2025-12-14 22:24:49 | training | train_main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_222323.json
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | log_header | INFO     | CONFIGURATION
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 22:29:31 | training | log_config | INFO     |   Epochs                         : 100
2025-12-14 22:29:31 | training | log_config | INFO     |   Device                         : cuda
2025-12-14 22:29:31 | training | log_config | INFO     |   Log Level                      : INFO
2025-12-14 22:29:31 | training | log_config | INFO     |   Augmentation                   : True
2025-12-14 22:29:31 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 22:29:31 | training | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:29:31 | training | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:29:31 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:29:31 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:29:31 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:29:31 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 22:29:31 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 22:29:31 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:29:31 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:29:31 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | log_config | INFO     |   hidden_size                    : 96
2025-12-14 22:29:31 | training | log_config | INFO     |   num_layers                     : 2
2025-12-14 22:29:31 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 22:29:31 | training | log_config | INFO     |   bidirectional                  : True
2025-12-14 22:29:31 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 22:29:31 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 22:29:31 | training | log_config | INFO     |   batch_size                     : 16
2025-12-14 22:29:31 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 22:29:31 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 22:29:31 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:29:31 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:31 | training | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 22:29:31 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:29:31 | training | log_model_summary | INFO     | 
2025-12-14 22:29:31 | training | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 22:29:31 | training | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 22:29:31 | training | log_model_summary | INFO     | 
2025-12-14 22:29:31 | training | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 22:29:31 | training | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 22:29:31 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:29:31 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:29:32 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 22:29:33 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 22:29:33 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 22:29:33 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 22:29:33 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 22:29:34 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 22:29:34 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 22:29:34 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 22:29:34 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 22:29:34 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 22:29:35 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 22:29:35 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 22:29:35 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 22:29:35 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 22:29:35 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 22:29:36 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 22:29:36 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 22:29:36 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 22:29:36 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 22:29:37 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 22:29:37 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 22:29:37 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 22:29:37 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 22:29:37 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 22:29:38 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 22:29:38 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 22:29:38 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 22:29:38 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 22:29:38 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 22:29:39 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 22:29:39 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 22:29:39 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 22:29:39 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 22:29:39 | training | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 22:29:39 | training | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 7.5s. Best validation F1: 0.6429
2025-12-14 22:29:39 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 22:29:39 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:39 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:29:39 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:39 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:29:39 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:29:39 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:29:39 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:29:39 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:39 | training | log_header | INFO     | TRAINING: HIERARCHICAL LSTM
2025-12-14 22:29:39 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:39 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:39 | training | log_header | INFO     | HIERARCHICAL LSTM CONFIG
2025-12-14 22:29:39 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:39 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:29:39 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 22:29:39 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 22:29:39 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:29:39 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:29:39 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:29:39 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:39 | training | log_header | INFO     | HIERARCHICAL LSTM BEST HYPERPARAMETERS
2025-12-14 22:29:39 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:39 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 22:29:39 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 22:29:39 | training | log_config | INFO     |   batch_size                     : 32
2025-12-14 22:29:39 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 22:29:39 | training | log_config | INFO     |   grad_clip                      : 0.5
2025-12-14 22:29:39 | training | log_config | INFO     |   s1_hidden_size                 : 192
2025-12-14 22:29:39 | training | log_config | INFO     |   s1_num_layers                  : 2
2025-12-14 22:29:39 | training | log_config | INFO     |   s1_dropout                     : 0.5
2025-12-14 22:29:39 | training | log_config | INFO     |   s1_bidirectional               : True
2025-12-14 22:29:39 | training | log_config | INFO     |   s2_hidden_size                 : 64
2025-12-14 22:29:39 | training | log_config | INFO     |   s2_num_layers                  : 3
2025-12-14 22:29:39 | training | log_config | INFO     |   s2_dropout                     : 0.3
2025-12-14 22:29:39 | training | log_config | INFO     |   s2_bidirectional               : False
2025-12-14 22:29:39 | training | log_config | INFO     |   use_class_weights              : True
2025-12-14 22:29:39 | training | log_config | INFO     |   class_weight_scale             : 0.6
2025-12-14 22:29:39 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:29:39 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:39 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:29:39 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:39 | training | log_model_summary | INFO     | Model: HierarchicalClassifier
2025-12-14 22:29:39 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:29:39 | training | log_model_summary | INFO     | 
2025-12-14 22:29:39 | training | log_model_summary | INFO     |   stage1                    :  1,193,091 params ( 1,193,091 trainable)
2025-12-14 22:29:39 | training | log_model_summary | INFO     |   stage2                    :     84,675 params (    84,675 trainable)
2025-12-14 22:29:39 | training | log_model_summary | INFO     | 
2025-12-14 22:29:39 | training | log_model_summary | INFO     | Total parameters          :  1,277,766
2025-12-14 22:29:39 | training | log_model_summary | INFO     | Trainable parameters      :  1,277,766
2025-12-14 22:29:39 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:29:39 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:29:40 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.3949 | Train Acc: 0.2390 | Val Loss: 1.9860 | Val Acc: 0.2101 | LR: 0.000288 | Val F1: 0.1914
2025-12-14 22:29:41 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.3152 | Train Acc: 0.2117 | Val Loss: 2.0607 | Val Acc: 0.2404 | LR: 0.000288 | Val F1: 0.2729
2025-12-14 22:29:42 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.3001 | Train Acc: 0.1852 | Val Loss: 1.9873 | Val Acc: 0.2655 | LR: 0.000288 | Val F1: 0.3146
2025-12-14 22:29:42 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.2574 | Train Acc: 0.1779 | Val Loss: 1.9836 | Val Acc: 0.2182 | LR: 0.000287 | Val F1: 0.2417
2025-12-14 22:29:43 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.2528 | Train Acc: 0.2352 | Val Loss: 1.9733 | Val Acc: 0.2745 | LR: 0.000287 | Val F1: 0.3110
2025-12-14 22:29:44 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.2219 | Train Acc: 0.2438 | Val Loss: 2.0731 | Val Acc: 0.1727 | LR: 0.000286 | Val F1: 0.1778
2025-12-14 22:29:45 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.2270 | Train Acc: 0.2021 | Val Loss: 2.0306 | Val Acc: 0.3899 | LR: 0.000285 | Val F1: 0.4496
2025-12-14 22:29:46 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.2329 | Train Acc: 0.2830 | Val Loss: 1.9791 | Val Acc: 0.1543 | LR: 0.000285 | Val F1: 0.1320
2025-12-14 22:29:46 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.2236 | Train Acc: 0.2172 | Val Loss: 2.0190 | Val Acc: 0.4240 | LR: 0.000283 | Val F1: 0.4919
2025-12-14 22:29:47 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.2004 | Train Acc: 0.3239 | Val Loss: 1.9722 | Val Acc: 0.1903 | LR: 0.000282 | Val F1: 0.2052
2025-12-14 22:29:48 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.2156 | Train Acc: 0.2425 | Val Loss: 2.0374 | Val Acc: 0.2116 | LR: 0.000281 | Val F1: 0.2280
2025-12-14 22:29:49 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.1825 | Train Acc: 0.2420 | Val Loss: 2.0586 | Val Acc: 0.1573 | LR: 0.000280 | Val F1: 0.1609
2025-12-14 22:29:50 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.1836 | Train Acc: 0.2435 | Val Loss: 1.9628 | Val Acc: 0.2378 | LR: 0.000278 | Val F1: 0.2691
2025-12-14 22:29:50 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.1835 | Train Acc: 0.2686 | Val Loss: 1.9564 | Val Acc: 0.4023 | LR: 0.000276 | Val F1: 0.4581
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | log_header | INFO     | CONFIGURATION
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | log_config | INFO     |   Selected Models                : ['transformer']
2025-12-14 22:29:51 | training | log_config | INFO     |   Epochs                         : 50
2025-12-14 22:29:51 | training | log_config | INFO     |   Device                         : cuda
2025-12-14 22:29:51 | training | log_config | INFO     |   Log Level                      : INFO
2025-12-14 22:29:51 | training | log_config | INFO     |   Augmentation                   : True
2025-12-14 22:29:51 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 22:29:51 | training | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:29:51 | training | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:29:51 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:29:51 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:29:51 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | log_config | INFO     |   epochs                         : 50
2025-12-14 22:29:51 | training | log_config | INFO     |   learning_rate                  : 0.00073
2025-12-14 22:29:51 | training | log_config | INFO     |   weight_decay                   : 1.6e-07
2025-12-14 22:29:51 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:29:51 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:29:51 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | log_config | INFO     |   d_model                        : 128
2025-12-14 22:29:51 | training | log_config | INFO     |   nhead                          : 2
2025-12-14 22:29:51 | training | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 22:29:51 | training | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 22:29:51 | training | log_config | INFO     |   dropout                        : 0.1
2025-12-14 22:29:51 | training | log_config | INFO     |   learnable_pe                   : False
2025-12-14 22:29:51 | training | log_config | INFO     |   learning_rate                  : 0.00073
2025-12-14 22:29:51 | training | log_config | INFO     |   weight_decay                   : 1.6e-07
2025-12-14 22:29:51 | training | log_config | INFO     |   batch_size                     : 32
2025-12-14 22:29:51 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 22:29:51 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:29:51 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.1889 | Train Acc: 0.2617 | Val Loss: 1.9553 | Val Acc: 0.1901 | LR: 0.000274 | Val F1: 0.2030
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:29:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:29:51 | training | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 22:29:51 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:29:51 | training | log_model_summary | INFO     | 
2025-12-14 22:29:51 | training | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 22:29:51 | training | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 22:29:51 | training | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 22:29:51 | training | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 22:29:51 | training | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 22:29:51 | training | log_model_summary | INFO     | 
2025-12-14 22:29:51 | training | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 22:29:51 | training | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 22:29:51 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:29:51 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:29:52 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.1606 | Train Acc: 0.2830 | Val Loss: 2.0343 | Val Acc: 0.2225 | LR: 0.000272 | Val F1: 0.2518
2025-12-14 22:29:53 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.1493 | Train Acc: 0.3194 | Val Loss: 2.0108 | Val Acc: 0.1785 | LR: 0.000270 | Val F1: 0.1895
2025-12-14 22:29:54 | training | log_epoch | INFO     | Epoch [   1/50] | Train Loss: 1.7741 | Train Acc: 0.5795 | Val Loss: 1.2587 | Val Acc: 0.6918 | LR: 0.000730 | Val F1: 0.5657
2025-12-14 22:29:54 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.1637 | Train Acc: 0.2176 | Val Loss: 2.0132 | Val Acc: 0.3490 | LR: 0.000268 | Val F1: 0.4048
2025-12-14 22:29:54 | training | log_epoch | INFO     | Epoch [   2/50] | Train Loss: 1.3177 | Train Acc: 0.6880 | Val Loss: 1.2108 | Val Acc: 0.6918 | LR: 0.000729 | Val F1: 0.5657
2025-12-14 22:29:55 | training | log_epoch | INFO     | Epoch [   3/50] | Train Loss: 1.3012 | Train Acc: 0.6839 | Val Loss: 1.1869 | Val Acc: 0.7029 | LR: 0.000727 | Val F1: 0.6287
2025-12-14 22:29:56 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.1246 | Train Acc: 0.3117 | Val Loss: 1.9923 | Val Acc: 0.2252 | LR: 0.000266 | Val F1: 0.2524
2025-12-14 22:29:56 | training | log_epoch | INFO     | Epoch [   4/50] | Train Loss: 1.2861 | Train Acc: 0.6801 | Val Loss: 1.2383 | Val Acc: 0.6914 | LR: 0.000724 | Val F1: 0.5980
2025-12-14 22:29:57 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.1473 | Train Acc: 0.2826 | Val Loss: 1.9858 | Val Acc: 0.3020 | LR: 0.000263 | Val F1: 0.3570
2025-12-14 22:29:57 | training | log_epoch | INFO     | Epoch [   5/50] | Train Loss: 1.3154 | Train Acc: 0.6806 | Val Loss: 1.1963 | Val Acc: 0.7020 | LR: 0.000719 | Val F1: 0.6064
2025-12-14 22:29:58 | training | log_epoch | INFO     | Epoch [   6/50] | Train Loss: 1.2960 | Train Acc: 0.6799 | Val Loss: 1.1874 | Val Acc: 0.7003 | LR: 0.000712 | Val F1: 0.6093
2025-12-14 22:29:58 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.1418 | Train Acc: 0.2769 | Val Loss: 1.9281 | Val Acc: 0.3245 | LR: 0.000261 | Val F1: 0.3781
2025-12-14 22:29:58 | training | log_epoch | INFO     | Epoch [   7/50] | Train Loss: 1.2614 | Train Acc: 0.6892 | Val Loss: 1.1434 | Val Acc: 0.7079 | LR: 0.000704 | Val F1: 0.6142
2025-12-14 22:29:59 | training | log_epoch | INFO     | Epoch [   8/50] | Train Loss: 1.2511 | Train Acc: 0.6903 | Val Loss: 1.1559 | Val Acc: 0.7029 | LR: 0.000695 | Val F1: 0.6112
2025-12-14 22:29:59 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.1467 | Train Acc: 0.2208 | Val Loss: 2.0474 | Val Acc: 0.3464 | LR: 0.000258 | Val F1: 0.4137
2025-12-14 22:30:00 | training | log_epoch | INFO     | Epoch [   9/50] | Train Loss: 1.2505 | Train Acc: 0.6880 | Val Loss: 1.2217 | Val Acc: 0.6968 | LR: 0.000685 | Val F1: 0.5952
2025-12-14 22:30:01 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.1245 | Train Acc: 0.3384 | Val Loss: 2.0052 | Val Acc: 0.2829 | LR: 0.000255 | Val F1: 0.3234
2025-12-14 22:30:01 | training | log_epoch | INFO     | Epoch [  10/50] | Train Loss: 1.2327 | Train Acc: 0.6931 | Val Loss: 1.2254 | Val Acc: 0.6942 | LR: 0.000673 | Val F1: 0.5979
2025-12-14 22:30:02 | training | log_epoch | INFO     | Epoch [  11/50] | Train Loss: 1.2519 | Train Acc: 0.6880 | Val Loss: 1.1612 | Val Acc: 0.7352 | LR: 0.000660 | Val F1: 0.6631
2025-12-14 22:30:02 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.1411 | Train Acc: 0.3000 | Val Loss: 2.0087 | Val Acc: 0.3490 | LR: 0.000252 | Val F1: 0.4004
2025-12-14 22:30:02 | training | train_model | INFO     | Early stopping triggered at epoch 24 due to no improvement in validation F1.
2025-12-14 22:30:02 | training | train_model | INFO     | Training for Hierarchical LSTM completed in 22.6s. Best validation F1: 0.4919
2025-12-14 22:30:02 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/hierarchical_v1/best_model.pth
2025-12-14 22:30:02 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:02 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:30:02 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:02 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:30:02 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:30:02 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:30:02 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:30:02 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:02 | training | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 22:30:02 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:02 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:02 | training | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 22:30:02 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:02 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:30:02 | training | log_config | INFO     |   learning_rate                  : 0.00073
2025-12-14 22:30:02 | training | log_config | INFO     |   weight_decay                   : 1.6e-07
2025-12-14 22:30:02 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:30:02 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:30:02 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:30:02 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:02 | training | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 22:30:02 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:02 | training | log_config | INFO     |   d_model                        : 128
2025-12-14 22:30:02 | training | log_config | INFO     |   nhead                          : 2
2025-12-14 22:30:02 | training | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 22:30:02 | training | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 22:30:02 | training | log_config | INFO     |   dropout                        : 0.1
2025-12-14 22:30:02 | training | log_config | INFO     |   learnable_pe                   : False
2025-12-14 22:30:02 | training | log_config | INFO     |   learning_rate                  : 0.00073
2025-12-14 22:30:02 | training | log_config | INFO     |   weight_decay                   : 1.6e-07
2025-12-14 22:30:02 | training | log_config | INFO     |   batch_size                     : 32
2025-12-14 22:30:02 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 22:30:02 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:30:02 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:02 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:30:02 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:02 | training | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 22:30:02 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:30:02 | training | log_model_summary | INFO     | 
2025-12-14 22:30:02 | training | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 22:30:02 | training | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 22:30:02 | training | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 22:30:02 | training | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 22:30:02 | training | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 22:30:02 | training | log_model_summary | INFO     | 
2025-12-14 22:30:02 | training | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 22:30:02 | training | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 22:30:02 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:30:02 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:30:02 | training | log_epoch | INFO     | Epoch [  12/50] | Train Loss: 1.2268 | Train Acc: 0.6900 | Val Loss: 1.1503 | Val Acc: 0.7015 | LR: 0.000646 | Val F1: 0.6090
2025-12-14 22:30:03 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.5497 | Train Acc: 0.6347 | Val Loss: 1.2972 | Val Acc: 0.6918 | LR: 0.000730 | Val F1: 0.5657
2025-12-14 22:30:03 | training | log_epoch | INFO     | Epoch [  13/50] | Train Loss: 1.2543 | Train Acc: 0.6890 | Val Loss: 1.2503 | Val Acc: 0.7015 | LR: 0.000631 | Val F1: 0.6055
2025-12-14 22:30:04 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3365 | Train Acc: 0.6826 | Val Loss: 1.2921 | Val Acc: 0.6918 | LR: 0.000730 | Val F1: 0.5662
2025-12-14 22:30:04 | training | log_epoch | INFO     | Epoch [  14/50] | Train Loss: 1.2530 | Train Acc: 0.6947 | Val Loss: 1.2598 | Val Acc: 0.6819 | LR: 0.000615 | Val F1: 0.5926
2025-12-14 22:30:05 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2787 | Train Acc: 0.6889 | Val Loss: 1.2436 | Val Acc: 0.7010 | LR: 0.000729 | Val F1: 0.5991
2025-12-14 22:30:05 | training | log_epoch | INFO     | Epoch [  15/50] | Train Loss: 1.2280 | Train Acc: 0.6945 | Val Loss: 1.1791 | Val Acc: 0.7014 | LR: 0.000598 | Val F1: 0.6059
2025-12-14 22:30:05 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2982 | Train Acc: 0.6780 | Val Loss: 1.2187 | Val Acc: 0.7004 | LR: 0.000728 | Val F1: 0.6014
2025-12-14 22:30:06 | training | log_epoch | INFO     | Epoch [  16/50] | Train Loss: 1.2351 | Train Acc: 0.6856 | Val Loss: 1.2465 | Val Acc: 0.6951 | LR: 0.000580 | Val F1: 0.5866
2025-12-14 22:30:06 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2927 | Train Acc: 0.6824 | Val Loss: 1.2101 | Val Acc: 0.6993 | LR: 0.000727 | Val F1: 0.6042
2025-12-14 22:30:06 | training | log_epoch | INFO     | Epoch [  17/50] | Train Loss: 1.2434 | Train Acc: 0.6885 | Val Loss: 1.1981 | Val Acc: 0.7028 | LR: 0.000561 | Val F1: 0.6132
2025-12-14 22:30:07 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2698 | Train Acc: 0.6908 | Val Loss: 1.3104 | Val Acc: 0.6907 | LR: 0.000726 | Val F1: 0.5845
2025-12-14 22:30:07 | training | log_epoch | INFO     | Epoch [  18/50] | Train Loss: 1.2316 | Train Acc: 0.6909 | Val Loss: 1.2115 | Val Acc: 0.7015 | LR: 0.000541 | Val F1: 0.6034
2025-12-14 22:30:08 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2600 | Train Acc: 0.6904 | Val Loss: 1.1432 | Val Acc: 0.7027 | LR: 0.000724 | Val F1: 0.6066
2025-12-14 22:30:08 | training | log_epoch | INFO     | Epoch [  19/50] | Train Loss: 1.2313 | Train Acc: 0.6863 | Val Loss: 1.1618 | Val Acc: 0.7063 | LR: 0.000521 | Val F1: 0.6315
2025-12-14 22:30:09 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2632 | Train Acc: 0.6859 | Val Loss: 1.1754 | Val Acc: 0.6976 | LR: 0.000721 | Val F1: 0.6020
2025-12-14 22:30:09 | training | log_epoch | INFO     | Epoch [  20/50] | Train Loss: 1.2210 | Train Acc: 0.6943 | Val Loss: 1.2366 | Val Acc: 0.6964 | LR: 0.000500 | Val F1: 0.6297
2025-12-14 22:30:09 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2772 | Train Acc: 0.6817 | Val Loss: 1.2188 | Val Acc: 0.6952 | LR: 0.000719 | Val F1: 0.6230
2025-12-14 22:30:10 | training | log_epoch | INFO     | Epoch [  21/50] | Train Loss: 1.2303 | Train Acc: 0.6937 | Val Loss: 1.2221 | Val Acc: 0.6983 | LR: 0.000478 | Val F1: 0.6053
2025-12-14 22:30:10 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2456 | Train Acc: 0.6892 | Val Loss: 1.1814 | Val Acc: 0.6980 | LR: 0.000716 | Val F1: 0.6080
2025-12-14 22:30:10 | training | log_epoch | INFO     | Epoch [  22/50] | Train Loss: 1.2270 | Train Acc: 0.6874 | Val Loss: 1.1323 | Val Acc: 0.7371 | LR: 0.000456 | Val F1: 0.6702
2025-12-14 22:30:11 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2266 | Train Acc: 0.6932 | Val Loss: 1.2654 | Val Acc: 0.7014 | LR: 0.000712 | Val F1: 0.6041
2025-12-14 22:30:11 | training | log_epoch | INFO     | Epoch [  23/50] | Train Loss: 1.2048 | Train Acc: 0.6941 | Val Loss: 1.1383 | Val Acc: 0.7049 | LR: 0.000434 | Val F1: 0.6192
2025-12-14 22:30:12 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2483 | Train Acc: 0.6882 | Val Loss: 1.2080 | Val Acc: 0.6990 | LR: 0.000708 | Val F1: 0.6119
2025-12-14 22:30:12 | training | log_epoch | INFO     | Epoch [  24/50] | Train Loss: 1.2289 | Train Acc: 0.6890 | Val Loss: 1.1837 | Val Acc: 0.6970 | LR: 0.000411 | Val F1: 0.6234
2025-12-14 22:30:13 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2339 | Train Acc: 0.6927 | Val Loss: 1.1917 | Val Acc: 0.7013 | LR: 0.000704 | Val F1: 0.6120
2025-12-14 22:30:13 | training | log_epoch | INFO     | Epoch [  25/50] | Train Loss: 1.2170 | Train Acc: 0.6907 | Val Loss: 1.1854 | Val Acc: 0.7020 | LR: 0.000388 | Val F1: 0.6092
2025-12-14 22:30:14 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2365 | Train Acc: 0.6917 | Val Loss: 1.2269 | Val Acc: 0.6973 | LR: 0.000700 | Val F1: 0.6287
2025-12-14 22:30:14 | training | log_epoch | INFO     | Epoch [  26/50] | Train Loss: 1.2045 | Train Acc: 0.6926 | Val Loss: 1.2109 | Val Acc: 0.6949 | LR: 0.000365 | Val F1: 0.5961
2025-12-14 22:30:14 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2291 | Train Acc: 0.6921 | Val Loss: 1.1780 | Val Acc: 0.6989 | LR: 0.000695 | Val F1: 0.6238
2025-12-14 22:30:15 | training | log_epoch | INFO     | Epoch [  27/50] | Train Loss: 1.2175 | Train Acc: 0.6926 | Val Loss: 1.1714 | Val Acc: 0.7028 | LR: 0.000343 | Val F1: 0.6132
2025-12-14 22:30:15 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2384 | Train Acc: 0.6851 | Val Loss: 1.1442 | Val Acc: 0.6985 | LR: 0.000690 | Val F1: 0.6094
2025-12-14 22:30:15 | training | log_epoch | INFO     | Epoch [  28/50] | Train Loss: 1.1928 | Train Acc: 0.7005 | Val Loss: 1.1670 | Val Acc: 0.7016 | LR: 0.000320 | Val F1: 0.6188
2025-12-14 22:30:16 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2351 | Train Acc: 0.6878 | Val Loss: 1.1736 | Val Acc: 0.7019 | LR: 0.000685 | Val F1: 0.6314
2025-12-14 22:30:16 | training | log_epoch | INFO     | Epoch [  29/50] | Train Loss: 1.2107 | Train Acc: 0.6891 | Val Loss: 1.1955 | Val Acc: 0.6957 | LR: 0.000297 | Val F1: 0.6071
2025-12-14 22:30:17 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2244 | Train Acc: 0.6907 | Val Loss: 1.1711 | Val Acc: 0.7150 | LR: 0.000679 | Val F1: 0.6236
2025-12-14 22:30:17 | training | log_epoch | INFO     | Epoch [  30/50] | Train Loss: 1.1914 | Train Acc: 0.6977 | Val Loss: 1.1335 | Val Acc: 0.7054 | LR: 0.000275 | Val F1: 0.6365
2025-12-14 22:30:18 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2169 | Train Acc: 0.6934 | Val Loss: 1.1907 | Val Acc: 0.6963 | LR: 0.000673 | Val F1: 0.6001
2025-12-14 22:30:18 | training | log_epoch | INFO     | Epoch [  31/50] | Train Loss: 1.2097 | Train Acc: 0.6930 | Val Loss: 1.1819 | Val Acc: 0.7109 | LR: 0.000253 | Val F1: 0.6191
2025-12-14 22:30:18 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2274 | Train Acc: 0.6912 | Val Loss: 1.2510 | Val Acc: 0.6947 | LR: 0.000667 | Val F1: 0.6108
2025-12-14 22:30:19 | training | log_epoch | INFO     | Epoch [  32/50] | Train Loss: 1.2071 | Train Acc: 0.6901 | Val Loss: 1.1688 | Val Acc: 0.7011 | LR: 0.000231 | Val F1: 0.6250
2025-12-14 22:30:19 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2284 | Train Acc: 0.6828 | Val Loss: 1.1751 | Val Acc: 0.7326 | LR: 0.000660 | Val F1: 0.6703
2025-12-14 22:30:19 | training | log_epoch | INFO     | Epoch [  33/50] | Train Loss: 1.1993 | Train Acc: 0.6969 | Val Loss: 1.1549 | Val Acc: 0.7038 | LR: 0.000210 | Val F1: 0.6227
2025-12-14 22:30:20 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2307 | Train Acc: 0.6846 | Val Loss: 1.1482 | Val Acc: 0.7207 | LR: 0.000654 | Val F1: 0.6509
2025-12-14 22:30:20 | training | log_epoch | INFO     | Epoch [  34/50] | Train Loss: 1.2068 | Train Acc: 0.6919 | Val Loss: 1.1742 | Val Acc: 0.7068 | LR: 0.000190 | Val F1: 0.6218
2025-12-14 22:30:21 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2172 | Train Acc: 0.6927 | Val Loss: 1.2276 | Val Acc: 0.6905 | LR: 0.000646 | Val F1: 0.6262
2025-12-14 22:30:21 | training | log_epoch | INFO     | Epoch [  35/50] | Train Loss: 1.1937 | Train Acc: 0.6971 | Val Loss: 1.1519 | Val Acc: 0.7039 | LR: 0.000170 | Val F1: 0.6275
2025-12-14 22:30:22 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2246 | Train Acc: 0.6884 | Val Loss: 1.1733 | Val Acc: 0.7305 | LR: 0.000639 | Val F1: 0.6634
2025-12-14 22:30:22 | training | log_epoch | INFO     | Epoch [  36/50] | Train Loss: 1.1865 | Train Acc: 0.6979 | Val Loss: 1.1566 | Val Acc: 0.7023 | LR: 0.000151 | Val F1: 0.6248
2025-12-14 22:30:22 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2184 | Train Acc: 0.6885 | Val Loss: 1.1675 | Val Acc: 0.7014 | LR: 0.000631 | Val F1: 0.6124
2025-12-14 22:30:23 | training | log_epoch | INFO     | Epoch [  37/50] | Train Loss: 1.1842 | Train Acc: 0.6990 | Val Loss: 1.1430 | Val Acc: 0.7035 | LR: 0.000133 | Val F1: 0.6286
2025-12-14 22:30:23 | training | train_model | INFO     | Early stopping triggered at epoch 37 due to no improvement in validation F1.
2025-12-14 22:30:23 | training | train_model | INFO     | Training for Transformer completed in 30.2s. Best validation F1: 0.6702
2025-12-14 22:30:23 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 22:30:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:23 | training | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 22:30:23 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:23 | training | train_main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 22:30:23 | training | train_main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 22:30:23 | training | train_main | INFO     | Transformer               0.0443       0.0038          0.0121       1.0000           1.0000            
2025-12-14 22:30:23 | training | train_main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_222951.json
2025-12-14 22:30:23 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2171 | Train Acc: 0.6895 | Val Loss: 1.1303 | Val Acc: 0.7030 | LR: 0.000623 | Val F1: 0.6318
2025-12-14 22:30:23 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2177 | Train Acc: 0.6933 | Val Loss: 1.2042 | Val Acc: 0.7005 | LR: 0.000615 | Val F1: 0.6207
2025-12-14 22:30:24 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2028 | Train Acc: 0.6963 | Val Loss: 1.1825 | Val Acc: 0.6985 | LR: 0.000607 | Val F1: 0.6002
2025-12-14 22:30:24 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2118 | Train Acc: 0.6907 | Val Loss: 1.1828 | Val Acc: 0.7111 | LR: 0.000598 | Val F1: 0.6231
2025-12-14 22:30:25 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2111 | Train Acc: 0.6873 | Val Loss: 1.1270 | Val Acc: 0.7029 | LR: 0.000589 | Val F1: 0.6336
2025-12-14 22:30:25 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.1957 | Train Acc: 0.6975 | Val Loss: 1.2060 | Val Acc: 0.6974 | LR: 0.000580 | Val F1: 0.6288
2025-12-14 22:30:26 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2010 | Train Acc: 0.6942 | Val Loss: 1.1406 | Val Acc: 0.6952 | LR: 0.000570 | Val F1: 0.6280
2025-12-14 22:30:26 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2118 | Train Acc: 0.6974 | Val Loss: 1.1712 | Val Acc: 0.6997 | LR: 0.000561 | Val F1: 0.6262
2025-12-14 22:30:27 | training | log_epoch | INFO     | Epoch [  34/100] | Train Loss: 1.2020 | Train Acc: 0.6924 | Val Loss: 1.1321 | Val Acc: 0.7046 | LR: 0.000551 | Val F1: 0.6420
2025-12-14 22:30:27 | training | log_epoch | INFO     | Epoch [  35/100] | Train Loss: 1.2316 | Train Acc: 0.6873 | Val Loss: 1.1813 | Val Acc: 0.6996 | LR: 0.000541 | Val F1: 0.6205
2025-12-14 22:30:28 | training | log_epoch | INFO     | Epoch [  36/100] | Train Loss: 1.2092 | Train Acc: 0.6916 | Val Loss: 1.1384 | Val Acc: 0.7023 | LR: 0.000531 | Val F1: 0.6331
2025-12-14 22:30:28 | training | train_model | INFO     | Early stopping triggered at epoch 36 due to no improvement in validation F1.
2025-12-14 22:30:28 | training | train_model | INFO     | Training for Transformer completed in 25.8s. Best validation F1: 0.6703
2025-12-14 22:30:28 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 22:30:28 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:28 | training | log_header | INFO     | TRAINING: STATISTICAL BASELINE
2025-12-14 22:30:28 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:29 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:29 | training | log_header | INFO     | BASELINE TEST SET METRICS
2025-12-14 22:30:29 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:29 | training | log_evaluation | INFO     |   accuracy                  : 0.4997
2025-12-14 22:30:29 | training | log_evaluation | INFO     |   f1_weighted               : 0.4644
2025-12-14 22:30:29 | training | log_evaluation | INFO     |   f1_macro                  : 0.1069
2025-12-14 22:30:29 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:29 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:29 | training | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 22:30:29 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:30:29 | training | train_main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 22:30:29 | training | train_main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 22:30:29 | training | train_main | INFO     | LSTM_v2 (SeqLabeling)     0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 22:30:29 | training | train_main | INFO     | Hierarchical LSTM         0.6703       0.5380          0.1147       0.0000           0.0000            
2025-12-14 22:30:29 | training | train_main | INFO     | Statistical Baseline      0.4997       0.4644          0.1069       0.0000           0.0000            
2025-12-14 22:30:29 | training | train_main | INFO     | Transformer               0.1037       0.0195          0.0269       1.0000           1.0000            
2025-12-14 22:30:29 | training | train_main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_222931.json
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | log_header | INFO     | CONFIGURATION
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | log_config | INFO     |   Selected Models                : ['transformer']
2025-12-14 22:33:03 | training | log_config | INFO     |   Epochs                         : 50
2025-12-14 22:33:03 | training | log_config | INFO     |   Device                         : cuda
2025-12-14 22:33:03 | training | log_config | INFO     |   Log Level                      : INFO
2025-12-14 22:33:03 | training | log_config | INFO     |   Augmentation                   : True
2025-12-14 22:33:03 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 22:33:03 | training | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:33:03 | training | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:33:03 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:33:03 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:33:03 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | log_config | INFO     |   epochs                         : 50
2025-12-14 22:33:03 | training | log_config | INFO     |   learning_rate                  : 0.00073
2025-12-14 22:33:03 | training | log_config | INFO     |   weight_decay                   : 1.6e-07
2025-12-14 22:33:03 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:33:03 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:33:03 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | log_config | INFO     |   d_model                        : 128
2025-12-14 22:33:03 | training | log_config | INFO     |   nhead                          : 2
2025-12-14 22:33:03 | training | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 22:33:03 | training | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 22:33:03 | training | log_config | INFO     |   dropout                        : 0.1
2025-12-14 22:33:03 | training | log_config | INFO     |   learnable_pe                   : False
2025-12-14 22:33:03 | training | log_config | INFO     |   learning_rate                  : 0.00073
2025-12-14 22:33:03 | training | log_config | INFO     |   weight_decay                   : 1.6e-07
2025-12-14 22:33:03 | training | log_config | INFO     |   batch_size                     : 32
2025-12-14 22:33:03 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 22:33:03 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:33:03 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:03 | training | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 22:33:03 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:33:03 | training | log_model_summary | INFO     | 
2025-12-14 22:33:03 | training | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 22:33:03 | training | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 22:33:03 | training | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 22:33:03 | training | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 22:33:03 | training | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 22:33:03 | training | log_model_summary | INFO     | 
2025-12-14 22:33:03 | training | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 22:33:03 | training | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 22:33:03 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:33:03 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:33:05 | training | log_epoch | INFO     | Epoch [   1/50] | Train Loss: 1.7741 | Train Acc: 0.5795 | Val Loss: 1.2587 | Val Acc: 0.6918 | LR: 0.000730 | Val F1: 0.5657
2025-12-14 22:33:05 | training | log_epoch | INFO     | Epoch [   2/50] | Train Loss: 1.3177 | Train Acc: 0.6880 | Val Loss: 1.2108 | Val Acc: 0.6918 | LR: 0.000729 | Val F1: 0.5657
2025-12-14 22:33:06 | training | log_epoch | INFO     | Epoch [   3/50] | Train Loss: 1.3012 | Train Acc: 0.6839 | Val Loss: 1.1869 | Val Acc: 0.7029 | LR: 0.000727 | Val F1: 0.6287
2025-12-14 22:33:06 | training | log_epoch | INFO     | Epoch [   4/50] | Train Loss: 1.2861 | Train Acc: 0.6801 | Val Loss: 1.2383 | Val Acc: 0.6914 | LR: 0.000724 | Val F1: 0.5980
2025-12-14 22:33:07 | training | log_epoch | INFO     | Epoch [   5/50] | Train Loss: 1.3154 | Train Acc: 0.6806 | Val Loss: 1.1963 | Val Acc: 0.7020 | LR: 0.000719 | Val F1: 0.6064
2025-12-14 22:33:07 | training | log_epoch | INFO     | Epoch [   6/50] | Train Loss: 1.2960 | Train Acc: 0.6799 | Val Loss: 1.1874 | Val Acc: 0.7003 | LR: 0.000712 | Val F1: 0.6093
2025-12-14 22:33:08 | training | log_epoch | INFO     | Epoch [   7/50] | Train Loss: 1.2614 | Train Acc: 0.6892 | Val Loss: 1.1434 | Val Acc: 0.7079 | LR: 0.000704 | Val F1: 0.6142
2025-12-14 22:33:08 | training | log_epoch | INFO     | Epoch [   8/50] | Train Loss: 1.2511 | Train Acc: 0.6903 | Val Loss: 1.1559 | Val Acc: 0.7029 | LR: 0.000695 | Val F1: 0.6112
2025-12-14 22:33:08 | training | log_epoch | INFO     | Epoch [   9/50] | Train Loss: 1.2505 | Train Acc: 0.6880 | Val Loss: 1.2217 | Val Acc: 0.6968 | LR: 0.000685 | Val F1: 0.5952
2025-12-14 22:33:09 | training | log_epoch | INFO     | Epoch [  10/50] | Train Loss: 1.2327 | Train Acc: 0.6931 | Val Loss: 1.2254 | Val Acc: 0.6942 | LR: 0.000673 | Val F1: 0.5979
2025-12-14 22:33:09 | training | log_epoch | INFO     | Epoch [  11/50] | Train Loss: 1.2519 | Train Acc: 0.6880 | Val Loss: 1.1612 | Val Acc: 0.7352 | LR: 0.000660 | Val F1: 0.6631
2025-12-14 22:33:10 | training | log_epoch | INFO     | Epoch [  12/50] | Train Loss: 1.2268 | Train Acc: 0.6900 | Val Loss: 1.1503 | Val Acc: 0.7015 | LR: 0.000646 | Val F1: 0.6090
2025-12-14 22:33:10 | training | log_epoch | INFO     | Epoch [  13/50] | Train Loss: 1.2543 | Train Acc: 0.6890 | Val Loss: 1.2503 | Val Acc: 0.7015 | LR: 0.000631 | Val F1: 0.6055
2025-12-14 22:33:11 | training | log_epoch | INFO     | Epoch [  14/50] | Train Loss: 1.2530 | Train Acc: 0.6947 | Val Loss: 1.2598 | Val Acc: 0.6819 | LR: 0.000615 | Val F1: 0.5926
2025-12-14 22:33:11 | training | log_epoch | INFO     | Epoch [  15/50] | Train Loss: 1.2280 | Train Acc: 0.6945 | Val Loss: 1.1791 | Val Acc: 0.7014 | LR: 0.000598 | Val F1: 0.6059
2025-12-14 22:33:11 | training | log_epoch | INFO     | Epoch [  16/50] | Train Loss: 1.2351 | Train Acc: 0.6856 | Val Loss: 1.2465 | Val Acc: 0.6951 | LR: 0.000580 | Val F1: 0.5866
2025-12-14 22:33:12 | training | log_epoch | INFO     | Epoch [  17/50] | Train Loss: 1.2434 | Train Acc: 0.6885 | Val Loss: 1.1981 | Val Acc: 0.7028 | LR: 0.000561 | Val F1: 0.6132
2025-12-14 22:33:12 | training | log_epoch | INFO     | Epoch [  18/50] | Train Loss: 1.2316 | Train Acc: 0.6909 | Val Loss: 1.2115 | Val Acc: 0.7015 | LR: 0.000541 | Val F1: 0.6034
2025-12-14 22:33:13 | training | log_epoch | INFO     | Epoch [  19/50] | Train Loss: 1.2313 | Train Acc: 0.6863 | Val Loss: 1.1618 | Val Acc: 0.7063 | LR: 0.000521 | Val F1: 0.6315
2025-12-14 22:33:13 | training | log_epoch | INFO     | Epoch [  20/50] | Train Loss: 1.2210 | Train Acc: 0.6943 | Val Loss: 1.2366 | Val Acc: 0.6964 | LR: 0.000500 | Val F1: 0.6297
2025-12-14 22:33:14 | training | log_epoch | INFO     | Epoch [  21/50] | Train Loss: 1.2303 | Train Acc: 0.6937 | Val Loss: 1.2221 | Val Acc: 0.6983 | LR: 0.000478 | Val F1: 0.6053
2025-12-14 22:33:14 | training | log_epoch | INFO     | Epoch [  22/50] | Train Loss: 1.2270 | Train Acc: 0.6874 | Val Loss: 1.1323 | Val Acc: 0.7371 | LR: 0.000456 | Val F1: 0.6702
2025-12-14 22:33:15 | training | log_epoch | INFO     | Epoch [  23/50] | Train Loss: 1.2048 | Train Acc: 0.6941 | Val Loss: 1.1383 | Val Acc: 0.7049 | LR: 0.000434 | Val F1: 0.6192
2025-12-14 22:33:15 | training | log_epoch | INFO     | Epoch [  24/50] | Train Loss: 1.2289 | Train Acc: 0.6890 | Val Loss: 1.1837 | Val Acc: 0.6970 | LR: 0.000411 | Val F1: 0.6234
2025-12-14 22:33:15 | training | log_epoch | INFO     | Epoch [  25/50] | Train Loss: 1.2170 | Train Acc: 0.6907 | Val Loss: 1.1854 | Val Acc: 0.7020 | LR: 0.000388 | Val F1: 0.6092
2025-12-14 22:33:16 | training | log_epoch | INFO     | Epoch [  26/50] | Train Loss: 1.2045 | Train Acc: 0.6926 | Val Loss: 1.2109 | Val Acc: 0.6949 | LR: 0.000365 | Val F1: 0.5961
2025-12-14 22:33:16 | training | log_epoch | INFO     | Epoch [  27/50] | Train Loss: 1.2175 | Train Acc: 0.6926 | Val Loss: 1.1714 | Val Acc: 0.7028 | LR: 0.000343 | Val F1: 0.6132
2025-12-14 22:33:17 | training | log_epoch | INFO     | Epoch [  28/50] | Train Loss: 1.1928 | Train Acc: 0.7005 | Val Loss: 1.1670 | Val Acc: 0.7016 | LR: 0.000320 | Val F1: 0.6188
2025-12-14 22:33:17 | training | log_epoch | INFO     | Epoch [  29/50] | Train Loss: 1.2107 | Train Acc: 0.6891 | Val Loss: 1.1955 | Val Acc: 0.6957 | LR: 0.000297 | Val F1: 0.6071
2025-12-14 22:33:18 | training | log_epoch | INFO     | Epoch [  30/50] | Train Loss: 1.1914 | Train Acc: 0.6977 | Val Loss: 1.1335 | Val Acc: 0.7054 | LR: 0.000275 | Val F1: 0.6365
2025-12-14 22:33:18 | training | log_epoch | INFO     | Epoch [  31/50] | Train Loss: 1.2097 | Train Acc: 0.6930 | Val Loss: 1.1819 | Val Acc: 0.7109 | LR: 0.000253 | Val F1: 0.6191
2025-12-14 22:33:19 | training | log_epoch | INFO     | Epoch [  32/50] | Train Loss: 1.2071 | Train Acc: 0.6901 | Val Loss: 1.1688 | Val Acc: 0.7011 | LR: 0.000231 | Val F1: 0.6250
2025-12-14 22:33:19 | training | log_epoch | INFO     | Epoch [  33/50] | Train Loss: 1.1993 | Train Acc: 0.6969 | Val Loss: 1.1549 | Val Acc: 0.7038 | LR: 0.000210 | Val F1: 0.6227
2025-12-14 22:33:19 | training | log_epoch | INFO     | Epoch [  34/50] | Train Loss: 1.2068 | Train Acc: 0.6919 | Val Loss: 1.1742 | Val Acc: 0.7068 | LR: 0.000190 | Val F1: 0.6218
2025-12-14 22:33:20 | training | log_epoch | INFO     | Epoch [  35/50] | Train Loss: 1.1937 | Train Acc: 0.6971 | Val Loss: 1.1519 | Val Acc: 0.7039 | LR: 0.000170 | Val F1: 0.6275
2025-12-14 22:33:20 | training | log_epoch | INFO     | Epoch [  36/50] | Train Loss: 1.1865 | Train Acc: 0.6979 | Val Loss: 1.1566 | Val Acc: 0.7023 | LR: 0.000151 | Val F1: 0.6248
2025-12-14 22:33:21 | training | log_epoch | INFO     | Epoch [  37/50] | Train Loss: 1.1842 | Train Acc: 0.6990 | Val Loss: 1.1430 | Val Acc: 0.7035 | LR: 0.000133 | Val F1: 0.6286
2025-12-14 22:33:21 | training | train_model | INFO     | Early stopping triggered at epoch 37 due to no improvement in validation F1.
2025-12-14 22:33:21 | training | train_model | INFO     | Training for Transformer completed in 16.6s. Best validation F1: 0.6702
2025-12-14 22:33:21 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 22:33:21 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:21 | training | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 22:33:21 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:21 | training | train_main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 22:33:21 | training | train_main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 22:33:21 | training | train_main | INFO     | Transformer               0.6749       0.6099          0.2024       0.3884           0.0334            
2025-12-14 22:33:21 | training | train_main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_223303.json
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | log_header | INFO     | CONFIGURATION
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 22:33:36 | training | log_config | INFO     |   Epochs                         : 100
2025-12-14 22:33:36 | training | log_config | INFO     |   Device                         : cuda
2025-12-14 22:33:36 | training | log_config | INFO     |   Log Level                      : INFO
2025-12-14 22:33:36 | training | log_config | INFO     |   Augmentation                   : True
2025-12-14 22:33:36 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 22:33:36 | training | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:33:36 | training | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:33:36 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:33:36 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:33:36 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:33:36 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 22:33:36 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 22:33:36 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:33:36 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:33:36 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | log_config | INFO     |   hidden_size                    : 96
2025-12-14 22:33:36 | training | log_config | INFO     |   num_layers                     : 2
2025-12-14 22:33:36 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 22:33:36 | training | log_config | INFO     |   bidirectional                  : True
2025-12-14 22:33:36 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 22:33:36 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 22:33:36 | training | log_config | INFO     |   batch_size                     : 16
2025-12-14 22:33:36 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 22:33:36 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 22:33:36 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:33:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:36 | training | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 22:33:36 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:33:36 | training | log_model_summary | INFO     | 
2025-12-14 22:33:36 | training | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 22:33:36 | training | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 22:33:36 | training | log_model_summary | INFO     | 
2025-12-14 22:33:36 | training | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 22:33:36 | training | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 22:33:36 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:33:36 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:33:38 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 22:33:38 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 22:33:38 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 22:33:38 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 22:33:39 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 22:33:39 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 22:33:39 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 22:33:39 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 22:33:40 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 22:33:40 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 22:33:40 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 22:33:40 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 22:33:40 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 22:33:41 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 22:33:41 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 22:33:41 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 22:33:41 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 22:33:42 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 22:33:42 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 22:33:42 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 22:33:42 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 22:33:43 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 22:33:43 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 22:33:43 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 22:33:43 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 22:33:44 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 22:33:44 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 22:33:44 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 22:33:44 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 22:33:44 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 22:33:45 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 22:33:45 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 22:33:45 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 22:33:45 | training | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 22:33:45 | training | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 8.0s. Best validation F1: 0.6429
2025-12-14 22:33:45 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 22:33:45 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:45 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:33:45 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:45 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:33:45 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:33:45 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:33:45 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:33:45 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:45 | training | log_header | INFO     | TRAINING: HIERARCHICAL LSTM
2025-12-14 22:33:45 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:45 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:45 | training | log_header | INFO     | HIERARCHICAL LSTM CONFIG
2025-12-14 22:33:45 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:45 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:33:45 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 22:33:45 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 22:33:45 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:33:45 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:33:45 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:33:45 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:45 | training | log_header | INFO     | HIERARCHICAL LSTM BEST HYPERPARAMETERS
2025-12-14 22:33:45 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:45 | training | log_config | INFO     |   learning_rate                  : 0.000288
2025-12-14 22:33:45 | training | log_config | INFO     |   weight_decay                   : 0.000149
2025-12-14 22:33:45 | training | log_config | INFO     |   batch_size                     : 32
2025-12-14 22:33:45 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 22:33:45 | training | log_config | INFO     |   grad_clip                      : 0.5
2025-12-14 22:33:45 | training | log_config | INFO     |   s1_hidden_size                 : 192
2025-12-14 22:33:45 | training | log_config | INFO     |   s1_num_layers                  : 2
2025-12-14 22:33:45 | training | log_config | INFO     |   s1_dropout                     : 0.5
2025-12-14 22:33:45 | training | log_config | INFO     |   s1_bidirectional               : True
2025-12-14 22:33:45 | training | log_config | INFO     |   s2_hidden_size                 : 64
2025-12-14 22:33:45 | training | log_config | INFO     |   s2_num_layers                  : 3
2025-12-14 22:33:45 | training | log_config | INFO     |   s2_dropout                     : 0.3
2025-12-14 22:33:45 | training | log_config | INFO     |   s2_bidirectional               : False
2025-12-14 22:33:45 | training | log_config | INFO     |   use_class_weights              : True
2025-12-14 22:33:45 | training | log_config | INFO     |   class_weight_scale             : 0.6
2025-12-14 22:33:45 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:33:45 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:45 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:33:45 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:33:45 | training | log_model_summary | INFO     | Model: HierarchicalClassifier
2025-12-14 22:33:45 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:33:45 | training | log_model_summary | INFO     | 
2025-12-14 22:33:45 | training | log_model_summary | INFO     |   stage1                    :  1,193,091 params ( 1,193,091 trainable)
2025-12-14 22:33:45 | training | log_model_summary | INFO     |   stage2                    :     84,675 params (    84,675 trainable)
2025-12-14 22:33:45 | training | log_model_summary | INFO     | 
2025-12-14 22:33:45 | training | log_model_summary | INFO     | Total parameters          :  1,277,766
2025-12-14 22:33:45 | training | log_model_summary | INFO     | Trainable parameters      :  1,277,766
2025-12-14 22:33:45 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:33:45 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:33:46 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 2.3949 | Train Acc: 0.2390 | Val Loss: 1.9860 | Val Acc: 0.2101 | LR: 0.000288 | Val F1: 0.1914
2025-12-14 22:33:47 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 2.3152 | Train Acc: 0.2117 | Val Loss: 2.0607 | Val Acc: 0.2404 | LR: 0.000288 | Val F1: 0.2729
2025-12-14 22:33:48 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 2.3001 | Train Acc: 0.1852 | Val Loss: 1.9873 | Val Acc: 0.2655 | LR: 0.000288 | Val F1: 0.3146
2025-12-14 22:33:48 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 2.2574 | Train Acc: 0.1779 | Val Loss: 1.9836 | Val Acc: 0.2182 | LR: 0.000287 | Val F1: 0.2417
2025-12-14 22:33:49 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 2.2528 | Train Acc: 0.2352 | Val Loss: 1.9733 | Val Acc: 0.2745 | LR: 0.000287 | Val F1: 0.3110
2025-12-14 22:33:50 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 2.2219 | Train Acc: 0.2438 | Val Loss: 2.0731 | Val Acc: 0.1727 | LR: 0.000286 | Val F1: 0.1778
2025-12-14 22:33:51 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 2.2270 | Train Acc: 0.2021 | Val Loss: 2.0306 | Val Acc: 0.3899 | LR: 0.000285 | Val F1: 0.4496
2025-12-14 22:33:52 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 2.2329 | Train Acc: 0.2830 | Val Loss: 1.9791 | Val Acc: 0.1543 | LR: 0.000285 | Val F1: 0.1320
2025-12-14 22:33:53 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 2.2236 | Train Acc: 0.2172 | Val Loss: 2.0190 | Val Acc: 0.4240 | LR: 0.000283 | Val F1: 0.4919
2025-12-14 22:33:54 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 2.2004 | Train Acc: 0.3239 | Val Loss: 1.9722 | Val Acc: 0.1903 | LR: 0.000282 | Val F1: 0.2052
2025-12-14 22:33:54 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 2.2156 | Train Acc: 0.2425 | Val Loss: 2.0374 | Val Acc: 0.2116 | LR: 0.000281 | Val F1: 0.2280
2025-12-14 22:33:55 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 2.1825 | Train Acc: 0.2420 | Val Loss: 2.0586 | Val Acc: 0.1573 | LR: 0.000280 | Val F1: 0.1609
2025-12-14 22:33:56 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 2.1836 | Train Acc: 0.2435 | Val Loss: 1.9628 | Val Acc: 0.2378 | LR: 0.000278 | Val F1: 0.2691
2025-12-14 22:33:57 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 2.1835 | Train Acc: 0.2686 | Val Loss: 1.9564 | Val Acc: 0.4023 | LR: 0.000276 | Val F1: 0.4581
2025-12-14 22:33:58 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 2.1889 | Train Acc: 0.2617 | Val Loss: 1.9553 | Val Acc: 0.1901 | LR: 0.000274 | Val F1: 0.2030
2025-12-14 22:33:59 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 2.1606 | Train Acc: 0.2830 | Val Loss: 2.0343 | Val Acc: 0.2225 | LR: 0.000272 | Val F1: 0.2518
2025-12-14 22:33:59 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 2.1493 | Train Acc: 0.3194 | Val Loss: 2.0108 | Val Acc: 0.1785 | LR: 0.000270 | Val F1: 0.1895
2025-12-14 22:34:00 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 2.1637 | Train Acc: 0.2176 | Val Loss: 2.0132 | Val Acc: 0.3490 | LR: 0.000268 | Val F1: 0.4048
2025-12-14 22:34:01 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 2.1246 | Train Acc: 0.3117 | Val Loss: 1.9923 | Val Acc: 0.2252 | LR: 0.000266 | Val F1: 0.2524
2025-12-14 22:34:02 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 2.1473 | Train Acc: 0.2826 | Val Loss: 1.9858 | Val Acc: 0.3020 | LR: 0.000263 | Val F1: 0.3570
2025-12-14 22:34:03 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 2.1418 | Train Acc: 0.2769 | Val Loss: 1.9281 | Val Acc: 0.3245 | LR: 0.000261 | Val F1: 0.3781
2025-12-14 22:34:04 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 2.1467 | Train Acc: 0.2208 | Val Loss: 2.0474 | Val Acc: 0.3464 | LR: 0.000258 | Val F1: 0.4137
2025-12-14 22:34:04 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 2.1245 | Train Acc: 0.3384 | Val Loss: 2.0052 | Val Acc: 0.2829 | LR: 0.000255 | Val F1: 0.3234
2025-12-14 22:34:05 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 2.1411 | Train Acc: 0.3000 | Val Loss: 2.0087 | Val Acc: 0.3490 | LR: 0.000252 | Val F1: 0.4004
2025-12-14 22:34:05 | training | train_model | INFO     | Early stopping triggered at epoch 24 due to no improvement in validation F1.
2025-12-14 22:34:05 | training | train_model | INFO     | Training for Hierarchical LSTM completed in 20.0s. Best validation F1: 0.4919
2025-12-14 22:34:05 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/hierarchical_v1/best_model.pth
2025-12-14 22:34:05 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:05 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:34:05 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:05 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:34:05 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:34:05 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:34:05 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:34:05 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:05 | training | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 22:34:05 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:05 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:05 | training | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 22:34:05 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:05 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:34:05 | training | log_config | INFO     |   learning_rate                  : 0.00073
2025-12-14 22:34:05 | training | log_config | INFO     |   weight_decay                   : 1.6e-07
2025-12-14 22:34:05 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:34:05 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:34:05 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:34:05 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:05 | training | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 22:34:05 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:05 | training | log_config | INFO     |   d_model                        : 128
2025-12-14 22:34:05 | training | log_config | INFO     |   nhead                          : 2
2025-12-14 22:34:05 | training | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 22:34:05 | training | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 22:34:05 | training | log_config | INFO     |   dropout                        : 0.1
2025-12-14 22:34:05 | training | log_config | INFO     |   learnable_pe                   : False
2025-12-14 22:34:05 | training | log_config | INFO     |   learning_rate                  : 0.00073
2025-12-14 22:34:05 | training | log_config | INFO     |   weight_decay                   : 1.6e-07
2025-12-14 22:34:05 | training | log_config | INFO     |   batch_size                     : 32
2025-12-14 22:34:05 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 22:34:05 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:34:05 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:05 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:34:05 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:05 | training | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 22:34:05 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:34:05 | training | log_model_summary | INFO     | 
2025-12-14 22:34:05 | training | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 22:34:05 | training | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 22:34:05 | training | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 22:34:05 | training | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 22:34:05 | training | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 22:34:05 | training | log_model_summary | INFO     | 
2025-12-14 22:34:05 | training | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 22:34:05 | training | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 22:34:05 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:34:05 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:34:06 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.5497 | Train Acc: 0.6347 | Val Loss: 1.2972 | Val Acc: 0.6918 | LR: 0.000730 | Val F1: 0.5657
2025-12-14 22:34:06 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3365 | Train Acc: 0.6826 | Val Loss: 1.2921 | Val Acc: 0.6918 | LR: 0.000730 | Val F1: 0.5662
2025-12-14 22:34:07 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2787 | Train Acc: 0.6889 | Val Loss: 1.2436 | Val Acc: 0.7010 | LR: 0.000729 | Val F1: 0.5991
2025-12-14 22:34:07 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2982 | Train Acc: 0.6780 | Val Loss: 1.2187 | Val Acc: 0.7004 | LR: 0.000728 | Val F1: 0.6014
2025-12-14 22:34:07 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2927 | Train Acc: 0.6824 | Val Loss: 1.2101 | Val Acc: 0.6993 | LR: 0.000727 | Val F1: 0.6042
2025-12-14 22:34:08 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2698 | Train Acc: 0.6908 | Val Loss: 1.3104 | Val Acc: 0.6907 | LR: 0.000726 | Val F1: 0.5845
2025-12-14 22:34:08 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2600 | Train Acc: 0.6904 | Val Loss: 1.1432 | Val Acc: 0.7027 | LR: 0.000724 | Val F1: 0.6066
2025-12-14 22:34:09 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2632 | Train Acc: 0.6859 | Val Loss: 1.1754 | Val Acc: 0.6976 | LR: 0.000721 | Val F1: 0.6020
2025-12-14 22:34:09 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2772 | Train Acc: 0.6817 | Val Loss: 1.2188 | Val Acc: 0.6952 | LR: 0.000719 | Val F1: 0.6230
2025-12-14 22:34:10 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2456 | Train Acc: 0.6892 | Val Loss: 1.1814 | Val Acc: 0.6980 | LR: 0.000716 | Val F1: 0.6080
2025-12-14 22:34:10 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2266 | Train Acc: 0.6932 | Val Loss: 1.2654 | Val Acc: 0.7014 | LR: 0.000712 | Val F1: 0.6041
2025-12-14 22:34:11 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2483 | Train Acc: 0.6882 | Val Loss: 1.2080 | Val Acc: 0.6990 | LR: 0.000708 | Val F1: 0.6119
2025-12-14 22:34:11 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2339 | Train Acc: 0.6927 | Val Loss: 1.1917 | Val Acc: 0.7013 | LR: 0.000704 | Val F1: 0.6120
2025-12-14 22:34:11 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2365 | Train Acc: 0.6917 | Val Loss: 1.2269 | Val Acc: 0.6973 | LR: 0.000700 | Val F1: 0.6287
2025-12-14 22:34:12 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2291 | Train Acc: 0.6921 | Val Loss: 1.1780 | Val Acc: 0.6989 | LR: 0.000695 | Val F1: 0.6238
2025-12-14 22:34:12 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2384 | Train Acc: 0.6851 | Val Loss: 1.1442 | Val Acc: 0.6985 | LR: 0.000690 | Val F1: 0.6094
2025-12-14 22:34:13 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2351 | Train Acc: 0.6878 | Val Loss: 1.1736 | Val Acc: 0.7019 | LR: 0.000685 | Val F1: 0.6314
2025-12-14 22:34:13 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2244 | Train Acc: 0.6907 | Val Loss: 1.1711 | Val Acc: 0.7150 | LR: 0.000679 | Val F1: 0.6236
2025-12-14 22:34:14 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2169 | Train Acc: 0.6934 | Val Loss: 1.1907 | Val Acc: 0.6963 | LR: 0.000673 | Val F1: 0.6001
2025-12-14 22:34:14 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2274 | Train Acc: 0.6912 | Val Loss: 1.2510 | Val Acc: 0.6947 | LR: 0.000667 | Val F1: 0.6108
2025-12-14 22:34:14 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2284 | Train Acc: 0.6828 | Val Loss: 1.1751 | Val Acc: 0.7326 | LR: 0.000660 | Val F1: 0.6703
2025-12-14 22:34:15 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2307 | Train Acc: 0.6846 | Val Loss: 1.1482 | Val Acc: 0.7207 | LR: 0.000654 | Val F1: 0.6509
2025-12-14 22:34:15 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2172 | Train Acc: 0.6927 | Val Loss: 1.2276 | Val Acc: 0.6905 | LR: 0.000646 | Val F1: 0.6262
2025-12-14 22:34:16 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2246 | Train Acc: 0.6884 | Val Loss: 1.1733 | Val Acc: 0.7305 | LR: 0.000639 | Val F1: 0.6634
2025-12-14 22:34:16 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2184 | Train Acc: 0.6885 | Val Loss: 1.1675 | Val Acc: 0.7014 | LR: 0.000631 | Val F1: 0.6124
2025-12-14 22:34:17 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2171 | Train Acc: 0.6895 | Val Loss: 1.1303 | Val Acc: 0.7030 | LR: 0.000623 | Val F1: 0.6318
2025-12-14 22:34:17 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2177 | Train Acc: 0.6933 | Val Loss: 1.2042 | Val Acc: 0.7005 | LR: 0.000615 | Val F1: 0.6207
2025-12-14 22:34:17 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2028 | Train Acc: 0.6963 | Val Loss: 1.1825 | Val Acc: 0.6985 | LR: 0.000607 | Val F1: 0.6002
2025-12-14 22:34:18 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2118 | Train Acc: 0.6907 | Val Loss: 1.1828 | Val Acc: 0.7111 | LR: 0.000598 | Val F1: 0.6231
2025-12-14 22:34:18 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2111 | Train Acc: 0.6873 | Val Loss: 1.1270 | Val Acc: 0.7029 | LR: 0.000589 | Val F1: 0.6336
2025-12-14 22:34:19 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.1957 | Train Acc: 0.6975 | Val Loss: 1.2060 | Val Acc: 0.6974 | LR: 0.000580 | Val F1: 0.6288
2025-12-14 22:34:19 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2010 | Train Acc: 0.6942 | Val Loss: 1.1406 | Val Acc: 0.6952 | LR: 0.000570 | Val F1: 0.6280
2025-12-14 22:34:20 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2118 | Train Acc: 0.6974 | Val Loss: 1.1712 | Val Acc: 0.6997 | LR: 0.000561 | Val F1: 0.6262
2025-12-14 22:34:20 | training | log_epoch | INFO     | Epoch [  34/100] | Train Loss: 1.2020 | Train Acc: 0.6924 | Val Loss: 1.1321 | Val Acc: 0.7046 | LR: 0.000551 | Val F1: 0.6420
2025-12-14 22:34:21 | training | log_epoch | INFO     | Epoch [  35/100] | Train Loss: 1.2316 | Train Acc: 0.6873 | Val Loss: 1.1813 | Val Acc: 0.6996 | LR: 0.000541 | Val F1: 0.6205
2025-12-14 22:34:21 | training | log_epoch | INFO     | Epoch [  36/100] | Train Loss: 1.2092 | Train Acc: 0.6916 | Val Loss: 1.1384 | Val Acc: 0.7023 | LR: 0.000531 | Val F1: 0.6331
2025-12-14 22:34:21 | training | train_model | INFO     | Early stopping triggered at epoch 36 due to no improvement in validation F1.
2025-12-14 22:34:21 | training | train_model | INFO     | Training for Transformer completed in 15.7s. Best validation F1: 0.6703
2025-12-14 22:34:21 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 22:34:21 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:21 | training | log_header | INFO     | TRAINING: STATISTICAL BASELINE
2025-12-14 22:34:21 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:22 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:22 | training | log_header | INFO     | BASELINE TEST SET METRICS
2025-12-14 22:34:22 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:22 | training | log_evaluation | INFO     |   accuracy                  : 0.4997
2025-12-14 22:34:22 | training | log_evaluation | INFO     |   f1_weighted               : 0.4644
2025-12-14 22:34:22 | training | log_evaluation | INFO     |   f1_macro                  : 0.1069
2025-12-14 22:34:22 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:22 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:22 | training | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 22:34:22 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:34:22 | training | train_main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 22:34:22 | training | train_main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 22:34:22 | training | train_main | INFO     | Transformer               0.6611       0.5954          0.1698       0.4226           0.0366            
2025-12-14 22:34:22 | training | train_main | INFO     | LSTM_v2 (SeqLabeling)     0.6653       0.5912          0.1750       0.3760           0.0286            
2025-12-14 22:34:22 | training | train_main | INFO     | Statistical Baseline      0.4997       0.4644          0.1069       0.0000           0.0000            
2025-12-14 22:34:22 | training | train_main | INFO     | Hierarchical LSTM         0.3082       0.3724          0.1483       0.8847           0.6598            
2025-12-14 22:34:22 | training | train_main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_223336.json
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | log_header | INFO     | CONFIGURATION
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | log_config | INFO     |   Selected Models                : ['lstm_v2', 'hierarchical_v1', 'transformer', 'baseline']
2025-12-14 22:36:49 | training | log_config | INFO     |   Epochs                         : 100
2025-12-14 22:36:49 | training | log_config | INFO     |   Device                         : cuda
2025-12-14 22:36:49 | training | log_config | INFO     |   Log Level                      : INFO
2025-12-14 22:36:49 | training | log_config | INFO     |   Augmentation                   : True
2025-12-14 22:36:49 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 22:36:49 | training | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:36:49 | training | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:36:49 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:36:49 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:36:49 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | log_header | INFO     | TRAINING: LSTM_V2 (SEQLABELING)
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) CONFIG
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:36:49 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 22:36:49 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 22:36:49 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:36:49 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:36:49 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | log_header | INFO     | LSTM_V2 (SEQLABELING) BEST HYPERPARAMETERS
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | log_config | INFO     |   hidden_size                    : 96
2025-12-14 22:36:49 | training | log_config | INFO     |   num_layers                     : 2
2025-12-14 22:36:49 | training | log_config | INFO     |   dropout                        : 0.5
2025-12-14 22:36:49 | training | log_config | INFO     |   bidirectional                  : True
2025-12-14 22:36:49 | training | log_config | INFO     |   learning_rate                  : 0.00106
2025-12-14 22:36:49 | training | log_config | INFO     |   weight_decay                   : 0.00057
2025-12-14 22:36:49 | training | log_config | INFO     |   batch_size                     : 16
2025-12-14 22:36:49 | training | log_config | INFO     |   scheduler                      : cosine
2025-12-14 22:36:49 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 22:36:49 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:36:49 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:49 | training | log_model_summary | INFO     | Model: SeqLabelingLSTM
2025-12-14 22:36:49 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:36:49 | training | log_model_summary | INFO     | 
2025-12-14 22:36:49 | training | log_model_summary | INFO     |   lstm                      :    301,056 params (   301,056 trainable)
2025-12-14 22:36:49 | training | log_model_summary | INFO     |   classifier                :      1,351 params (     1,351 trainable)
2025-12-14 22:36:49 | training | log_model_summary | INFO     | 
2025-12-14 22:36:49 | training | log_model_summary | INFO     | Total parameters          :    302,407
2025-12-14 22:36:49 | training | log_model_summary | INFO     | Trainable parameters      :    302,407
2025-12-14 22:36:49 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:36:49 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:36:50 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:50 | training | log_header | INFO     | MULTI-MODEL TRAINING & COMPARISON
2025-12-14 22:36:50 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:50 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:50 | training | log_header | INFO     | CONFIGURATION
2025-12-14 22:36:50 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:50 | training | log_config | INFO     |   Selected Models                : ['hierarchical_v1']
2025-12-14 22:36:50 | training | log_config | INFO     |   Epochs                         : 50
2025-12-14 22:36:50 | training | log_config | INFO     |   Device                         : cuda
2025-12-14 22:36:50 | training | log_config | INFO     |   Log Level                      : INFO
2025-12-14 22:36:50 | training | log_config | INFO     |   Augmentation                   : True
2025-12-14 22:36:50 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:36:50 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:50 | training | log_header | INFO     | LOADING SPLIT DATA
2025-12-14 22:36:50 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:50 | training | load_split_data | INFO     | Loaded train data: X=(210, 256, 4), Y=(210, 256)
2025-12-14 22:36:50 | training | load_split_data | INFO     | Loaded val data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:36:50 | training | load_split_data | INFO     | Loaded test data: X=(45, 256, 4), Y=(45, 256)
2025-12-14 22:36:50 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:50 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:36:50 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:50 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:36:50 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:36:50 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:36:50 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:36:50 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:50 | training | log_header | INFO     | TRAINING: HIERARCHICAL LSTM
2025-12-14 22:36:50 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:50 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:50 | training | log_header | INFO     | HIERARCHICAL LSTM CONFIG
2025-12-14 22:36:50 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:50 | training | log_config | INFO     |   epochs                         : 50
2025-12-14 22:36:50 | training | log_config | INFO     |   learning_rate                  : 7.29e-05
2025-12-14 22:36:50 | training | log_config | INFO     |   weight_decay                   : 1.98e-06
2025-12-14 22:36:50 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:36:50 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:36:50 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:36:50 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:50 | training | log_header | INFO     | HIERARCHICAL LSTM BEST HYPERPARAMETERS
2025-12-14 22:36:50 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:50 | training | log_config | INFO     |   learning_rate                  : 7.29e-05
2025-12-14 22:36:50 | training | log_config | INFO     |   weight_decay                   : 1.98e-06
2025-12-14 22:36:50 | training | log_config | INFO     |   batch_size                     : 16
2025-12-14 22:36:50 | training | log_config | INFO     |   scheduler                      : step
2025-12-14 22:36:50 | training | log_config | INFO     |   grad_clip                      : 2.0
2025-12-14 22:36:50 | training | log_config | INFO     |   s1_hidden_size                 : 128
2025-12-14 22:36:50 | training | log_config | INFO     |   s1_num_layers                  : 2
2025-12-14 22:36:50 | training | log_config | INFO     |   s1_dropout                     : 0.2
2025-12-14 22:36:50 | training | log_config | INFO     |   s1_bidirectional               : True
2025-12-14 22:36:50 | training | log_config | INFO     |   s2_hidden_size                 : 96
2025-12-14 22:36:50 | training | log_config | INFO     |   s2_num_layers                  : 3
2025-12-14 22:36:50 | training | log_config | INFO     |   s2_dropout                     : 0.25
2025-12-14 22:36:50 | training | log_config | INFO     |   s2_bidirectional               : True
2025-12-14 22:36:50 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 22:36:50 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:36:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:51 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:36:51 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:36:51 | training | log_model_summary | INFO     | Model: HierarchicalClassifier
2025-12-14 22:36:51 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:36:51 | training | log_model_summary | INFO     | 
2025-12-14 22:36:51 | training | log_model_summary | INFO     |   stage1                    :    533,251 params (   533,251 trainable)
2025-12-14 22:36:51 | training | log_model_summary | INFO     |   stage2                    :    524,355 params (   524,355 trainable)
2025-12-14 22:36:51 | training | log_model_summary | INFO     | 
2025-12-14 22:36:51 | training | log_model_summary | INFO     | Total parameters          :  1,057,606
2025-12-14 22:36:51 | training | log_model_summary | INFO     | Trainable parameters      :  1,057,606
2025-12-14 22:36:51 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:36:51 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:36:51 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.4794 | Train Acc: 0.6378 | Val Loss: 1.3050 | Val Acc: 0.6918 | LR: 0.001060 | Val F1: 0.5657
2025-12-14 22:36:51 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3273 | Train Acc: 0.6778 | Val Loss: 1.2956 | Val Acc: 0.6892 | LR: 0.001060 | Val F1: 0.5721
2025-12-14 22:36:51 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2849 | Train Acc: 0.6831 | Val Loss: 1.2577 | Val Acc: 0.6894 | LR: 0.001059 | Val F1: 0.5998
2025-12-14 22:36:51 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2801 | Train Acc: 0.6824 | Val Loss: 1.2680 | Val Acc: 0.6932 | LR: 0.001058 | Val F1: 0.5786
2025-12-14 22:36:52 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2700 | Train Acc: 0.6832 | Val Loss: 1.2554 | Val Acc: 0.6872 | LR: 0.001056 | Val F1: 0.5990
2025-12-14 22:36:52 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2598 | Train Acc: 0.6854 | Val Loss: 1.2314 | Val Acc: 0.6992 | LR: 0.001053 | Val F1: 0.6044
2025-12-14 22:36:52 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2719 | Train Acc: 0.6790 | Val Loss: 1.2190 | Val Acc: 0.7010 | LR: 0.001051 | Val F1: 0.6008
2025-12-14 22:36:52 | training | log_epoch | INFO     | Epoch [   1/50] | Train Loss: 1.4084 | Train Acc: 0.6702 | Val Loss: 1.2417 | Val Acc: 0.6918 | LR: 0.000073 | Val F1: 0.5657
2025-12-14 22:36:52 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2565 | Train Acc: 0.6813 | Val Loss: 1.2002 | Val Acc: 0.7003 | LR: 0.001047 | Val F1: 0.6066
2025-12-14 22:36:53 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2430 | Train Acc: 0.6916 | Val Loss: 1.1875 | Val Acc: 0.7003 | LR: 0.001043 | Val F1: 0.6242
2025-12-14 22:36:53 | training | log_epoch | INFO     | Epoch [   2/50] | Train Loss: 1.3036 | Train Acc: 0.6858 | Val Loss: 1.2660 | Val Acc: 0.6918 | LR: 0.000073 | Val F1: 0.5657
2025-12-14 22:36:53 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2426 | Train Acc: 0.6897 | Val Loss: 1.2658 | Val Acc: 0.7030 | LR: 0.001039 | Val F1: 0.6019
2025-12-14 22:36:53 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2456 | Train Acc: 0.6939 | Val Loss: 1.2091 | Val Acc: 0.7040 | LR: 0.001034 | Val F1: 0.6118
2025-12-14 22:36:53 | training | log_epoch | INFO     | Epoch [   3/50] | Train Loss: 1.2817 | Train Acc: 0.6874 | Val Loss: 1.2624 | Val Acc: 0.6918 | LR: 0.000073 | Val F1: 0.5657
2025-12-14 22:36:54 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2488 | Train Acc: 0.6863 | Val Loss: 1.2232 | Val Acc: 0.7005 | LR: 0.001029 | Val F1: 0.5977
2025-12-14 22:36:54 | training | log_epoch | INFO     | Epoch [   4/50] | Train Loss: 1.2852 | Train Acc: 0.6819 | Val Loss: 1.2318 | Val Acc: 0.6918 | LR: 0.000072 | Val F1: 0.5657
2025-12-14 22:36:54 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2517 | Train Acc: 0.6880 | Val Loss: 1.2097 | Val Acc: 0.6991 | LR: 0.001023 | Val F1: 0.5986
2025-12-14 22:36:54 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2540 | Train Acc: 0.6840 | Val Loss: 1.1841 | Val Acc: 0.6960 | LR: 0.001016 | Val F1: 0.5826
2025-12-14 22:36:55 | training | log_epoch | INFO     | Epoch [   5/50] | Train Loss: 1.2898 | Train Acc: 0.6807 | Val Loss: 1.2216 | Val Acc: 0.6918 | LR: 0.000072 | Val F1: 0.5657
2025-12-14 22:36:55 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2467 | Train Acc: 0.6855 | Val Loss: 1.1621 | Val Acc: 0.6998 | LR: 0.001010 | Val F1: 0.6174
2025-12-14 22:36:55 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2356 | Train Acc: 0.6935 | Val Loss: 1.1613 | Val Acc: 0.7148 | LR: 0.001002 | Val F1: 0.6277
2025-12-14 22:36:55 | training | log_epoch | INFO     | Epoch [   6/50] | Train Loss: 1.2543 | Train Acc: 0.6864 | Val Loss: 1.2360 | Val Acc: 0.6918 | LR: 0.000071 | Val F1: 0.5667
2025-12-14 22:36:55 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2381 | Train Acc: 0.6897 | Val Loss: 1.1820 | Val Acc: 0.7009 | LR: 0.000995 | Val F1: 0.6212
2025-12-14 22:36:56 | training | log_epoch | INFO     | Epoch [   7/50] | Train Loss: 1.2579 | Train Acc: 0.6845 | Val Loss: 1.1724 | Val Acc: 0.6918 | LR: 0.000070 | Val F1: 0.5669
2025-12-14 22:36:56 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2371 | Train Acc: 0.6921 | Val Loss: 1.1751 | Val Acc: 0.7165 | LR: 0.000986 | Val F1: 0.6429
2025-12-14 22:36:56 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2375 | Train Acc: 0.6881 | Val Loss: 1.1810 | Val Acc: 0.7020 | LR: 0.000978 | Val F1: 0.6208
2025-12-14 22:36:56 | training | log_epoch | INFO     | Epoch [   8/50] | Train Loss: 1.2326 | Train Acc: 0.6913 | Val Loss: 1.1805 | Val Acc: 0.6929 | LR: 0.000069 | Val F1: 0.5696
2025-12-14 22:36:56 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2351 | Train Acc: 0.6902 | Val Loss: 1.1992 | Val Acc: 0.7063 | LR: 0.000968 | Val F1: 0.6170
2025-12-14 22:36:57 | training | log_epoch | INFO     | Epoch [   9/50] | Train Loss: 1.2430 | Train Acc: 0.6876 | Val Loss: 1.2295 | Val Acc: 0.6852 | LR: 0.000068 | Val F1: 0.5731
2025-12-14 22:36:57 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2401 | Train Acc: 0.6886 | Val Loss: 1.2261 | Val Acc: 0.6998 | LR: 0.000959 | Val F1: 0.6127
2025-12-14 22:36:57 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2333 | Train Acc: 0.6908 | Val Loss: 1.1799 | Val Acc: 0.7022 | LR: 0.000949 | Val F1: 0.6211
2025-12-14 22:36:57 | training | log_epoch | INFO     | Epoch [  10/50] | Train Loss: 1.2375 | Train Acc: 0.6860 | Val Loss: 1.1677 | Val Acc: 0.7303 | LR: 0.000067 | Val F1: 0.6422
2025-12-14 22:36:57 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2349 | Train Acc: 0.6892 | Val Loss: 1.2069 | Val Acc: 0.7007 | LR: 0.000938 | Val F1: 0.6231
2025-12-14 22:36:58 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2257 | Train Acc: 0.6954 | Val Loss: 1.1913 | Val Acc: 0.6981 | LR: 0.000928 | Val F1: 0.6281
2025-12-14 22:36:58 | training | log_epoch | INFO     | Epoch [  11/50] | Train Loss: 1.2297 | Train Acc: 0.6900 | Val Loss: 1.1806 | Val Acc: 0.7112 | LR: 0.000066 | Val F1: 0.6142
2025-12-14 22:36:58 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2200 | Train Acc: 0.6947 | Val Loss: 1.1877 | Val Acc: 0.7016 | LR: 0.000916 | Val F1: 0.6250
2025-12-14 22:36:58 | training | log_epoch | INFO     | Epoch [  12/50] | Train Loss: 1.2100 | Train Acc: 0.6964 | Val Loss: 1.1896 | Val Acc: 0.6982 | LR: 0.000065 | Val F1: 0.6057
2025-12-14 22:36:58 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2200 | Train Acc: 0.6945 | Val Loss: 1.1943 | Val Acc: 0.7084 | LR: 0.000905 | Val F1: 0.6194
2025-12-14 22:36:59 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.2209 | Train Acc: 0.6921 | Val Loss: 1.2047 | Val Acc: 0.7027 | LR: 0.000893 | Val F1: 0.6258
2025-12-14 22:36:59 | training | log_epoch | INFO     | Epoch [  13/50] | Train Loss: 1.2207 | Train Acc: 0.6915 | Val Loss: 1.1554 | Val Acc: 0.7325 | LR: 0.000063 | Val F1: 0.6515
2025-12-14 22:36:59 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.2201 | Train Acc: 0.6953 | Val Loss: 1.1673 | Val Acc: 0.7039 | LR: 0.000881 | Val F1: 0.6256
2025-12-14 22:36:59 | training | log_epoch | INFO     | Epoch [  14/50] | Train Loss: 1.2131 | Train Acc: 0.6945 | Val Loss: 1.1908 | Val Acc: 0.6937 | LR: 0.000062 | Val F1: 0.5934
2025-12-14 22:36:59 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.2208 | Train Acc: 0.6947 | Val Loss: 1.1984 | Val Acc: 0.6963 | LR: 0.000868 | Val F1: 0.6101
2025-12-14 22:37:00 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.2214 | Train Acc: 0.6933 | Val Loss: 1.1813 | Val Acc: 0.6972 | LR: 0.000855 | Val F1: 0.6150
2025-12-14 22:37:00 | training | log_epoch | INFO     | Epoch [  15/50] | Train Loss: 1.2188 | Train Acc: 0.6905 | Val Loss: 1.1600 | Val Acc: 0.6944 | LR: 0.000060 | Val F1: 0.5908
2025-12-14 22:37:00 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.2230 | Train Acc: 0.6932 | Val Loss: 1.2066 | Val Acc: 0.6991 | LR: 0.000842 | Val F1: 0.6164
2025-12-14 22:37:00 | training | log_epoch | INFO     | Epoch [  16/50] | Train Loss: 1.2199 | Train Acc: 0.6889 | Val Loss: 1.1545 | Val Acc: 0.7335 | LR: 0.000058 | Val F1: 0.6479
2025-12-14 22:37:00 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.2156 | Train Acc: 0.6943 | Val Loss: 1.1886 | Val Acc: 0.7016 | LR: 0.000828 | Val F1: 0.6176
2025-12-14 22:37:01 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.2173 | Train Acc: 0.6950 | Val Loss: 1.1796 | Val Acc: 0.7021 | LR: 0.000814 | Val F1: 0.6229
2025-12-14 22:37:01 | training | train_model | INFO     | Early stopping triggered at epoch 33 due to no improvement in validation F1.
2025-12-14 22:37:01 | training | train_model | INFO     | Training for LSTM_v2 (SeqLabeling) completed in 10.7s. Best validation F1: 0.6429
2025-12-14 22:37:01 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/lstm_v2/best_model.pth
2025-12-14 22:37:01 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:01 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:37:01 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:01 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:37:01 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:37:01 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:37:01 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:37:01 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:01 | training | log_header | INFO     | TRAINING: HIERARCHICAL LSTM
2025-12-14 22:37:01 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:01 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:01 | training | log_header | INFO     | HIERARCHICAL LSTM CONFIG
2025-12-14 22:37:01 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:01 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:37:01 | training | log_config | INFO     |   learning_rate                  : 7.29e-05
2025-12-14 22:37:01 | training | log_config | INFO     |   weight_decay                   : 1.98e-06
2025-12-14 22:37:01 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:37:01 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:37:01 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:37:01 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:01 | training | log_header | INFO     | HIERARCHICAL LSTM BEST HYPERPARAMETERS
2025-12-14 22:37:01 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:01 | training | log_config | INFO     |   learning_rate                  : 7.29e-05
2025-12-14 22:37:01 | training | log_config | INFO     |   weight_decay                   : 1.98e-06
2025-12-14 22:37:01 | training | log_config | INFO     |   batch_size                     : 16
2025-12-14 22:37:01 | training | log_config | INFO     |   scheduler                      : step
2025-12-14 22:37:01 | training | log_config | INFO     |   grad_clip                      : 2.0
2025-12-14 22:37:01 | training | log_config | INFO     |   s1_hidden_size                 : 128
2025-12-14 22:37:01 | training | log_config | INFO     |   s1_num_layers                  : 2
2025-12-14 22:37:01 | training | log_config | INFO     |   s1_dropout                     : 0.2
2025-12-14 22:37:01 | training | log_config | INFO     |   s1_bidirectional               : True
2025-12-14 22:37:01 | training | log_config | INFO     |   s2_hidden_size                 : 96
2025-12-14 22:37:01 | training | log_config | INFO     |   s2_num_layers                  : 3
2025-12-14 22:37:01 | training | log_config | INFO     |   s2_dropout                     : 0.25
2025-12-14 22:37:01 | training | log_config | INFO     |   s2_bidirectional               : True
2025-12-14 22:37:01 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 22:37:01 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:37:01 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:01 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:37:01 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:01 | training | log_model_summary | INFO     | Model: HierarchicalClassifier
2025-12-14 22:37:01 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:37:01 | training | log_model_summary | INFO     | 
2025-12-14 22:37:01 | training | log_model_summary | INFO     |   stage1                    :    533,251 params (   533,251 trainable)
2025-12-14 22:37:01 | training | log_model_summary | INFO     |   stage2                    :    524,355 params (   524,355 trainable)
2025-12-14 22:37:01 | training | log_model_summary | INFO     | 
2025-12-14 22:37:01 | training | log_model_summary | INFO     | Total parameters          :  1,057,606
2025-12-14 22:37:01 | training | log_model_summary | INFO     | Trainable parameters      :  1,057,606
2025-12-14 22:37:01 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:37:01 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:37:01 | training | log_epoch | INFO     | Epoch [  17/50] | Train Loss: 1.2176 | Train Acc: 0.6881 | Val Loss: 1.1441 | Val Acc: 0.7329 | LR: 0.000056 | Val F1: 0.6548
2025-12-14 22:37:02 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.3870 | Train Acc: 0.6782 | Val Loss: 1.3301 | Val Acc: 0.6918 | LR: 0.000073 | Val F1: 0.5657
2025-12-14 22:37:02 | training | log_epoch | INFO     | Epoch [  18/50] | Train Loss: 1.2055 | Train Acc: 0.6909 | Val Loss: 1.1706 | Val Acc: 0.6895 | LR: 0.000054 | Val F1: 0.6014
2025-12-14 22:37:02 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3188 | Train Acc: 0.6831 | Val Loss: 1.2595 | Val Acc: 0.6918 | LR: 0.000073 | Val F1: 0.5657
2025-12-14 22:37:02 | training | log_epoch | INFO     | Epoch [  19/50] | Train Loss: 1.2050 | Train Acc: 0.6909 | Val Loss: 1.1694 | Val Acc: 0.7259 | LR: 0.000052 | Val F1: 0.6365
2025-12-14 22:37:03 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.2825 | Train Acc: 0.6842 | Val Loss: 1.2271 | Val Acc: 0.6918 | LR: 0.000073 | Val F1: 0.5657
2025-12-14 22:37:03 | training | log_epoch | INFO     | Epoch [  20/50] | Train Loss: 1.1996 | Train Acc: 0.6956 | Val Loss: 1.1583 | Val Acc: 0.7016 | LR: 0.000050 | Val F1: 0.6143
2025-12-14 22:37:04 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2822 | Train Acc: 0.6845 | Val Loss: 1.2387 | Val Acc: 0.6918 | LR: 0.000073 | Val F1: 0.5657
2025-12-14 22:37:04 | training | log_epoch | INFO     | Epoch [  21/50] | Train Loss: 1.2036 | Train Acc: 0.6907 | Val Loss: 1.1288 | Val Acc: 0.7424 | LR: 0.000048 | Val F1: 0.6715
2025-12-14 22:37:04 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2807 | Train Acc: 0.6819 | Val Loss: 1.2840 | Val Acc: 0.6918 | LR: 0.000073 | Val F1: 0.5657
2025-12-14 22:37:04 | training | log_epoch | INFO     | Epoch [  22/50] | Train Loss: 1.2006 | Train Acc: 0.6918 | Val Loss: 1.1410 | Val Acc: 0.6967 | LR: 0.000046 | Val F1: 0.6018
2025-12-14 22:37:05 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2676 | Train Acc: 0.6835 | Val Loss: 1.2416 | Val Acc: 0.6918 | LR: 0.000072 | Val F1: 0.5657
2025-12-14 22:37:05 | training | log_epoch | INFO     | Epoch [  23/50] | Train Loss: 1.1985 | Train Acc: 0.6931 | Val Loss: 1.1209 | Val Acc: 0.6994 | LR: 0.000044 | Val F1: 0.6140
2025-12-14 22:37:06 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2442 | Train Acc: 0.6901 | Val Loss: 1.2024 | Val Acc: 0.6918 | LR: 0.000072 | Val F1: 0.5695
2025-12-14 22:37:06 | training | log_epoch | INFO     | Epoch [  24/50] | Train Loss: 1.1937 | Train Acc: 0.6941 | Val Loss: 1.1254 | Val Acc: 0.6963 | LR: 0.000041 | Val F1: 0.6042
2025-12-14 22:37:06 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2433 | Train Acc: 0.6868 | Val Loss: 1.2459 | Val Acc: 0.7095 | LR: 0.000072 | Val F1: 0.6041
2025-12-14 22:37:06 | training | log_epoch | INFO     | Epoch [  25/50] | Train Loss: 1.1930 | Train Acc: 0.6931 | Val Loss: 1.1526 | Val Acc: 0.6949 | LR: 0.000039 | Val F1: 0.5971
2025-12-14 22:37:07 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2509 | Train Acc: 0.6842 | Val Loss: 1.2697 | Val Acc: 0.6918 | LR: 0.000072 | Val F1: 0.5671
2025-12-14 22:37:07 | training | log_epoch | INFO     | Epoch [  26/50] | Train Loss: 1.1911 | Train Acc: 0.6949 | Val Loss: 1.1537 | Val Acc: 0.6997 | LR: 0.000037 | Val F1: 0.6145
2025-12-14 22:37:08 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2346 | Train Acc: 0.6862 | Val Loss: 1.1780 | Val Acc: 0.7320 | LR: 0.000071 | Val F1: 0.6408
2025-12-14 22:37:08 | training | log_epoch | INFO     | Epoch [  27/50] | Train Loss: 1.1861 | Train Acc: 0.6952 | Val Loss: 1.1456 | Val Acc: 0.7027 | LR: 0.000035 | Val F1: 0.6213
2025-12-14 22:37:08 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2292 | Train Acc: 0.6883 | Val Loss: 1.1665 | Val Acc: 0.7196 | LR: 0.000071 | Val F1: 0.6244
2025-12-14 22:37:09 | training | log_epoch | INFO     | Epoch [  28/50] | Train Loss: 1.1875 | Train Acc: 0.6945 | Val Loss: 1.1396 | Val Acc: 0.6986 | LR: 0.000032 | Val F1: 0.6115
2025-12-14 22:37:09 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2355 | Train Acc: 0.6874 | Val Loss: 1.2007 | Val Acc: 0.7220 | LR: 0.000071 | Val F1: 0.6258
2025-12-14 22:37:09 | training | log_epoch | INFO     | Epoch [  29/50] | Train Loss: 1.1820 | Train Acc: 0.6951 | Val Loss: 1.1366 | Val Acc: 0.6985 | LR: 0.000030 | Val F1: 0.6137
2025-12-14 22:37:10 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2316 | Train Acc: 0.6882 | Val Loss: 1.2315 | Val Acc: 0.6919 | LR: 0.000070 | Val F1: 0.5827
2025-12-14 22:37:10 | training | log_epoch | INFO     | Epoch [  30/50] | Train Loss: 1.1909 | Train Acc: 0.6912 | Val Loss: 1.1755 | Val Acc: 0.6967 | LR: 0.000028 | Val F1: 0.6027
2025-12-14 22:37:10 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2287 | Train Acc: 0.6859 | Val Loss: 1.1755 | Val Acc: 0.7224 | LR: 0.000070 | Val F1: 0.6273
2025-12-14 22:37:11 | training | log_epoch | INFO     | Epoch [  31/50] | Train Loss: 1.1804 | Train Acc: 0.6943 | Val Loss: 1.1367 | Val Acc: 0.7076 | LR: 0.000026 | Val F1: 0.6285
2025-12-14 22:37:11 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2171 | Train Acc: 0.6916 | Val Loss: 1.1596 | Val Acc: 0.7320 | LR: 0.000069 | Val F1: 0.6627
2025-12-14 22:37:11 | training | log_epoch | INFO     | Epoch [  32/50] | Train Loss: 1.1887 | Train Acc: 0.6924 | Val Loss: 1.1313 | Val Acc: 0.7408 | LR: 0.000024 | Val F1: 0.6648
2025-12-14 22:37:12 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2178 | Train Acc: 0.6892 | Val Loss: 1.1743 | Val Acc: 0.7322 | LR: 0.000069 | Val F1: 0.6483
2025-12-14 22:37:12 | training | log_epoch | INFO     | Epoch [  33/50] | Train Loss: 1.1823 | Train Acc: 0.6934 | Val Loss: 1.1357 | Val Acc: 0.7414 | LR: 0.000022 | Val F1: 0.6691
2025-12-14 22:37:12 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2191 | Train Acc: 0.6909 | Val Loss: 1.1264 | Val Acc: 0.7349 | LR: 0.000068 | Val F1: 0.6491
2025-12-14 22:37:13 | training | log_epoch | INFO     | Epoch [  34/50] | Train Loss: 1.1808 | Train Acc: 0.6957 | Val Loss: 1.1407 | Val Acc: 0.6985 | LR: 0.000020 | Val F1: 0.6129
2025-12-14 22:37:13 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2220 | Train Acc: 0.6883 | Val Loss: 1.2111 | Val Acc: 0.7065 | LR: 0.000068 | Val F1: 0.6053
2025-12-14 22:37:13 | training | log_epoch | INFO     | Epoch [  35/50] | Train Loss: 1.1742 | Train Acc: 0.6966 | Val Loss: 1.1308 | Val Acc: 0.7204 | LR: 0.000018 | Val F1: 0.6490
2025-12-14 22:37:14 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2267 | Train Acc: 0.6866 | Val Loss: 1.1645 | Val Acc: 0.7349 | LR: 0.000067 | Val F1: 0.6541
2025-12-14 22:37:14 | training | log_epoch | INFO     | Epoch [  36/50] | Train Loss: 1.1735 | Train Acc: 0.6974 | Val Loss: 1.1363 | Val Acc: 0.7365 | LR: 0.000016 | Val F1: 0.6645
2025-12-14 22:37:14 | training | train_model | INFO     | Early stopping triggered at epoch 36 due to no improvement in validation F1.
2025-12-14 22:37:14 | training | train_model | INFO     | Training for Hierarchical LSTM completed in 22.5s. Best validation F1: 0.6715
2025-12-14 22:37:14 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/hierarchical_v1/best_model.pth
2025-12-14 22:37:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:14 | training | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 22:37:14 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:14 | training | train_main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 22:37:14 | training | train_main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 22:37:14 | training | train_main | INFO     | Hierarchical LSTM         0.6706       0.5852          0.1480       0.3125           0.0241            
2025-12-14 22:37:14 | training | train_main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_223650.json
2025-12-14 22:37:14 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2138 | Train Acc: 0.6885 | Val Loss: 1.1337 | Val Acc: 0.7385 | LR: 0.000067 | Val F1: 0.6566
2025-12-14 22:37:15 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2115 | Train Acc: 0.6873 | Val Loss: 1.2092 | Val Acc: 0.7043 | LR: 0.000066 | Val F1: 0.5960
2025-12-14 22:37:15 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2081 | Train Acc: 0.6918 | Val Loss: 1.1608 | Val Acc: 0.7387 | LR: 0.000065 | Val F1: 0.6663
2025-12-14 22:37:16 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.1971 | Train Acc: 0.6928 | Val Loss: 1.1606 | Val Acc: 0.7401 | LR: 0.000065 | Val F1: 0.6587
2025-12-14 22:37:16 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.1984 | Train Acc: 0.6926 | Val Loss: 1.1557 | Val Acc: 0.7353 | LR: 0.000064 | Val F1: 0.6552
2025-12-14 22:37:17 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.1943 | Train Acc: 0.6907 | Val Loss: 1.1484 | Val Acc: 0.7375 | LR: 0.000063 | Val F1: 0.6554
2025-12-14 22:37:17 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.1819 | Train Acc: 0.6970 | Val Loss: 1.1230 | Val Acc: 0.7435 | LR: 0.000062 | Val F1: 0.6791
2025-12-14 22:37:18 | training | log_epoch | INFO     | Epoch [  27/100] | Train Loss: 1.1852 | Train Acc: 0.6965 | Val Loss: 1.1574 | Val Acc: 0.7404 | LR: 0.000062 | Val F1: 0.6597
2025-12-14 22:37:18 | training | log_epoch | INFO     | Epoch [  28/100] | Train Loss: 1.1937 | Train Acc: 0.6915 | Val Loss: 1.1572 | Val Acc: 0.7082 | LR: 0.000061 | Val F1: 0.6347
2025-12-14 22:37:18 | training | log_epoch | INFO     | Epoch [  29/100] | Train Loss: 1.1843 | Train Acc: 0.6955 | Val Loss: 1.1506 | Val Acc: 0.7427 | LR: 0.000060 | Val F1: 0.6624
2025-12-14 22:37:19 | training | log_epoch | INFO     | Epoch [  30/100] | Train Loss: 1.1916 | Train Acc: 0.6925 | Val Loss: 1.1390 | Val Acc: 0.7396 | LR: 0.000059 | Val F1: 0.6632
2025-12-14 22:37:19 | training | log_epoch | INFO     | Epoch [  31/100] | Train Loss: 1.1930 | Train Acc: 0.6926 | Val Loss: 1.1544 | Val Acc: 0.7345 | LR: 0.000058 | Val F1: 0.6593
2025-12-14 22:37:20 | training | log_epoch | INFO     | Epoch [  32/100] | Train Loss: 1.1936 | Train Acc: 0.6926 | Val Loss: 1.1406 | Val Acc: 0.7408 | LR: 0.000057 | Val F1: 0.6667
2025-12-14 22:37:20 | training | log_epoch | INFO     | Epoch [  33/100] | Train Loss: 1.1854 | Train Acc: 0.6950 | Val Loss: 1.1771 | Val Acc: 0.6932 | LR: 0.000056 | Val F1: 0.5967
2025-12-14 22:37:21 | training | log_epoch | INFO     | Epoch [  34/100] | Train Loss: 1.1828 | Train Acc: 0.6945 | Val Loss: 1.1463 | Val Acc: 0.7403 | LR: 0.000055 | Val F1: 0.6641
2025-12-14 22:37:21 | training | log_epoch | INFO     | Epoch [  35/100] | Train Loss: 1.1794 | Train Acc: 0.6952 | Val Loss: 1.1479 | Val Acc: 0.7391 | LR: 0.000054 | Val F1: 0.6626
2025-12-14 22:37:22 | training | log_epoch | INFO     | Epoch [  36/100] | Train Loss: 1.1808 | Train Acc: 0.6945 | Val Loss: 1.1762 | Val Acc: 0.7270 | LR: 0.000053 | Val F1: 0.6456
2025-12-14 22:37:22 | training | log_epoch | INFO     | Epoch [  37/100] | Train Loss: 1.1813 | Train Acc: 0.6959 | Val Loss: 1.1667 | Val Acc: 0.7355 | LR: 0.000052 | Val F1: 0.6616
2025-12-14 22:37:23 | training | log_epoch | INFO     | Epoch [  38/100] | Train Loss: 1.1664 | Train Acc: 0.6996 | Val Loss: 1.1519 | Val Acc: 0.7243 | LR: 0.000051 | Val F1: 0.6486
2025-12-14 22:37:23 | training | log_epoch | INFO     | Epoch [  39/100] | Train Loss: 1.1672 | Train Acc: 0.6968 | Val Loss: 1.1467 | Val Acc: 0.7319 | LR: 0.000050 | Val F1: 0.6620
2025-12-14 22:37:24 | training | log_epoch | INFO     | Epoch [  40/100] | Train Loss: 1.1645 | Train Acc: 0.7008 | Val Loss: 1.1392 | Val Acc: 0.7056 | LR: 0.000049 | Val F1: 0.6303
2025-12-14 22:37:24 | training | log_epoch | INFO     | Epoch [  41/100] | Train Loss: 1.1659 | Train Acc: 0.6977 | Val Loss: 1.1418 | Val Acc: 0.7372 | LR: 0.000048 | Val F1: 0.6694
2025-12-14 22:37:24 | training | train_model | INFO     | Early stopping triggered at epoch 41 due to no improvement in validation F1.
2025-12-14 22:37:24 | training | train_model | INFO     | Training for Hierarchical LSTM completed in 23.2s. Best validation F1: 0.6791
2025-12-14 22:37:24 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/hierarchical_v1/best_model.pth
2025-12-14 22:37:24 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:24 | training | log_header | INFO     | DATALOADER PREPARATION
2025-12-14 22:37:24 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:24 | training | prepare_dataloaders | INFO     | Data normalized using OHLCScaler (fitted on training data).
2025-12-14 22:37:24 | training | prepare_dataloaders | INFO     | Train data augmented. New size: 504 samples
2025-12-14 22:37:24 | training | prepare_dataloaders | INFO     | Calculated class weights (capped at 5.0): [0.21 4.11 2.02 1.95 3.58 3.03 2.79]
2025-12-14 22:37:24 | training | prepare_dataloaders | INFO     | Dataloaders created. Train: 504, Val: 45 samples.
2025-12-14 22:37:24 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:24 | training | log_header | INFO     | TRAINING: TRANSFORMER
2025-12-14 22:37:24 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:24 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:24 | training | log_header | INFO     | TRANSFORMER CONFIG
2025-12-14 22:37:24 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:24 | training | log_config | INFO     |   epochs                         : 100
2025-12-14 22:37:24 | training | log_config | INFO     |   learning_rate                  : 0.00073
2025-12-14 22:37:24 | training | log_config | INFO     |   weight_decay                   : 1.6e-07
2025-12-14 22:37:24 | training | log_config | INFO     |   patience                       : 15
2025-12-14 22:37:24 | training | log_config | INFO     |   label_smoothing                : 0.1
2025-12-14 22:37:24 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:37:24 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:24 | training | log_header | INFO     | TRANSFORMER BEST HYPERPARAMETERS
2025-12-14 22:37:24 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:24 | training | log_config | INFO     |   d_model                        : 128
2025-12-14 22:37:24 | training | log_config | INFO     |   nhead                          : 2
2025-12-14 22:37:24 | training | log_config | INFO     |   num_encoder_layers             : 6
2025-12-14 22:37:24 | training | log_config | INFO     |   dim_feedforward                : 128
2025-12-14 22:37:24 | training | log_config | INFO     |   dropout                        : 0.1
2025-12-14 22:37:24 | training | log_config | INFO     |   learnable_pe                   : False
2025-12-14 22:37:24 | training | log_config | INFO     |   learning_rate                  : 0.00073
2025-12-14 22:37:24 | training | log_config | INFO     |   weight_decay                   : 1.6e-07
2025-12-14 22:37:24 | training | log_config | INFO     |   batch_size                     : 32
2025-12-14 22:37:24 | training | log_config | INFO     |   use_class_weights              : False
2025-12-14 22:37:24 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:37:24 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:24 | training | log_header | INFO     | MODEL ARCHITECTURE
2025-12-14 22:37:24 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:24 | training | log_model_summary | INFO     | Model: SeqTransformer
2025-12-14 22:37:24 | training | log_model_summary | INFO     | Input shape: (256, 4)
2025-12-14 22:37:24 | training | log_model_summary | INFO     | 
2025-12-14 22:37:24 | training | log_model_summary | INFO     |   input_proj                :        640 params (       640 trainable)
2025-12-14 22:37:24 | training | log_model_summary | INFO     |   pos_encoder               :          0 params (         0 trainable)
2025-12-14 22:37:24 | training | log_model_summary | INFO     |   transformer_encoder       :    597,504 params (   597,504 trainable)
2025-12-14 22:37:24 | training | log_model_summary | INFO     |   norm                      :        256 params (       256 trainable)
2025-12-14 22:37:24 | training | log_model_summary | INFO     |   classifier                :        903 params (       903 trainable)
2025-12-14 22:37:24 | training | log_model_summary | INFO     | 
2025-12-14 22:37:24 | training | log_model_summary | INFO     | Total parameters          :    599,303
2025-12-14 22:37:24 | training | log_model_summary | INFO     | Trainable parameters      :    599,303
2025-12-14 22:37:24 | training | log_model_summary | INFO     | Non-trainable parameters  :          0
2025-12-14 22:37:24 | training | log_separator | INFO     | ----------------------------------------------------------------------
2025-12-14 22:37:25 | training | log_epoch | INFO     | Epoch [   1/100] | Train Loss: 1.5888 | Train Acc: 0.5638 | Val Loss: 1.3576 | Val Acc: 0.6918 | LR: 0.000730 | Val F1: 0.5657
2025-12-14 22:37:25 | training | log_epoch | INFO     | Epoch [   2/100] | Train Loss: 1.3408 | Train Acc: 0.6821 | Val Loss: 1.2437 | Val Acc: 0.6900 | LR: 0.000730 | Val F1: 0.5764
2025-12-14 22:37:26 | training | log_epoch | INFO     | Epoch [   3/100] | Train Loss: 1.3045 | Train Acc: 0.6833 | Val Loss: 1.2502 | Val Acc: 0.6981 | LR: 0.000729 | Val F1: 0.5880
2025-12-14 22:37:26 | training | log_epoch | INFO     | Epoch [   4/100] | Train Loss: 1.2971 | Train Acc: 0.6804 | Val Loss: 1.2291 | Val Acc: 0.6938 | LR: 0.000728 | Val F1: 0.5842
2025-12-14 22:37:26 | training | log_epoch | INFO     | Epoch [   5/100] | Train Loss: 1.2856 | Train Acc: 0.6820 | Val Loss: 1.3013 | Val Acc: 0.6826 | LR: 0.000727 | Val F1: 0.5853
2025-12-14 22:37:27 | training | log_epoch | INFO     | Epoch [   6/100] | Train Loss: 1.2781 | Train Acc: 0.6869 | Val Loss: 1.1910 | Val Acc: 0.6984 | LR: 0.000726 | Val F1: 0.6031
2025-12-14 22:37:27 | training | log_epoch | INFO     | Epoch [   7/100] | Train Loss: 1.2587 | Train Acc: 0.6934 | Val Loss: 1.1877 | Val Acc: 0.7023 | LR: 0.000724 | Val F1: 0.6254
2025-12-14 22:37:28 | training | log_epoch | INFO     | Epoch [   8/100] | Train Loss: 1.2495 | Train Acc: 0.6904 | Val Loss: 1.2315 | Val Acc: 0.6965 | LR: 0.000721 | Val F1: 0.5974
2025-12-14 22:37:28 | training | log_epoch | INFO     | Epoch [   9/100] | Train Loss: 1.2563 | Train Acc: 0.6886 | Val Loss: 1.2503 | Val Acc: 0.6989 | LR: 0.000719 | Val F1: 0.6076
2025-12-14 22:37:29 | training | log_epoch | INFO     | Epoch [  10/100] | Train Loss: 1.2489 | Train Acc: 0.6923 | Val Loss: 1.2429 | Val Acc: 0.6911 | LR: 0.000716 | Val F1: 0.5977
2025-12-14 22:37:29 | training | log_epoch | INFO     | Epoch [  11/100] | Train Loss: 1.2549 | Train Acc: 0.6927 | Val Loss: 1.3052 | Val Acc: 0.6751 | LR: 0.000712 | Val F1: 0.6427
2025-12-14 22:37:30 | training | log_epoch | INFO     | Epoch [  12/100] | Train Loss: 1.2702 | Train Acc: 0.6874 | Val Loss: 1.2496 | Val Acc: 0.6881 | LR: 0.000708 | Val F1: 0.5842
2025-12-14 22:37:30 | training | log_epoch | INFO     | Epoch [  13/100] | Train Loss: 1.2680 | Train Acc: 0.6851 | Val Loss: 1.2392 | Val Acc: 0.6937 | LR: 0.000704 | Val F1: 0.5938
2025-12-14 22:37:30 | training | log_epoch | INFO     | Epoch [  14/100] | Train Loss: 1.2724 | Train Acc: 0.6856 | Val Loss: 1.1879 | Val Acc: 0.6987 | LR: 0.000700 | Val F1: 0.6004
2025-12-14 22:37:31 | training | log_epoch | INFO     | Epoch [  15/100] | Train Loss: 1.2642 | Train Acc: 0.6825 | Val Loss: 1.2470 | Val Acc: 0.7026 | LR: 0.000695 | Val F1: 0.6148
2025-12-14 22:37:31 | training | log_epoch | INFO     | Epoch [  16/100] | Train Loss: 1.2501 | Train Acc: 0.6913 | Val Loss: 1.2079 | Val Acc: 0.6977 | LR: 0.000690 | Val F1: 0.6015
2025-12-14 22:37:32 | training | log_epoch | INFO     | Epoch [  17/100] | Train Loss: 1.2221 | Train Acc: 0.6949 | Val Loss: 1.1634 | Val Acc: 0.7064 | LR: 0.000685 | Val F1: 0.6132
2025-12-14 22:37:32 | training | log_epoch | INFO     | Epoch [  18/100] | Train Loss: 1.2246 | Train Acc: 0.6938 | Val Loss: 1.1850 | Val Acc: 0.6944 | LR: 0.000679 | Val F1: 0.6020
2025-12-14 22:37:33 | training | log_epoch | INFO     | Epoch [  19/100] | Train Loss: 1.2493 | Train Acc: 0.6865 | Val Loss: 1.1927 | Val Acc: 0.7042 | LR: 0.000673 | Val F1: 0.6029
2025-12-14 22:37:33 | training | log_epoch | INFO     | Epoch [  20/100] | Train Loss: 1.2283 | Train Acc: 0.6922 | Val Loss: 1.1922 | Val Acc: 0.7054 | LR: 0.000667 | Val F1: 0.6080
2025-12-14 22:37:34 | training | log_epoch | INFO     | Epoch [  21/100] | Train Loss: 1.2347 | Train Acc: 0.6880 | Val Loss: 1.1488 | Val Acc: 0.7065 | LR: 0.000660 | Val F1: 0.6217
2025-12-14 22:37:34 | training | log_epoch | INFO     | Epoch [  22/100] | Train Loss: 1.2492 | Train Acc: 0.6851 | Val Loss: 1.2319 | Val Acc: 0.6988 | LR: 0.000654 | Val F1: 0.5925
2025-12-14 22:37:34 | training | log_epoch | INFO     | Epoch [  23/100] | Train Loss: 1.2250 | Train Acc: 0.6937 | Val Loss: 1.2178 | Val Acc: 0.6930 | LR: 0.000646 | Val F1: 0.6058
2025-12-14 22:37:35 | training | log_epoch | INFO     | Epoch [  24/100] | Train Loss: 1.2179 | Train Acc: 0.6916 | Val Loss: 1.2088 | Val Acc: 0.6995 | LR: 0.000639 | Val F1: 0.6038
2025-12-14 22:37:35 | training | log_epoch | INFO     | Epoch [  25/100] | Train Loss: 1.2106 | Train Acc: 0.6943 | Val Loss: 1.2103 | Val Acc: 0.7016 | LR: 0.000631 | Val F1: 0.6095
2025-12-14 22:37:36 | training | log_epoch | INFO     | Epoch [  26/100] | Train Loss: 1.2159 | Train Acc: 0.6964 | Val Loss: 1.1759 | Val Acc: 0.7095 | LR: 0.000623 | Val F1: 0.6209
2025-12-14 22:37:36 | training | train_model | INFO     | Early stopping triggered at epoch 26 due to no improvement in validation F1.
2025-12-14 22:37:36 | training | train_model | INFO     | Training for Transformer completed in 11.5s. Best validation F1: 0.6427
2025-12-14 22:37:36 | training | train_main | INFO     | Saved best model checkpoint to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/models/transformer/best_model.pth
2025-12-14 22:37:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:36 | training | log_header | INFO     | TRAINING: STATISTICAL BASELINE
2025-12-14 22:37:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:36 | training | log_header | INFO     | BASELINE TEST SET METRICS
2025-12-14 22:37:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:36 | training | log_evaluation | INFO     |   accuracy                  : 0.4997
2025-12-14 22:37:36 | training | log_evaluation | INFO     |   f1_weighted               : 0.4644
2025-12-14 22:37:36 | training | log_evaluation | INFO     |   f1_macro                  : 0.1069
2025-12-14 22:37:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:36 | training | log_header | INFO     | OVERALL COMPARISON SUMMARY
2025-12-14 22:37:36 | training | log_separator | INFO     | ======================================================================
2025-12-14 22:37:36 | training | train_main | INFO     | Model                     Accuracy     F1 (Weighted)   F1 (Macro)   Detection Rate   False Alarm Rate  
2025-12-14 22:37:36 | training | train_main | INFO     | ----------------------------------------------------------------------------------------------------
2025-12-14 22:37:36 | training | train_main | INFO     | Hierarchical LSTM         0.6784       0.5963          0.1661       0.3410           0.0254            
2025-12-14 22:37:36 | training | train_main | INFO     | LSTM_v2 (SeqLabeling)     0.6653       0.5912          0.1750       0.3760           0.0286            
2025-12-14 22:37:36 | training | train_main | INFO     | Transformer               0.6846       0.5878          0.1823       0.2372           0.0149            
2025-12-14 22:37:36 | training | train_main | INFO     | Statistical Baseline      0.4997       0.4644          0.1069       0.0000           0.0000            
2025-12-14 22:37:36 | training | train_main | INFO     | 
Final comparison results saved to /home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/log/comparison_results_20251214_223649.json
