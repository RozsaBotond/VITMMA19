2025-12-14 15:23:44 | INFO     | Device: cuda
2025-12-14 15:23:44 | INFO     | Loaded data: X=(300, 256, 4), Y=(300, 256)
Original samples per class:
  Class 0: 0
  Class 1: 84
  Class 2: 25
  Class 3: 13
  Class 4: 67
  Class 5: 13
  Class 6: 8

Target samples per pattern class: 84
  Class 2: adding 59 augmented samples
  Class 3: adding 71 augmented samples
  Class 4: adding 17 augmented samples
  Class 5: adding 71 augmented samples
  Class 6: adding 76 augmented samples

Balanced dataset: 504 samples (was 210)
2025-12-14 15:23:44 | INFO     |   Augmented: 504 train samples
2025-12-14 15:23:44 | INFO     | 
============================================================
2025-12-14 15:23:44 | INFO     | Training: Statistical Baseline
2025-12-14 15:23:44 | INFO     | Description: Trend + volatility-based rule detection
2025-12-14 15:23:44 | INFO     | ============================================================
Computing training statistics for baseline...
  Bullish slope: -0.1777 ± 0.6838
  Bearish slope: 0.2115 ± 0.6808
  Class priors: [0.69659598 0.1109003  0.05238095 0.03149182 0.07209821 0.02267485
 0.01385789]
2025-12-14 15:23:44 | INFO     | 
  Test Results:
2025-12-14 15:23:44 | INFO     |     Accuracy: 0.5240
2025-12-14 15:23:44 | INFO     |     F1 (weighted): 0.5579
2025-12-14 15:23:44 | INFO     |     Detection Rate: 0.2794
2025-12-14 15:23:44 | INFO     |     False Alarm: 0.3296
2025-12-14 15:23:44 | INFO     | 
============================================================
2025-12-14 15:23:44 | INFO     | Training: LSTM v2a (baseline config)
2025-12-14 15:23:44 | INFO     | Description: Baseline LSTM: 2 layers, 128 hidden, moderate dropout
2025-12-14 15:23:44 | INFO     | ============================================================
2025-12-14 15:23:45 | INFO     | Parameters: 534,279
2025-12-14 15:23:46 | INFO     |   Epoch   0: loss=1.9456, val_f1=0.1501, lr=1.00e-04
2025-12-14 15:23:51 | INFO     |   Epoch  20: loss=1.7813, val_f1=0.3766, lr=9.05e-04
2025-12-14 15:23:55 | INFO     |   Early stopping at epoch 35
2025-12-14 15:23:55 | INFO     | 
  Test Results:
2025-12-14 15:23:55 | INFO     |     Accuracy: 0.3510
2025-12-14 15:23:55 | INFO     |     F1 (weighted): 0.4178
2025-12-14 15:23:55 | INFO     |     Detection Rate: 0.8884
2025-12-14 15:23:55 | INFO     |     False Alarm: 0.6066
2025-12-14 15:23:55 | INFO     |     Training Time: 8.9s
2025-12-14 15:23:55 | INFO     | 
============================================================
2025-12-14 15:23:55 | INFO     | Training: Transformer v1a (baseline)
2025-12-14 15:23:55 | INFO     | Description: Baseline Transformer: 3 layers, 4 heads, d=64
2025-12-14 15:23:55 | INFO     | ============================================================
Traceback (most recent call last):
  File "/home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/src/train_incremental_hp.py", line 788, in <module>
    main()
    ~~~~^^
  File "/home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/src/train_incremental_hp.py", line 759, in main
    result = train_neural_model(
        model_key, cfg, train_loader, val_loader, test_loader, class_weights, device
    )
  File "/home/rozsa/Dokumentumok/egyetem/msc/VITMMA19/src/train_incremental_hp.py", line 479, in train_neural_model
    model = SeqTransformer(**model_params)
TypeError: SeqTransformer.__init__() got an unexpected keyword argument 'num_layers'. Did you mean 'num_classes'?
